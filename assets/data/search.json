[
  
  {
    "title"    : "求值的顺序",
    "category" : "",
    "tags"     : " c++",
    "url"      : "/2020/10/12/order-of-evaluation.html",
    "date"     : "October 12, 2020",
    "excerpt"  : "\n用 Seastar 的时候，常常需要推迟一个对象的析构。于是问题来了。\n\n\n平时，我们这样写程序：\n\n\n\nvoid scan(func_t&amp;amp;&amp;amp; f)\n{\n  Node root = get_root();\n  return root.scan(std::move(f));\n}\n\n\n\n但是用 Seastar 的话，因为 get_root() 可能会阻塞，我们可能可以把代码写成下面这样：\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;...",
  "content"  : "\n用 Seastar 的时候，常常需要推迟一个对象的析构。于是问题来了。\n\n\n平时，我们这样写程序：\n\n\n\nvoid scan(func_t&amp;amp;&amp;amp; f)\n{\n  Node root = get_root();\n  return root.scan(std::move(f));\n}\n\n\n\n但是用 Seastar 的话，因为 get_root() 可能会阻塞，我们可能可以把代码写成下面这样：\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;amp; f)\n{\n  return get_root().then([f=std::move(f)](Node&amp;amp;&amp;amp; root) {\n    return root.scan(std::move(f));\n  });\n}\n\n\n\n那么既然 get_root() 是异步的，那么等到调用 then() 的时候，f 会不会已经析构了呢？我们是不是应该这么写？\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;amp; f)\n{\n  return seastar::do_with(std::move(f), [this](auto&amp;amp; f) {\n    return get_root().then([&amp;amp;f](Node&amp;amp;&amp;amp; root) {\n      return root.scan(f);\n    });\n  });\n}\n\n\n\n这里就是例子：\n\n\n\nstruct foo_t {\n  foo_t&amp;amp; func(int i) {\n    cout &amp;lt;&amp;lt; &quot;func(&quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;)&quot; &amp;lt;&amp;lt; endl;\n    return *this;\n  }\n};\n\nint gen(int i) {\n  cout &amp;lt;&amp;lt; &quot;gen(&quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;)&quot; &amp;lt;&amp;lt; endl;\n  return i;\n}\n\nint main()\n{\n  foo_t foo;\n  foo.func(1).func(gen(2));\n}\n\n\n\n的输出是:\n\n\n\n func(1)\n gen(2)\n func(2)\n\n\n\n由此可知，gen(2) 是在 func(1) 返回之后才调用的。 而 get_root() 里面的代码如果是异步调用的话，可能在 scan() 返回的时候也还没有“完成”。因为异步调用返回的是一个 future ，如果 future state 当时还没有准备好，那么 .then(func) 则会把 func 包装成一个 task 等待调度。\n\n\n但是从 C&amp;#43;&amp;#43; 的角度来说呢？简化版本的代码中，有这么个表达式\n\n\n\n\n\n\n\n很明显，这里的 AST 的树根是个函数调用。n4659 中， expr.call 一节说道\n\n\n\n\nA function call is a postfix expression followed by parentheses containing a possibly empty, comma-separated list of initializer-clauses which constitute the arguments to the function.\n\n\n\n\n接着标准规定了函数参数的求值顺序\n\n\n\n\nThe postfix-expression is sequenced before each expression in the expression-list and any default argument. The initialization of a parameter, including every associated value computation and side effect, is indeterminately sequenced with respect to that of any other parameter.\n\n\n\n\n用通俗易懂的话，就是：\n\n\n\n\nWhen calling a function (whether or not the function is inline, and whether or not explicit function call syntax is used), every value computation and side effect associated with any argument expression, or with the postfix expression designating the called function, is sequenced before execution of every expression or statement in the body of the called function.\n\n\n\n\n所以，我们这个例子里面 .then() 有两个参数，在真正调用 .then() 之前，我们必须先对这两个参数求值。\n\n\n\n\n一个是 this，它的值由 get_root() 返回，为了得到这个参数必须对 get_root() 求值。\n\n\n另一个是一个 lambda 表达式，它的值由 capture list 和后面的函数体决定。但是请注意，要对这个表达式求值并不需要执行这个 lambda 表达式。它的值就是一个 lambda 表达式。\n\n\n\n\n所以在调用 .then() 之前，f 的值就被稳妥地保存在第二个参数里面了，并且因为我们是 capture by move，所以第二个参数析构的时候，f 也会随之而去。我们并不需要为它专门做一个 seastar::do_with() 用智能指针保存它的值，延长其生命周期。\n\n\n回到一开始的 foo_t 的那个例子，其实它有些许误导。我们按照结合律，可以把这个表达式拆成这么几个\n\n\n\n\n\n\n\n所以对第二个 .func() 求值，我们必须先对 foo.func(1) 和 gen(2) 求值，当然它们的顺序不一定。然后再调用 foo.func(2)。\n\n\n但是和前文 scan() 的例子不一样，scan() 的第二个参数是个 lambda 表达式，为了对它求值，我们必须初始化 lambda 表达式中的 capture 列表。所以看上去好像有“写在后面的代码反而在之前执行了”的错觉。但是如果把语法关系理清楚，这个问题就迎刃而解了。\n"
} ,
  
  {
    "title"    : "Read the Docs 之路",
    "category" : "",
    "tags"     : " ceph, ci",
    "url"      : "/2020/10/02/cython-doc.html",
    "date"     : "October 2, 2020",
    "excerpt"  : "\n\n\n最近为了支持多单词的准确搜索，把 Ceph 的文档编译和 host 转移到了 Read the Docs 上。但是还有一些问题。\n\n\n\n\nCeph 里的 API 文档\nSphinx 的搜索\n假的 librados\n方案\n\nSphinx 能看见的预处理结果\n加入 stub 函数\n浏览器能看到的\n\n\n\n\n\n\n\nCeph 里的 API 文档\n\n\nCeph 用 Sphinx 编译文档。它作为一个平台提供 librados，让大家可以用它写程序。librados 有各种语言的绑定，其中一些有对...",
  "content"  : "\n\n\n最近为了支持多单词的准确搜索，把 Ceph 的文档编译和 host 转移到了 Read the Docs 上。但是还有一些问题。\n\n\n\n\nCeph 里的 API 文档\nSphinx 的搜索\n假的 librados\n方案\n\nSphinx 能看见的预处理结果\n加入 stub 函数\n浏览器能看到的\n\n\n\n\n\n\n\nCeph 里的 API 文档\n\n\nCeph 用 Sphinx 编译文档。它作为一个平台提供 librados，让大家可以用它写程序。librados 有各种语言的绑定，其中一些有对应的文档：\n\n\n\n\nC: 用 Breathe 的 autodoxygenfile directive\n\n\nC&amp;#43;&amp;#43;: 还没有加上去。不过我觉得用 Breathe 和 Doxygen 的组合应该就够了\n\n\nPython: Sphinx 有内置的支持，即 sphinx.ext.autodoc 扩展。我们主要用它的 automethod directive\n\n\nOpenAPI: 用 sphinxcontrib.openapi 提供的 openapi directive\n\n\n\n\n\n\nSphinx 的搜索\n\n\nSphinx 内置有搜索功能，它也支持多词搜索，但是通常我们希望搜索 source code 的话，Sphinx 会返回所有包含 source code 的文档，即使文档里面出现的是 &quot;code source&quot; 或者 &quot;source of code&quot;。换言之，它不是我们习惯上的多词搜索。这个问题其他开发者也碰到了，在 Sphinx 上有相关的 issue。我研究了一下，这个问题在于 Sphinx 搜索的实现比较直接。它分这么几步\n\n\n\n\n分词。每种语言的分词都不一样。值得一提的是，中文分词用的是“结巴”分词\n\n\n逐词预处理，用对应语言的 stemming 规则把词归一化。英语的实现可以参考这里\n\n\n\n去掉后缀。比如说 apples 这个词就会变成 apple。civilize 则会变成 civil。\n\n\n去掉常见的介词、连词和代词。比如说 at、and 和 they 就会被去掉。\n\n\n\n\n\n把索引关系加入倒排表。组织成一个大数据结构，保存在磁盘上。\n\n\n在搜索的时候，javascript 会直接从这个表里面找。\n\n\n\n\n所以可以想见，如果我们搜索 &quot;fuse support&quot; 想看看 Ceph 对 FUSE 的支持，它会返回 mount.fuse.ceph 的 manpage。这虽然也不算离谱，但是里面出现的 support 是这么一句话\n\n\n\n\nThe old format /etc/fstab entries are also supported:\n\n\n\n\n通篇没有出现 &quot;fuse support&quot; 这个序列。搜索返回了 32 篇文章，后 31 篇 文章被检索到的关键字就是 unsupported。这个很可能不是我们想要的。而 RTD 的多词搜索的结果要好很多。对于不挑剔的读者基本上够用了。\n\n\n那为什么要纠结多词搜索呢？因为我们很多命令是多个单词构成的。比如说\n\n\n\nceph df\n\n\n\n要是用户想搜索 &quot;ceph df&quot;，多词搜索要是能精确匹配，问题不就能解决了吗？那有没有其他办法呢？\n\n\n\n\ngoogle 的站内搜索。但是 google 是一个商业公司。有的人可能会浑身不自在，如果他用一个广告公司的搜索。虽然在这个世界上，我们和商业公司有千丝万缕的联系，但是，哎。让我们留一点理想主义的念想吧。\n\n\n直接用 Read the Docs 的一揽子方案。它和 travis 这些服务很像，不仅内置了 CI 的功能，也能帮着 host 这些静态页面。对我们很合适。但是它的 build 流程是很死的。看看它的配置文件就知道了。这是为一个纯 Python 项目度身定制的。我们后文分析这个限制的影响。\n\n\n其他 sphinx search plugin。找了一圈，没有不收费的。功能比较好的也需要自己搭建 Elasticsearch。RTD 开源了他们的方案。但是一想到要挠我们实验室小哥的门，我就知趣地把爪子收起来了。\n\n\n\n\n\n\n假的 librados\n\n\nRead the Docs 的搜索不错，但是它的限制也很明显。\n\n\n\n\n只能通过 requirements.txt 安装第三方依赖。那么 requirements.txt 到底是啥呢？它是 pip 用来给 pip install 传参数的。 文档说得明白。\n\n\n也可以用 setuptools 或者 pip 安装源码里面的 Python 项目。\n\n\n没有预处理阶段。pip 装好了，直接就是 sphinx-build。\n\n\n\n\n我们回到各种语言的绑定：\n\n\n\n\nC: Breathe 其实本身并不能解析 C 代码里面的注释，也不能理解头文件。它事实上担当的角色是 Doxygen 产生的 XML 文件到 Sphinx 中间的桥梁。但是如果这些 XML 不存在，巧妇难为无米之炊。所以它会调用用 Doxygen 预处理一下指定的文件。但是问题来了，doxygen 怎么安装呢？它是一个 C++ 的项目。\n\n\nPython: automethod 读取制定方法的 docstring，产生 Sphinx 的文档。最近一部分代码用上了 PEP484 风格的标注，所以我们也用 sphinx_autodoc_typehints 来把这些标注变成 Sphinx 文档。这两种办法都要求 Sphinx 的 Python 环境能访问被处理的 Python 扩展 (模块)。\n\n\nOpenAPI: openapi 读取的是一个 yaml 文件。我们目前解决这个问题的办法是直接把这个文件放到了 repo 里面。但是大家都知道这个 yaml 文件其实是从代码产生的。把预处理的结果放到 repo 里面显然不是一个最好的方案，在现阶段这是一个折中。\n\n\n\n\n但是对于 Python API 来说，以 python-rados 为例，它是用 Cython 编写的 Python 扩展，它的底层则是 librados C API。我们编译文档的时候其实并不需要一个功能上完备的 librados，我们只需要让 sphinx 能导入 python-rados 就行了。sphinx 并不会真正运行 python-rados 的函数，它只会读取代码里面的元数据。所以 Ceph 里面用了一个比较取巧的方法。\n\n\n\n\n为 lib/librados.so 建立一个空链接，指向 lib/librados.so.1\n\n\n用 GCC 编译一个空的 lib/librados.so.1\n\n\n用 pip 安装 python-rados，pip 会自动执行 setup.py 脚本，后者会\n\n\n\n调用 Cython 编译对应的 rados.pyx，生成 C 代码，然后\n\n\nGCC 继续用指定源代码里的头文件目录，刚才生成的空动态链接库，生成 rados 的 Python 扩展。\n\n\n\n\n\n至此，rados 的 python 扩展编译好了。但是它链接的 librados.so 只是个空壳子。如果有人希望 import rados ，一定会出错。因为那些符号都不存在呢。所以我们用 nm 分析这个 Python extension，找出它引用的所有符号，看看它有没有 librados  API 的前缀。把这些符号，其实也是函数，统一写成 void func(void) {} 的样子，用管道交给 GCC 生成新的 lib/librados.so.1 。虽然它是假的，但是至少 import 的时候就不会出错了。\n\n\n\n\n\n\n\n\n\nOpenAPI 文档的 yaml 文件的产生过程要简单很多，但是也需要使用我们自己编写的 python 脚本。但是 RTD 的 requirements.txt 没法实现这么复杂的预处理逻辑。\n\n\n\n\n方案\n\n\nSphinx 能看见的预处理结果\n\n为了能有一个 librados，我们可以在 PyPI 注册一个项目，让 Ceph 发布新版本的时候也更新它。同时，我们的文档编译流程也能直接从 PyPI 安装 python-rados。openapi.yaml 其实也可以放在这里面。具体说就是\n\n\n\n\n注册 python-rados 项目。其他 Python 绑定也同理，比如 cephfs、rgw、rbd。\n\n\n一旦修改任何 Python 绑定的 pyx，就需要发布一个新版。\n\n\n让 ceph/admin/doc-read-the-docs.txt 安装 python-rados， python-cephfs 等。\n\n\n\n\n\n加入 stub 函数\n\n在编译文档的时候，在 rados.pyx 中实现所有使用到的 C 函数。不过需要注意，这些函数也应该暴露出来给 python-cephfs 它们用。当然，只有在编译文档的时候才这么做。\n\n\n\n浏览器能看到的\n\n另外一个办法就是保留我们的 CI 流程，让它编译 API 相关的文档，然后让 RTD 的文档引用我们自己编译的文档。这需要\n\n\n\n\n新建一个域名，专门用来保存 API 文档。题外话，它也可以用来保存 CI 产生的文档。\n\n\n修改文档里面所有引用 API 文档的超链接，加入条件：\n\n\n\n如果是 RTD 编译的话，就链接到刚才的域名\n\n\n其他情况，就使用相对路径\n\n\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "std::error_code",
    "category" : "",
    "tags"     : " c++",
    "url"      : "/2020/09/30/error-code.html",
    "date"     : "September 30, 2020",
    "excerpt"  : "\nC++11 中的错误处理的核心是 std::error_code。它的引进满足了错误处理可扩展的需求。\n\n\n\n\nvalue() 返回一个 int，这个是 error code 本身。\n\n\ncategory() 则是“可扩展的关键”。它允许在不同上下文或者说 domain 里面，有相同 value 的 error_code 代表不同的意思。不过，value 为 0 一般来说，就是没有错误。这个是约定俗成的。\n\n\n\n\nerror_code 有三个构造函数，其中， template&amp;lt;cl...",
  "content"  : "\nC++11 中的错误处理的核心是 std::error_code。它的引进满足了错误处理可扩展的需求。\n\n\n\n\nvalue() 返回一个 int，这个是 error code 本身。\n\n\ncategory() 则是“可扩展的关键”。它允许在不同上下文或者说 domain 里面，有相同 value 的 error_code 代表不同的意思。不过，value 为 0 一般来说，就是没有错误。这个是约定俗成的。\n\n\n\n\nerror_code 有三个构造函数，其中， template&amp;lt;class ErrCodeEnum&amp;gt; error_code(ErrCodeEnum e) 是最有意思的。它调用的是 make_error_code() 。看上去，似乎它让我们能从自定义的类型构造出 error_code。是的，不过我们需要用\n\n\n\nstd::is_error_code_enum&amp;lt;ErrorCodeEnum&amp;gt;::value == true\n\n\n\n来明确地规定这个行为。只有为这个类型定义特化的模板，才能让该类型能转换到 error_code。\n\n\n请注意，std::error_code 定义了多个 operator==()，它不仅仅可以和相同类型的 std::error_code 比较，而且可以和另一个 std::error_condition 比较。只要后者和前者 equivalent 或者反之，那么就把他两个视为相等。\n\n\n\nbool operator==(const error_code&amp;amp; __x, const error_condition&amp;amp; __y) _NOEXCEPT\n{\n    return __x.category().equivalent(__x.value(), __y)\n        || __y.category().equivalent(__x, __y.value());\n}\n\n\n\n具体分析，如果 Seastar 底下抛出来的是一个 std::system_error(ret, std::system_category())，我们希望看看这个 exception 是不是匹配我们的 conditon。第一个判断是\n\n\n\n\nsystem_category.equivalent(ret, condition) &amp;#8658; default_error_condition(code) == condition\n\n\n\nsystem_category() 返回的是一个 system_error_category 的 singleton，它的 default_error_condition(int ev) 的定义类似\n\n\nreturn error_condition(ev, generic_category())\n\n\n\n\n\n\n\n\n\n但是如果 ELAST 这个宏定义了的话，当 ev &amp;gt; ELAST, 返回的 category 则是 system_category() 而非 generic_category()\n  所以，如果 == 的右边是一个 error_condition，而且 val 也是 ret，category 也是 generic_category 的话，那么就是匹配的。\n\n\n\n\n\n\n如果根据左边 category 判断不匹配，我们需要看看右边的意见。\n\n\n\n\n\nconditon.category.equivalent(const error_code&amp;amp; code, int condition)\n\n\n\n如果是我们自定义的 condition 的话，就根据自定义的实现来判断。\n\n\n否则就看缺省的实现\n\n\n*this == code.category() &amp;amp;&amp;amp; code.value() == condition;\n\n\n\n所以，首先要求 y.category() == x.category()，即，两者同属一个 category；而且要求 x.value() == y.value() 即两者的 value 一致。\n\n\n\n\n\n\n\n\n这就是为什么下面的代码是有效的原因：\n\n\n\ntry {\n  // ..\n} catch (const std::system_error&amp;amp; e) {\n  if (e.code() == std::errc::invalid_argument) {\n    //..\n  }\n}\n\n\n\n请注意，std::errc::invalid_argument 本身并不是一个 std::error_condition。它只是标准库用来表示 error_condition 的一系列 errc 枚举中的一个。 标准库不希望用平台或是领域相关的 errno 来直接表示错误，而是想把不同平台和 domain 的比较方式统一到一个框架下面，通过各自定义的 error_condition 来进行比较。std::errc 就是这个框架下面的一个例子，它表示平台无关的各种系统错误，通过 error_condition 得以用来于判断和比较 std::error_code。和 std::error_code 类似，error_condition 本身也应该是可以扩展的，它的几个构造函数和 error_code 如出一辙，第三个构造函数是\n\n\n\ntemplate&amp;lt;class ErrorConditionEnum&amp;gt; error_condition(ErrorConditionEnum e)\n\n\n\n和 error_code 也一样，它要求下面的特化来限定这个行为，只有满足它，error_condition 的这个构造函数才会去调用 std::make_error_condition(errc)：\n\n\n\nstd::is_error_condition_enum&amp;lt;T&amp;gt;::value == true\n\n\n\n不过和 std::error_code 不一样，std::error_condition 是平台无关的。std::error_code 用来表示一个平台相关和领域相关的错误，而 std::error_condition 专门用来处理\n\n\nsystem_error 实例的 code() 返回的是一个 std::error_code() 引用，所以，它可以用多态的方式来比较。这里，std::errc::invalid_argument 是一个 std::errc 的 enum class 成员。按照 C&amp;#43;&amp;#43;11 它可以自动转换成为 error_condition 以方便 error_code 来比较。而标准库规定了 std::is_error_condition_enum&amp;lt;std::errc&amp;gt;::value == true 的特化。所以，当我们比较 std::error_code 和 std::errc 的时候，C&amp;#43;&amp;#43; 会把后者自动转换成对应的 std::error_condition。不过，也不全然是“自动”的。因为这里调用的 make_error_condition(errc) 其实也是一个特化，否则谁知道这个 error_condition 里面的 value 和 category 应该是什么样子呢？\n\n\n错误处理的三驾马车分别是 error_code, error_condition 和 error_category。error_category 是一个多态的类型。他负责\n\n\n\n\n比较 (int code, error_condition&amp;amp;), 或者 (error_code&amp;amp;, int condition) 这两个 equivalanet() 正是 error_code operator==() 所使用的两个条件。\n\n\nname 自报家门，我是哪个 domain 的。\n\n\nmessage(int condition) 这个 error condition 是什么\n\n\ndefault_error_condition(int code) 根据 error code 构造一个 condition\n\n\n\n\n可以说， error_category 是衔接 error_code 和 error_condition 的枢纽。它既知道平台相关 error_code，又了解平台无关的 error_condition。有了它，我们才能知道代码抛出的 code 是不是我们所关心的 condition。\n"
} ,
  
  {
    "title"    : "RADOS 里的顺序",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2020/09/20/out-of-order-in-ceph.html",
    "date"     : "September 20, 2020",
    "excerpt"  : "\n\n\n乱序和并发的问题无处不在，Ceph 里面也是这样。\n\n\nRADOS 是 Ceph 的基础。而它在不同的上下文中有不同的含义\n\n\n\n\n集群。比如说数据保存在 RADOS 集群里。\n\n\nAPI。比如说，大家提到 librados，可能就是说用来访问 RADOS 集群的 C API。\n\n\nRADOS 协议。librados 客户端要和集群通信，就基于这个协议。它包含着身份验证，鉴权，控制面的操作，以及 I/O，等等。\n\n\n一个叫做 rados 命令行工具。它可以说是 Ceph 的瑞士军刀。...",
  "content"  : "\n\n\n乱序和并发的问题无处不在，Ceph 里面也是这样。\n\n\nRADOS 是 Ceph 的基础。而它在不同的上下文中有不同的含义\n\n\n\n\n集群。比如说数据保存在 RADOS 集群里。\n\n\nAPI。比如说，大家提到 librados，可能就是说用来访问 RADOS 集群的 C API。\n\n\nRADOS 协议。librados 客户端要和集群通信，就基于这个协议。它包含着身份验证，鉴权，控制面的操作，以及 I/O，等等。\n\n\n一个叫做 rados 命令行工具。它可以说是 Ceph 的瑞士军刀。\n\n\n\n\n这次我们从协议出发，讨论一下 OSD 对读写顺序的处理。它和前面的 多核和顺序 提到的乱序访问内存的问题非常相似。\n\n\n\n\nlibrados 的访问顺序\n\n\n客户端如果要访问集群中的数据，就需要向相关的 OSD 发送 MOSDOp 请求，OSD 则会用 MOSDOpReply 进行回应。它们两个是 RADOS 协议用来传输数据的很重要的消息类型。每个 MOSDOp 都包含\n\n\n\n\n指定要访问的 hobject_t。它包含 pool ，对象的名字，对象的 snapid。\n\n\n一系列 OSDOp，这些 op 就像是一系列指令，不过它们的操作的对象是同一个。要是有写操作，那么写入的数据也放在这个 MOSDOp 里面，按照先后顺序，放在同一个 payload 里面，解码的时候分给各自的 op。\n\n\n\n\nlibrados 为客户端提供了两种调用方式\n\n\n\n\n同步调用。\n\n\n异步调用。\n\n\n\n\n其中，同步调用很好理解。客户端在发送完请求之后，就开始等待，直到收到 OSD 的回应。但是如果客户端选择使用异步调用的话，它就可以同时发送多个 MOSDOp，而不用等待之前的请求返回。从 OSD 的角度来看，它有机会在同一时刻看到同一客户端先后发来的多个请求。不管如何，OSD 都有义务顺序执行这些请求，让客户端收到 MOSDOpReply 的顺序和它当初发送对应 MOSDOp 的顺序一致。Ceph 有个专门的测试，叫做 ceph_test_rados。这个测试会检查客户端收到的回应的顺序是不是对应请求发出的顺序。换言之，如果某个客户端的请求序列是\n\n\n\n\nreq(write(offset=601750, len=535546, payload))\n\n\nreq(write(offset=1929910, len=271840, payload))\n\n\nreq(setxattr(&quot;header&quot;, payload), truncate(size))\n\n\nreq(read(offset=0, len=1))\n\n\n\n\n那么，OSD 的返回序列也应该是\n\n\n\n\nack(write)\n\n\nack(write)\n\n\nack([setxattr,truncate])\n\n\nack(read)\n\n\n\n\n虽然 Ceph 也可以不坚持顺序。我的理解是，sequential consistency 是便于客户端编程的一个模型，所以 Ceph 选择顺序返回是很自然的事情。后来想了一下，对于 RBD 来说，如果系统先发了一个写请求，然后再发一个读请求，那么我们一定要先完成写请求吗？答案是&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;哪有那么巧啊！&quot;`我们`&quot;在这里其实是某个特定的 OSD，这两个时间上相邻的请求，如果不是访问同一个对象，那么怎么会这么巧，这两个对象都被分配到同一个 OSD，而且先后被访问到。就好像你和大学室友在市郊的加油站排队的时候遇见了。如果是同一个对象，那么问题来了。RBD 的使用者为什么会有这种需求呢？先写一块数据，然后不等写完，开始读同一块数据，而且不关心这个读到的数据是不是之前写下去的。RBD 使用者一般来说 Linux 的本地文件系统，它们对一致性还是有要求的，所以一般不会忍受这种脏数据。所以，按照 RBD 的需求分析，这种 load-store 的乱序执行是没有意义的，而且是错误的。\n\n\n\n\nOSD 里的内存屏障\n\n\n在 OSD 的这一边，它可能会看到多个 MOSDOp 同时访问一个对象。这些请求可能来自同一客户端，也可能来自不同的客户端。在保证原子性的前提下，为了提高并发度，我们要区别对待这些 MOSDOp 对应的事务。事务可以分为三种：\n\n\n\n\n只读。很明显这些事务不会改变对象本身，所以它们可以同时进行。\n\n\n只写。提到并发写，有人可能会有点犹豫。不过请不用担心，\n\n\n读然后写或者写然后读。\n\n\n\n\n如果好几个连续的 MOSDOp 都只进行读操作，那么我们是可以同时处理它们的。只需要按照顺序把它们依次发送给 object store 就可以了。 我们把这叫做 pipelined read:\n\n\n\n\n\n\n\n我们甚至可以把这些读请求一起发给下面的 object store，让它酌情同步处理。\n\n\n和 pipelined write:\n\n\n\n\n\n\n\nobject store 自然会把它们序列化，依次处理。因为在 object store 里面也需要多个步骤才能完成一个写操作，所以把它当成一个流水线，提交多个操作更有利于提高并发。\n\n\n但是如果某个请求里面既有写，又有读呢？感受一下：\n\n\n\n\n\n\n\n第二个事务中所有的请求都是读操作。在客户端发送消息的顺序上，这个只读事务排在第一个只写的事务之后。 TCP 保证了 OSD 也是按照发送的顺序收到这两个消息的，但是在 OSD 这边，如果不加以限制的话，或者允许乱序执行的话，就会出现 store-load 重排的情况。因为在 Ceph 里面，读操作一般来说比写操作要快，因为读操作运气好的话，直接在缓存里面就能找到想要的数据，直接返回了。最不济，通过本机读几次磁盘，也能找到想要的数据。但是写操作不仅仅需要读本机磁盘，获得对象的元数据，还需要写本地磁盘，提交分布式事务，让其他副本也持久化，所以它的延迟比读请求是相对高一些的。如果我们放任自流，让两者的延迟决定谁先返回，不仅仅返回的顺序不对，返回的数据也可能是不正确的。如果我们希望实现一个严格的 sequential consistency 的系统，那么 read.2 就有义务体现 write.1 的结果。最简单的办法就是加上一个 sfence，保证 read.2 之前的写操作的事务提交完成。\n\n\n\n\n\n\n\n解决了 store-load 重排，那么 load-store 呢？我们允许在 read.2 仍然进行的时候，开始执行 write.3 吗？这取决于下面 object store 的处理顺序。我们假设这里使用的是 seastore。根据现在 seastore 的设计，要读取某个对象的指定 extent，需要\n\n\n\n\n先根据索引 onode block 的 b+ 树，找到这个对象 onode 所在的 block\n\n\n每个对象自己又有一个 b+ 树管理各自的 extent，如果运气好的话，b+ 树所有的叶子节点就内置在 onode 的 block 里面，但是如果这个对象比较大，或者 extent 的 b+ 树还没有来得及压缩，那么它就会有一些 extent 是需要再查询几个中间节点才能知道具体的逻辑地址的\n\n\n其实上层根据逻辑地址访问下面的物理介质，都需要先把逻辑地址翻译成物理地址，这个过程也需要查索引，也就是要用 LBA 树来查找。而 LBA 树的节点也是不一定都在内存里面。\n\n\n\n\n而 write.3 所对应的 extent 相关的索引信息说不定就在内存里面，可以很快的找到，从而开始写日志。同时呢，read.2 虽然身为读操作，有可能就没那么好运，需要读多次磁盘，才能找到对应的物理地址。所以我们无法保证读操作肯定是比写操作先完成的，即使读操作比写操作先开始。而且，这里的 read.2 和 write.3 都各自包含了多个操作，任何一个操作都会成为瓶颈。所以在某种极端情况下可能会是这样\n\n\n\n\n\n\n\n在这个捏造的例子里面，read.2.1 拖慢了整个事务的后腿，read.2 是在 write.3 之前开始的，但却在 write.3 之后完成。这对于期望 sequential consistency 客户端显然无法接受。同时，我们还能想象一个更复杂的场景，因为每个读请求都会指定一个区间，告诉 OSD 自己希望读的偏移量和长度。但是这个区间可能会映射到对象的多个 extent，而每个 extent 的读延迟可能会不一样。倘若 read.2.1 指定的区间正好映射到某个 extent，而这个 extent 又正好和 write.3.1 所写的 extent 有重合呢？而且，请注意，例子里面 write.3 先结束，它的事务提交的时候，刷新了 OSD 内存里面所有相关的 extent 对应 block 的 cache。所以 read.2.1 有可能读到的是 write.3 所写的内容。更可怕的是，因为 read.2 读的是多个 extent，返回的 extent 中有的可能是新的，有的则是则是老的。所以这里还有一致性的问题。\n\n\n\n\n\n\n\n简单粗暴的办法就是在 read.2 之后直接加一个 lfence，确保所有的读请求都完成，防止乱序的发送，也避免读到不一致的数据。\n\n\n\n\n\n\n\n对于 erasure coded pool 这个问题更复杂一些。如果对象保存在 erasure coded pool 里面，Ceph 在往里面写数据的时候，会\n\n\n\n\n把数据拆开成 k 等份\n\n\n再根据选择的算法计算出 m 个校验块\n\n\n再把这些数据发往 m + k 个 OSD\n\n\n\n\n倘若写操作的偏移量不是 m x chunk size 对齐的，那么这个写操作就会升级成 rmw (read modifiy write) 操作，因为它需要把自己少的那部分先读出来，解码，然后再和自己的没对齐的部分拼起来再重新拆分编码。\n\n\n\n\n\n\n\n在上图中，在编码的时候产生了 6 块数据，其中 4 块是原始数据，2 块是校验数据。为了修改这个对象，而修改的位置正好落在了 3 里面，我们必须把整个数据都读进来，然后再把写请求的数据嫁接到 3 的对应位置，重新编码。得到被修改过的 3 和全新 4，以及融合了老数据和新数据的 5 和 6。正因为 erasure coded 的写操作事实上包含了\n\n\n\n\n相邻区域的读操作\n\n\n指定区域的写操作\n\n\n\n\n所以它无法和其他的写操作在对象层面上同时进行。除非我们实现了更细粒度的访问隔离控制，确保事务的独立性。当然我们目前没有这么做并不意味着不可能，而是因为这样会比较复杂。因为每个写的事务都会涉及多个 extent。extent 可能会含有多个 stripe。两个写事务之间没有读写依赖的话，那么完全可以一起执行。也就是说，如果事务 A 不会写到事务 B 读取的数据，反之亦然，那么我们就可以认为两者是独立的。然是这需要在往下发送写请求之前，先把这些关系先分析清楚才能决定。这个可能太复杂了。而且得不偿失，以 RBD 为例，允许并发写一个 block 的请求的可能并不大。所以我们还是选择直接加 lfence。\n\n\n在 crimson 里面使用了一个 shared_mutex 的变形 tri_mutex 来解决这个问题。常规的 shared_mutex 是一个读写锁，允许多个读者，或者单个写者。tri_mutex 借用了 mutex 的名字，其实它实现的是自动添加 sfence 和 lfence 的功能。它维护着一个等待者的队列，如果有新的请求进来，tri_mutex 就看看这个请求和当前的请求是不是能一起执行，如果不能的话，就进入队列，等到现在所有正在执行的请求结束之后才能开始；如果可以的话，就直接放行。从前面的讨论，可以知道我们有下面这个规则：\n\n\n\n\n读操作可以和读操作并行\n\n\n写操作可以和写操作并行\n\n\nRMW 不能和任何操作并行\n\n\n\n\n"
} ,
  
  {
    "title"    : "博客维护指南",
    "category" : "",
    "tags"     : " jekyll",
    "url"      : "/2020/09/05/my-blog.html",
    "date"     : "September 5, 2020",
    "excerpt"  : "\n工具链\n\n\n我对 web 开发和 ruby 不熟悉，磕磕碰碰用下面的技术和工具把博客搭起来了：\n\n\n\nAsciiDoc\n\n标准化的 markup 语言。和各家 markdown 不一样，它有标准化的工具链。\n\nAsciiDoctor\n\nAsciiDoc 的参考实现。\n\njekyll-asciidoc\n\njekyll 的 AsciiDoc 插件。通过它就能用 AsciiDoc 写网站了。\n\njekyll-text-theme\n\n一开始用的 minima 主题虽然开箱即用，但是这个主题的红色...",
  "content"  : "\n工具链\n\n\n我对 web 开发和 ruby 不熟悉，磕磕碰碰用下面的技术和工具把博客搭起来了：\n\n\n\nAsciiDoc\n\n标准化的 markup 语言。和各家 markdown 不一样，它有标准化的工具链。\n\nAsciiDoctor\n\nAsciiDoc 的参考实现。\n\njekyll-asciidoc\n\njekyll 的 AsciiDoc 插件。通过它就能用 AsciiDoc 写网站了。\n\njekyll-text-theme\n\n一开始用的 minima 主题虽然开箱即用，但是这个主题的红色主题一下子让人有些亲近感。忍不住还是套用上了。\n\n\n\n\n\n\n一些补丁\n\n\n另外做了一些修改：\n\n\n\n\njekyll-asciidoc 没法支持一些插件。\n至少用这个补丁是修好了。\n测试的时候如果用自己本地的 jekyll-asciidoc 的话，需要参考 bundler 的文档，\n让 bundler 用本地 repo，而不是 RubyGems 上的版本。这也是为什么我用 Gemfile 管理 jekyll\n插件的原因。感觉这样更接近 Ruby 一些，方便理解 bundler 是如何找到插件的。\n\n\nasciidoctor_cjk_breaks 是 asciidoctor 的扩展。因为有时候一句话太长了，就希望分成几行写。但是和英文不一样，中文的字和字之间一般是不加空格的，asciidoctor 看到 linebreak 就当成了空格处理，输出的 HTML 中行尾和行首中间就加了个空格。读者看起来就很变扭，所以这个扩展就把空格去掉了。这个问题在 asciidoctor 也有报告，但是还没得到解决。这个扩展很久以前写的，可能这两年没有更新，所以改了一下，让它能和比较新的 asciidoctor 2.x 一起用。\n\n\njekyll-TeXt-theme 自带的 archive.html 页面如果用 prettier 处理的话，它会报错。用上这个补丁就没问题了。\n\n\n\n\n\n\n一些设置\n\n\n\n\nCNAME:\n\n\n\n配置 GitHub 让我的域名成为 canonical domainname\n\n\n配置我的域名服务商加了 CNAME 指向 GitHub\n\n\n\n\n\nGitHub Actions: 因为贪图用新的的 jekyll。GitHub Pages 提供的版本就有点旧了。所以用他家的 Actions 来做 CI，自动编译页面。用了下面几个 action\n\n\n\nactions/checkout\n\n\nruby/setup-ruby\n\n\nlimjh16/jekyll-action-ts\n\n\npeaceiris/actions-gh-pages\n\n\n\n\n\njekyll-TeXt-theme: 把它的配置抄了一堆。\n\n\nCSS: rouge 的语法高亮在 jekyll-text-theme 中无法生效，因为后者没有定义一些 rouge 要的 CSS 规则。\n所以从 minima 拷贝了一份 _syntax-highlighting.scss 然后按照个人喜好改了一下，放到了 _sass/custom.scss。网上也能找到好些好看的 rouge 语法高亮的主题。因为 rouge 高亮的 HTML 输出和 pygments 是兼容的，所以那些 pygments 的主题也可以拿来用。\n其实之前并不知道应该用这个名字，只是觉得 text-theme 缺了这些定义，应该补上，是后来分析了它的 assets/css/main.scss，看着 custom.scss 的名字，找到它，发现这是个空的文件。才猜测这是给用户自定义的一个 stub。\n\n\n\n\n\n\n写作要注意的事项\n\n\n多余的空格\n\n加链接的时候，如果里面的文本是中文。通常我们不希望在链接前面有空格。所以只能用\n\n\n\n这个行星上最大的link:https://github.com/[交友网站]\n\n\n\n\n结果就像这样\n\n这个行星上最大的交友网站\n\n\n\n\n要是用\n\n\n\n这个行星上最大的 https://github.com/[交友网站]\n\n\n\n\n结果就像这样\n\n这个行星上最大的 交友网站\n\n\n\n\n“交友网站”前面多了个空格。\n\n\n\n嵌入\\(LaTeX\\)公式\n\nAsciiDoc 的文档里写得很清楚。但是为了方便查找，还是自己记一下。\n\n\ninline 的 \\(LaTeX\\) 公式用\n\n\n\ninline 的 latexmath:[$LaTeX$] 公式用\n\n\n\nblock 的可以用\n\n\n\ne = mc^2\n\n\n\n结果是：\n\n\n\n\\[e = mc^2\\]\n\n\n\n记得在 AsciiDoc 文件头里加入\n\n\n\n:page-mathjax: true\n\n\n\n虽然也可以在 _config.yml 里加\n\n\n\nmathjax: true\nmathjax_autoNumber: false\n\n\n\n但是我觉得会减慢页面加载的速度，毕竟 MathJax.js 还有一些支持的字体体积也不小。\n\n\n\n\n\n常规维护\n\n\n另外，如果以后要加个插件或者其他 ruby 包，先修改 Gemfile，然后\n\n\n\nbundle install\n\n\n\n就行了。\n\n\n"
} ,
  
  {
    "title"    : "TCP 长连接的最大个数",
    "category" : "",
    "tags"     : " networking",
    "url"      : "/2020/08/23/tcp-connections.html",
    "date"     : "August 23, 2020",
    "excerpt"  : "\n\n\n提个问题，单机单网卡最大对某个特定服务器 TCP 长连接的最大个数是多少？\n\n\n和平时一样，我们假设客户端是 Linux。这可能不是一个生造出来的问题，想一想，如果我们希望设计一个支持长连接的代理服务器呢？如果有海量的客户端希望连接被代理的服务器呢？或者说我们希望为用户提供实时的消息服务呢？这是一种 c1000k 问题。\n\n\n我们从 TCP 协议开始，慢慢往外推，到操作系统直到硬件。看看一路上都有哪些限制。我们可以假设服务器是个怪物，它在 TCP 的框架下面可以有无穷的计算能力和带宽...",
  "content"  : "\n\n\n提个问题，单机单网卡最大对某个特定服务器 TCP 长连接的最大个数是多少？\n\n\n和平时一样，我们假设客户端是 Linux。这可能不是一个生造出来的问题，想一想，如果我们希望设计一个支持长连接的代理服务器呢？如果有海量的客户端希望连接被代理的服务器呢？或者说我们希望为用户提供实时的消息服务呢？这是一种 c1000k 问题。\n\n\n我们从 TCP 协议开始，慢慢往外推，到操作系统直到硬件。看看一路上都有哪些限制。我们可以假设服务器是个怪物，它在 TCP 的框架下面可以有无穷的计算能力和带宽，各种资源取之不尽用之不竭。那么问题到了 TCP。\n\n\n\n\nTCP\n\n\n数据库问题\n\n有人说，这是个数据库问题。因为 TCP 实现为了区别 TCP 连接，用了个四元组标记每个 TCP 报文\n\n\n\n\nsource ip: 32 bit for IPv4\n\n\nsource port: 16 bit\n\n\ndestination ip: 16 bit (fixed)\n\n\ndestination port: 32 bit for IPv4 (fixed)\n\n\n\n\n所以这四个数字合起来，就是数据库的复合主键。其中，目标服务的 IP 和端口都是固定的，所以我们只能从客户端这边发掘潜力：\n\n\nsource ip\n\n用 iproute2 可以为同一块网卡添加多个 IP 地址。理论上说，这就是 232 个地址。\n\n\n\nip addr add &amp;lt;ip&amp;gt;/&amp;lt;network&amp;gt; dev &amp;lt;interface&amp;gt;\n\n\n\n但是要细究的话，IPv4 有很多特殊的地址段是不能使用或者使用上有限制的。如果服务器是对公网开放的，那么我们作为客户端就不能使用外部地址，只能用那些本地的地址，比如 192.168.x.x 或者 127.x.x.x 这些。如果使用 NAT/PAT 这类技术在内部实现 IP 复用，那么就需要把 NAT 设备的限制考虑进去了。不管怎么样，数量级差不多是这个。\n\n\n\nsource port\n\n对于特定目标地址，本地端口可以选择的区间是由 net.ipv4.ip_local_port_range 决定的。\n\n\n\n$ cat /proc/sys/net/ipv4/ip_local_port_range\n32768\t60999\n\n\n\n对于给定的目标地址，以及给定的本地 IP，可以发出的连接数量就是本地端口区间的大小。所以单个本地 IP 最多可以产生 65535 个 TCP 连接。为了打破这个限制，我们必须为网卡添加多个虚拟 IP。满打满算，这就是 232+16 个链接，约为 281万亿。打个比方，我想开个公司，先从员工的工号的编码方式开始计划！嗯，就用 IPv4 的地址和 16 位的端口号来吧，所以，我的公司最多支持 281万亿个员工。这个思路扩展性很好，很强大！但是每个人都得发工资啊，我陷入了沉思&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;\n\n\n\n\nTCP 的运行时开销\n\n什么是 TCP 连接\n\n我们熟知的三次握手就能建立一个 TCP 连接\n\n\n\n\nSYN\n\n\n等待对方回应 SYN/ACK\n\n\n最后回答 ACK\n\n\n\n\n一旦双方完成这个规定的礼仪，就可以说这个连接建立了。一旦两边接上头，剩下的事情就是运行时的开销。\n\n\n\n系统 TCP 协议栈\n\n如果服务使用系统的 TCP 协议栈，那么每个连接都需要占用一个文件描述符。回忆一下 send() 和 recv()，它们的第一个参数是 socket，而socket 可不就是个 fd 嘛。所以操作系统文件描述符的最大值，这个全局的 设置 是 fs.file-max。在我的 RHEL8 上，它的值是 19603816，接近两千万了。如果我们希望用单进程实现这个服务，还需要改 ulimit -n 的限制。当然，如果内存够大，多操作系统或者用容器化的实现，以及用多进程的实现都可以越过这些限制。代价就是更多的额外开销。\n\n\n另外，还需要关注协议栈用到的缓冲区，看看 net.ipv4.tcp_wmem 和 net.ipv4.tcp_rmem，在 RHEL8 上，它们的缺省大小分别是 85K 和 16K。设置都有三组数字，分别是下限、缺省值和上限。以及 net.ipv4.tcp_mem，它控制着整个系统中所有 TCP 缓冲区的总大小的上限。这个设置表示的是内存页的数量，有三组数字，分别是下限、警戒值和上限。如果 TCP 缓冲空间总使用量达到上限之前，TCP 就会开始减少每个 TCP 连接缓冲区的分配。一旦达到上限，TCP 实现就开始丢包，希望减轻对内存系统的压力。\n\n\n\n$ grep . /proc/sys/net/ipv4/tcp*mem\n/proc/sys/net/ipv4/tcp_mem:2295903\t3061204\t4591806\n/proc/sys/net/ipv4/tcp_rmem:4096\t87380\t6291456\n/proc/sys/net/ipv4/tcp_wmem:4096\t16384\t4194304\n\n\n\n所以缺省设置下，最多使用 17G 内存，可以同时支持二百万以上的长连接。\n\n\nLinux 下新的防火墙是使用 netfilter 实现的，如果开启了防火墙那么还需要关注\n\n\n\n\nnet.ipv4.ip_conntrack_max\n\n\nnet.ipv4.netfilter.ip_conntrack_max\n\n\n\n\nnetfilter 维护着一张哈希表用来跟踪所有的 TCP 连接，所以如果这张表放不下新的 TCP 连接，TCP 就会开始丢包。\n\n\n\n用户态 TCP 协议栈\n\n\n\n带宽\n\n需要估计所有连接中活跃的比例，并且需要了解活跃连接需要的带宽是多少。\n\n\n\n内存\n\n前面如果使用系统的 TCP/IP 栈，就需要为每个连接保证 tcp_rmem.min + tcp_wmem.min 的空间。\n\n\n\n\n"
} ,
  
  {
    "title"    : "多核和顺序",
    "category" : "",
    "tags"     : " arch, x86",
    "url"      : "/2020/08/10/memory-ordering.html",
    "date"     : "August 10, 2020",
    "excerpt"  : "\n\n\n性能和易用性之间常常会有矛盾。\n\n\n晚上到住处，有人可能会先放水，休息一下，简单吃点东西，然后呢，看看水温，差不多了洗个澡。用文艺的说法，这是生活的智慧。用体系结构的话说，这是乱序执行，使用了简单的调度算法。对单身汉来说，打乱计划，用不一样的顺序安排生活可以获得更高的效率。但是对多核程序来说，这其实不一定是好事。Jim Keller 打了个 比方，他说计算机是在顺序的手法说一个故事，书里面有很多段落，段落是由句子构成的。读者可以画一个示意图，看看哪些段落和句子读的时候可以打乱顺序，而...",
  "content"  : "\n\n\n性能和易用性之间常常会有矛盾。\n\n\n晚上到住处，有人可能会先放水，休息一下，简单吃点东西，然后呢，看看水温，差不多了洗个澡。用文艺的说法，这是生活的智慧。用体系结构的话说，这是乱序执行，使用了简单的调度算法。对单身汉来说，打乱计划，用不一样的顺序安排生活可以获得更高的效率。但是对多核程序来说，这其实不一定是好事。Jim Keller 打了个 比方，他说计算机是在顺序的手法说一个故事，书里面有很多段落，段落是由句子构成的。读者可以画一个示意图，看看哪些段落和句子读的时候可以打乱顺序，而不改变表达的意思。比如\n\n\n\n\nHe is tall and smart and &amp;#8230;&amp;#8203;\n\n\n\n\n可以改成\n\n\n\n\nHe is smart and tall and &amp;#8230;&amp;#8203;\n\n\n\n\n但是这个句子如果打乱顺序的话，&quot;red&quot; 修饰的对象可能就不对了\n\n\n\n\nTall man who is wearing a red shirt\n\n\n\n\n句子里面的元素之间有依赖关系。\n\n\n\n\nmutex 的问题\n\n\n在多核环境下，要实现无锁编程或者尽量减少锁的使用，就不能用 mutex。其实，为了最大程度上优化，内核里的 mutex 在加锁的时候为了避免不必要的开销甚至分了 三种情况：\n\n\n\n\n用 lock cmpxchg() 指令，检查 mutex 的 owner 是否为空。\n\n\n如果 mutex 的所有者正在运行，那么用 spin lock 等待它。\n\n\n把希望得到锁的线程阻塞，挂到等待队列。等到锁释放的时候，再调度自己。\n\n\n\n\n这个机制叫做 futex。在 Linux、NetBSD、FreeBSD、Windows 以及 Zicron 中都实现了它。在 FreeBSD 中甚至有明确的 spin/sleep mutex 和 spin mutex 的 概念。而在 Linux 中，futex 则是一个 系统调用。\n\n\n不过，这些原语有两个问题\n\n\n\n\n系统调用是个原罪。\n\n\n\n鉴权。\n\n\n切换栈。即使 x86 使用 syscall 来实现系统调用，因为内核的地址空间和用户态程序不同，进入内核要求修改段寄存器，相应的带来 TLB 的刷新。\n\n\n缓存的刷新。\n\n\n\n\n\nPOSIX.1 要求 mutex 同步内存。\n\n\n\n\n前者大家做了很多实验，说明不管如何系统调用都比 longjmp 的开销都要大很多。而今天我想把后者展开说一下。\n\n\n\n\n内存一致性\n\n\n首先，什么叫同步内存？在多核系统里面，每个核都有自己的一个小天地，为了加快对内存的访问，CPU 核在内存中间有多层的 cache。这个设计有点像现在大家说的朋友圈。根据一些权衡，这个朋友圈中的 cache 根据亲疏远近又分了三六九等，即 L1、L2 和 L3 缓存。通常每个核都有自己的 L1 cache，L1 其中又分数据 cache 和指令 cache。L2 不分这么细。L3 缓存由多个核共享。CPU 的片内总线设计很大程度就是各个核和片内缓存的关系网的拓扑。 和硬盘一样，cache 对内存的映射也是有最小单位的，硬盘的单位是 block，而 cache 的单位叫做 cache line。所以，从硬盘往内存读数据是以页为单位，通常大小是 4K。从内存往缓存读数据以 cache line 为单位，大小一般是 64 字节。另外，还有一种特殊的缓存叫 TLB，它是用来缓存线性地址到物理地址映射的页表。每个进程都有自己的地址空间，所以每个进程的 TLB 表项也各自不同。这里涉及内存的寻址、分配和管理，我们可以另外说。\n\n\n既然是缓存，就会有缓存的一系列问题\n\n\n\n\n替换策略。比如大家耳熟能详的 LRU。再啰嗦一句，替换的最小单位也是 cache line。\n\n\n写策略。\n\n\n\nwrite through 写数据的时候，数据不仅写进 cache，而且也同时刷新内存。\n\n\nwrite back 写数据的时候，数据仅仅写到 cache 里面，把相应的 cache line 标记成 dirty。在真正需要刷内存的时候再把数据&quot;`写回`&quot;去，一旦内存和缓存同步，这个 cache line 又是 clean 得了。因为 write back 性能比较好，缓存通常用它。\n\n\n\n\n\n一致性。既然每个核读写都是通过自己的 cache，而不是直接访问内存，那么怎么保证各个核看到的数据是一致的？\n\n\n\n\nMESI\n\n这里主要就写策略和一致性展开来说一下。为了解决一致性的问题，CPU 的设计者会用 MESI 的某种改进版来保证缓存之间的同步问题。\n\n\n\n\nModified 我的这份数据是被修改过的最新版。这个数据所在的 cache line 被标记成 dirty。这个状态要求其他人的状态是 I。别人想要读这个数据，必须等我把它写回内存。一旦写回去，状态成为 S 了。\n\n\nExclusive 我的这个 cache line 别人都没有缓存，所以如果修改它的话，不会产生不一致。\n\n\nShared 我的版本和别人的版本是一样的，我们的版本都是最新的。不过，我们都是&quot;`读`&quot;者，如果要写的话，得先获得排他锁。\n\n\nInvalid 我的版本比较老了。对于一个缓存来说，一个 cache line 如果是 I 的状态，那就相当于它不存在。要是内核希望读它的话，会得到一个 cache miss。\n\n\n\n\n假设 core#0 想写 0x1347 地址，它写的不仅仅这个地址对应的内存空间，它写的是这个地址映射到的整个 cache line。\n\n\n\n\ncore#0 告诉内存说，请把 0x1347 所在的 cache line 交给我。\n\n\n内存说，好的，这里是 0x1347 所在 cache line 的 64 字节数据。\n\n\ncore#0 告诉其它核，你们的 cache 里要是有这个 cache line，立即把它作废掉。因为它的值就快过时了。\n\n\n其它核听到这个消息纷纷回应\n\n\n\n好的，我把那个 cache line 给作废了。或者干脆清除，或者把它标记成 I。\n\n\n好的。虽然我这里没有那个 cache line。不过你可以放心了。\n\n\n\n\n\ncore#0 找到 cache line 中对应 0x1347 的字节，改成自己想要的值，把那个 cache line 标记成 M。\n\n\ncore#1 想读 0x1347，但是它对应的 cache line 是 I 状态。\n\n\ncore#1 问内存，请把 0x1347 所在的 cache line 交给我。\n\n\ncore#0 不得已，把那个 cache line 写回内存。core#1 立即读到了最新的 cache line，这时他们缓存对应 cache line 的状态都改成了 S。\n\n\n\n\n\n硬件重排序\n\n这带来了另外一个问题，MESI 协议里面有两种操作会比较慢。\n\n\n其中，写一个 cache line 需要好几步。如果 cache line 不在本地缓存，或者是 I 状态。这就是个 cache miss。那么这种情况下，还需要读内存。然后为了获得 cache line 的排他锁，还需要得到其它核的确认。要是 core#1 的缓存在收到 invalidate 消息的时候正在忙其它事情呢？这会 core#0 的写操作。更何况现代处理器的核那么多，core#0 的写操作的瓶颈之一是最慢的一个核对 invalidate 请求的回应。所以 core#0 的 invalidate 最好能立即返回。所以我们在每个核的缓存前面放一个 invalidation queue，让这个操作成为异步的。core#0 只要把消息放在队列里面，就可以继续执行下一条指令。等 core#1 的缓存忙完了手里的事情，就会检查它的队列更新对应 cache line 的状态。CPU 的设计者没有就此止步，因为只是读内存，往每家的队列里面投递消息也很耗时，core#0 的流水线还有余力做其他工作，它不希望因为这个 cache miss 就干等着。最好能并行地多做几件事情。所以我们在本地的核边上也加了一个队列，叫做 store buffer 或者 write buffer。把写操作扔到 store buffer 里面，就可以立即返回。而本地缓存一旦做完那些准备工作，它就会从 store buffer 里面拿到要修改的数据，更新自己的 cache line。反之，要是等待本地缓存和其他各方把所有这些步骤完成再循规蹈矩往下执行下一条指令，就太慢了！\n\n\n\n\n\n\n\n而读一个 cache line 也不容易。类似的，要是 cache miss 的话，那么当前核就会要求另外一个核把它的数据先刷到内存。这将引起一个内存事务。\n\n\n但是这样引入了一个问题--------内存读写操作的乱序执行。这不仅让单核的顺序执行成为一个有前提的表象，更让多核的环境下的内存一致性和顺序执行更加错综复杂。对于特定的内核来说，可能会在一个写操作完成之前，就开始执行下一条指令。而对于其他内核来说，读指令可能会得到一个事实上过时 (invalid) 的数据。因为即使是写操作的发出者也还没有真正完成这个写操作，它只是把这个操作提交给了 store buffer。不过和其他内核相比，它是可以读到最新的数据的，在它执行读指令的时候，可以先检查 store buffer，如果 store buffer 里面没有对应的数据，再检查缓存。这个叫做 store buffer forwarding。因为它在当前核通过 buffer 把数据&quot;转交&quot;给将来要执行的读指令。这个设计保证了数据依赖和控制依赖，也就是单核上下一个操作的结果如果依赖上个操作的副作用，那么下个操作必须能看到上个操作的副作用。换句话说，如果从单核的角度出发，看不出这种&quot;`依赖`&quot;问题，那么 CPU 就认为它可以把读写操作重新排列，以此获得更高的并发度。另外，store buffer 的存在也催生了另外一些优化，如果有两个写操作修改的是连续的内存地址，在刷内存的时候，这两个写操作就可以合并成一个大的写操作，从而减轻内存总线的负担。这个技术叫做 write combining。 write combining buffer 就是处在 store buffer 和系统总线中间的地方。如果有往同一地址的写操作，那么时间顺序上后面操作就会覆盖前面的操作，这个技术叫 write collapsing。\n\n\n这种读写指令的乱序执行破坏了严格意义上的顺序一致性。对很多人来说，如果你要的是咖啡加奶，那么做法应该是先加咖啡再加奶，但是对一个追求效率的人来说，可能就会应该先做咖啡，在咖啡机哼哧的时候，把奶加进去，等咖啡机好了，再把咖啡倒进去。不过要是有原教旨主义者看到这个顺序可能会很不高兴，他说顺序和比例一样重要！简单说，顺序对自己可能不那么重要，但是旁人可能会很在意。\n\n\n但是甚至在不对齐写的情况下也会造成不一致的结果。说到 store buffer forwarding，之前 Linus 举了一个 例子。\n\n\n假设有个系统有三个核，开始的时候 dword [mem] 的内容是 0。执行下面的程序\n\n\n\nxor %eax, %eax\ncmpxchl $0x01010101, (mem)\n\n\n\n\nmovl $0x01010101, %eax\ncmpxchl $0x02020202, (mem)\n\n\n\n\nmovb $0x03, (mem),\nmovl (mem), reg\n\n\n\n程序结束的时候，dword [mem] 可能是 0x02020203 ，但是有趣的是，这时第三个核上 reg 里面则会是 0x01010103。因为 MESI 协议保证了 cache coherency，dword [mem] 的值先后是 0 -&amp;gt; 0x01010101 -&amp;gt; 0x02020202 -&amp;gt; 0x02020203。因为最后一次第三个核的 mov 也获得了排他锁，然后把整个 cache line 刷到了内存里面。但是第三个核的寄存器为什么读到了一个奇怪的值。这个值甚至在 cache line 里面没有缓存过。原因是第三个核会这样解释：\n\n\n\nmovb $0x03, store_buffer[mem] ; 把 [mem] &amp;lt;- 0x03 的操作放到，store buffer，写操作比较慢。先继续执行读操作\nmovl (mem), reg              ; 把 [mem] 的内容读出来\nmovb store_buffer[mem], reg  ; 读操作也会查看一下 store_buffer，看看手里面最新的数据\n\n\n\n所以第三个核寄存器中看到是一个脏数据。这个数据从来没有在内存中出现过。它有两个来源：高 24 位是第一个核写进去的，低 8 位是自己写的。而按照 cmpxch 的原子操作的语义，这个过程中是不可能有这样的不一致出现的。这也是为什么 amd64 不能保证非对齐写操作的原子性的原因。\n\n\n话说回来，不仅仅是写数据上的核可能看到脏数据，也因为 store buffer 的存在，使得各个核看到的内存并不一样 (coherent)。如果某个核的对某个 cache line 的修改存在 store buffer 里面，那么这个 cache line 在其它核眼中则是旧的数据。另外，就算本地缓存检查了 store buffer，发送了 invalidate 消息给其他核。但是在其它核在检查 invalidation queue 之前，仍然会认为那个 cache line 是有效的。有人可能会说，其他内核可以在读缓存之前看看 invalidation queue 啊，可能是因为 invalidation queue 只是个 queue，内核在读缓存之前不会去检查 invalidation queue。所以如果多个内核共享一块内存，那么某个核上读写顺序重新排列会导致程序有不同的执行的结果。有的时候我们不在乎，但是有的时候这种不一致的结果是致命的。再举个例子，在餐馆吃饭。有的餐馆在顾客点菜之后会给一个电子闹钟，等闹钟响了，就可以去自助取餐。以此为背景，我们想象有两个核分别代表等餐的顾客老王 (wong) 和面馆老马 (mars)：\n\n\n\nbool placed_order = false;\nbool beep = false;\nchar meal[128];\n\nvoid wong() {\n  placed_order = true;\n  while (!beep);\n  claim(meal)\n}\n\nvoid mars() {\n  while (!placed_order);\n  cook(meal);\n  beep = true;\n}\n\n\n\n要是平时写这个程序，大家可能会很自然地用 atomic&amp;lt;bool&amp;gt; 来定义 placed_order 和 beep。但是既然 amd64 保证了 单字节数据访问的原子性\n\n\n\n\nCacheable, naturally-aligned single loads or stores of up to a quadword are atomic on any processor model, as are misaligned loads or stores of less than a quadword that are contained entirely within a naturally-aligned quadword.\n\n\n\n\n所以 placed_order 的读写都是原子的。那么我们为什么还要用 atomic&amp;lt;bool&amp;gt; 呢？所以上面的代码就直接用 bool 了。接下来，我们在老王和兰州拉面的互动中加入 store buffer，看看会发生什么：\n\n\n\n\n老王来到面馆，大碗牛肉面！于是更新 placed_order。但是 placed_order 是在内存里面，写内存太慢了。先更新自己桌上的的 store buffer 吧。等会儿结账的时候再一起更新 placed_order 好了。\n\n\n老王看着桌上的闹钟，焦急地等待。beep 啊，你怎么还是 false 呢？都十秒钟过去了。\n\n\n面馆的马老板看着老王，这个人没有下单，眼神呆滞，从一坐下来就盯着桌上的闹钟不动。怕是昨晚加班到三点，还没缓过劲？\n\n\n又过了十秒钟&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;两个人都隐约觉得有点不对，但是不知道出了什么问题。\n\n\n\n\n对老王和老马来说，这都是个僵局。而这个僵局是 store-load 重排造成的。所以即使从单核的角度看，数据依赖和控制依赖是能够保证的，多核环境下也无法确保程序的&quot;`顺序`&quot;执行。换言之，cache conherence 不等于 sequential consistency。后者的语义需要引入更强约束。但是因为后者的约束太强了，我们在实际工作中往往会采用一些折中。\n\n\n另外，如果文献中提到 load buffer 或者 load queue，它是用来保存读请求的。比如说，如果处理器预测某个写请求之后会读取地址 X，它会把这个请求放到 load buffer 里面。一个读请求的地址计算出来之后，这个请求也会保存在 load cache 里面。对于那个写请求，它在写内存之前则会检查 load buffer，如果发现命中的话，就会让读取 X 的请求返回写请求要写入的值。load buffer 可以让内存读取批次化，使得 cache miss 的处理更有效率。\n\n\n\n\n\n一致性模型\n\n\n不同体系结构在 consistency 这个问题上有着不同的答案，这些答案就是不同的一致性模型：\n\n\n\n\nsequential consistency: 顺序一致，简称 SC。这是最死板的一致性模型。即使看上去没有危险，每个核也会以完全忠实原著的方式执行，除了缓存，不加入任何可能产生乱序的设计。所以 store buffer 和 invalidation queue 这种东西是禁止的。这种简单粗暴的限制对 CPU 的自尊心和性能是一种强烈的伤害。\n\n\nweak consistency: 弱一致。在一定程度上允许重排序，受到 memory barrier 的约束。\n\n\nrelaxed consistency: 处理器完全可以乱来乱序。\n\n\n\n\n大家对性能都有自己的坚持，没有一个有追求的处理器是顺序一致的。或者说，做到高性能的严格的顺序一致会非常困难。不过 amd64 是最接近的。它只会把代码里面的 store-load 顺序打乱，变成 load-store。像刚才老王吃面的例子里面，本来老王先点面，再看闹钟，被处理器一乱序，优化成了先看闹钟，再点面。完全乱了套。\n\n\n除此之外，还有下面几种排列。对于它们，amd64 就完全按照脚本执行了。\n\n\n\n\nstore / store\n\n\nload / store\n\n\nload / load\n\n\n\n\n在各种架构里面，amd64 是比较保守的。其他架构就比较放飞自我，比如对于 aarch64 中的 ARMv8-A 架构， 它的文档提到\n\n\n\n\nThe ARMv8 architecture employs a weakly-ordered model of memory. In general terms, this means that the order of memory accesses is not required to be the same as the program order for load and store operations. The processor is able to re-order memory read operations with respect to each other.\n\n\n\n\n而 Alpha 处理器则是另外一个极端。有这么一个 例子\n\n\n\nint x = 1;\nint y = 0;\nint* p = &amp;amp;x;\n\nvoid p1() {\n  y = 1;\n  mb();\n  p = &amp;amp;y;\n}\n\nvoid p2() {\n  int i = *p;\n}\n\n\n\n在处理器两个核分头运行完这个程序，i 竟然可能是 0！ 可以这样解释\n\n\n\n\np2 开始前就缓存了 y，它知道 y 的地址保存的值是 0\n\n\np1 执行 y = 1 ，发了一个 invalidate 消息给 p2，然后立即返回了。\n\n\np2 收到了 y 的 invalidate 消息，但是它并不急着处理，人家前面又没有 mb() 催着，于是这个消息在 invalidation queue 里躺着。\n\n\np1 这边因为 invalidate 消息立即返回，满足了 mb() 的要求，所以程序得以继续往下执行 p = &amp;amp;y。\n\n\np2 为了得到 *p 的值，先读取 p。读 p 并不要求刷 invalidation queue，所以它得到了 y 的地址。\n\n\np2 根据这个地址，索引到了自己的缓存。缓存里面有，为什么不用呢？\n\n\np2 把原来缓存的 y 的值 0 赋给了 i。\n\n\n\n\n这里，Alpha 没有根据数据依赖来刷 invalidation queue，因为为了得到 *p 读了两次内存。分别是\n\n\n\n\nmov p, %reg\n\n\nmov (%reg), reg\n\n\n\n\n这里有一个数据依赖的关系，因为第二次的输入是第一次的输出。本来很明显，最后 reg 的值至少应该是一致的。也就是说，不会出现历史上 *p 从来没有过的值。就像这个夏天你一直喝啤酒，从没喝过汽水。但是年前和一个朋友吃饭的时候，他说你们七月份在日本玩儿的时候，一起还喝过可乐。这一定是个错觉。你会觉得他记错了，把你记成另外一个人了。并不是说你从没喝过汽水，你小时候还挺喜欢喝。而是你和这个朋友才认识一年，你这一年的确没喝过汽水啊。\n\n\n不过这些选择并没有高下之分。如果只允许重排一两种读写序列，好处是程序员可以按照直觉编写多核程序，而不用太关心读写重排的问题。问题在于处理器的设计会有一些限制。要是需要同时有高并发，和严格顺序，那么处理器就必须把这些读写序列组织成一个个内存事务，如果处理器发觉因为乱序执行破坏了事务，那么就必须把乱序执行的操作取消掉。这使得高性能的并行处理器的设计变得更复杂了。如果处理器遵循的内存模型允许处理器做很多类型的重排序，那么处理器的设计会有很高的自由度，能无所顾虑地应用一些提高并发性的技术，来提高访问内存的效率，比如\n\n\n\n\nout-of-order issue\n\n\nspeculative read\n\n\nwrite-combining\n\n\nwrite-collasping\n\n\n\n\n如果处理器不需要保证访存的顺序，在相同性能指标下，功耗也低一些。在保证数据依赖和控制依赖的前提下，处理器有最大的自由度重新排序读写指令的顺序。但是对程序员的要求就更高了。他们需要再需要顺序的地方安插一些指令，手动加入 memory barrier，让处理器在那些地方收敛一下。这些 memory barrier 要求当前的内核把自己的 invalidation queue 里所有的 invalidate 消息都处理完毕，再处理读写请求。而程序员也可以帮助处理器做一些猜测，比如说 prefetch 和 clflush 具体影响处理器的 cache 行为。\n\n\nmemory barrier 和 lock\n\nlfence, sfence, mfence 是 SSE1/SSE2 指令集提供的指令：\n\n\nAMD64 Architecture Programmer&amp;#8217;s Manual 卷 2，7.13 ：\n\n\n\n\nThe LFENCE, SFENCE, and MFENCE instructions are provided as dedicated read, write, and read/write barrier instructions (respectively). Serializing instructions, I/O instructions, and locked instructions (including the implicitly locked XCHG instruction) can also be used as read/write barriers.\n\n\n\n\n\nlfence\n\nLoad Fence: 即 read barrier。以 lfence 调用的地方为界，定义了读操作的偏序集合。保证系统在执行到它的时候，把之前的所有 load 指令全部完成，同时，在其之后的所有 load 指令必须在其之后完成，不能调度到它的前面。换句话说，它要求刷 invalidation queue，这样当前核所有的 invalidate 的 cache line 都会被标记成 I，因此，接下来对它们的读操作就会 cache miss，从而乖乖地从内存读取最新数据。\n\nsfence\n\nStore Fence: 即 write barrier。以 mfence 调用的地方为界，定义了写操作的偏序集合。保证系统在执行到它的时候，把之前的所有 store 指令全部完成，同时，在其之后的所有 store 指令必须在其之后完成，不能调度到它的前面。它要求刷 store buffer，这样当前核所有积攒的写操作都会发送到缓存，缓存刷新的时候会发送 invalidate 消息到其他核的缓存。sfence 是 SSE1 提供的指令。\n\nmfence\n\nmemory Fence: 即 read/write barrier。以 mfence 调用为界，定义了读和写操作的偏序集合。确保系统在执行到它的时候，把之前的所有 store 和 load 指令悉数完成，同事，在其之后的所有 store 和 load 指令必须在其之后完成，不能调度到它的签名。也就是说，它会清空 store buffer 和 invalidation queue。\n\nlock\n\nlock 前缀：它本身不是指令。但是我们用它来修饰一些 read-modify-write 指令，确保它们是原子的。带有 lock 前缀的指令的效果和 mfence 相同。另外，文档告诉我们，xchg 缺省带有 lock 属性，所以也可以作为 read/write barrier。所以在 内核里面有时会看到类似 lock; addl $0, 0(%%esp) 的代码，这里就是在加 memory barrier，同时检查 0(%%esp) 是否为零。\n\n\n\n\n其实 x86 还有一些指令也有 memory barrier 的作用，但是它们本身有很强的副作用，比如 IRET 会改变处理器的控制流，所以一般来说，要控制内存访问的顺序还是用专门的 memory barrier 和 lock 指令比较容易驾驭。\n\n\n所以有了 lfence 我们可以这么改\n\n\n\nvoid p2() {\n  int* local_p = p;\n  lfence();\n  int i = *local_p;\n}\n\n\n\n禁止处理器重排这两个 load 指令。\n\n\n\nC&amp;#43;&amp;#43; 的一致性模型\n\nC&amp;#43;&amp;#43; 程序员一般不会直接使用这些 memory barrier，它们太接近硬件，可移植性也很差。比如说 aarch64 上的 memory barrier 就叫别的 名字，功能也有些许的不同。所以 C&amp;#43;&amp;#43;11 以及之后的标准规定了几种内存一致性模型，用更抽象的工具来解决这些问题。\n\n\n在解释这些一致性模型之前，我们先回到刚才的面馆。假设老王顺利地下了单，老马也看到了老王的 placed_order ，开始做面条。但是问题来了，处理器不知道 beep 和 noodle 是有先后关系的，所以负责老马的那个核就自作主张，先刷新了 beep，而把 noodle 的写操作放在 store buffer 里面了。这是一种 store-store 重排，在 amd64 上不会发生，但是在其它架构是有可能的。\n\n\n\nbool placed_order = false;\nbool beep = false;\nuint64_t noodle;\n\nvoid wong() {\n  placed_order = true;\n  while (!beep);\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  noodle = cook();\n  beep = true;\n}\n\n\n\n这里有两种数据\n\n\n\n\n被保护的数据 noodle\n\n\n用来表示 noodle 状态的标志&amp;#8201;&amp;#8212;&amp;#8201;beep\n\n\n\n\n这有点像使用 mutex 的情况。mutex 一般用来保护共享的数据，它自己则是有明确的状态的，即 mutex 当前的所有者。在这里也是如此，\n\n\n\n老王\n\n通过读取 beep 的状态，获取锁，一旦 beep 告诉他，&amp;#8220;可以通过&amp;#8221;，那么他就可以放心访问被保护的 noodle。这个过程叫做 acquire。\n老马 开始的时候，老马其实已经是锁的所有者了。正是因为这样，他才得以放心地煮面，修改 noodle 。一旦完成了修改，他就可以通过修改 beep 的值来放弃锁。告诉别人，你们看到 beep 没有，它现在是响着的，可以来访问这个 noodle 了！这个过程叫做 release。\n\n\n\n\n所以，为了避免 store-store 重排，我们用 release-acquire 语义改进了实现：\n\n\n\nbool placed_order = false;\natomic&amp;lt;bool&amp;gt; beep = false;\nuint64_t noodle;\n\nvoid wong() {\n  placed_order = true;\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n这里除了避免 store-store 重排，其实还确保 load-load 的顺序：\n\n\n\n\nbeep.store(true, std::memory_order_release) 确保 noodle = cook() 产生的读写操作不会被放到 beep.store() 后面去。你想想，beep 一响，就像泼出去的水，如果这时候告诉顾客，我还在擀面，那不是很让人恼火？所以我们一定要保证 beep.store() 之前事情不会拖到后面去。\n\n\nbeep.load(std::memory_order_acquire) 确保 consume(noodle) 产生的读写操作不会放到 beep.load() 之前。否则就会出现老王在 beep 响之前，就直接去拿面的情况。让正在擀面的老马措手不及。\n\n\n\n\n我们再回到 load-store 的问题。这个问题其实很难用 release-acquire 的模型描述，因为 placed_order 不是用来保护一个共享的数据的，或者说它本身就是一个共享的标记。在老王下单之前，他没有加老马家拉面的微信号，也没有填写老马搞的调查问卷。不过这个问题可以这么思考，placed_order 应该在老马看到它之后重新设置成 false，这样老马再次看到它的时候就不会以为老王又要了一碗面了。老王这边其实也有类似的问题，和他一起去吃面的老李也会改 placed_order，要是两个人都把 placed_order 改成了 true，那么老马做的下一碗面到底归谁呢？所以程序应该这么改：\n\n\n\nbool placed_order = false;\natomic&amp;lt;bool&amp;gt; beep{false};\nuint64_t noodle;\n\nvoid wong() {\n  while (placed_order);\n  placed_order = true;\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  placed_order = false;\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n这样还是有问题，因为老李搞不好会中途插一脚\n\n\n\n\n老王看到没人下单了，正准备把 placed_order 改成 true。他还没开始 placed_order = true 就开小差了，看着门外突如其来的暴雨，又陷入了沉思。\n\n\n老李也注意到了，他立即把 placed_order 改成了 true。开始看着 beep 焦急地等待自己的大碗牛肉面。\n\n\n老王回过神来，也把 placed_order 改成了 true。\n\n\n两个人一起焦急地等待那碗面。\n\n\n\n\n所以我们应该用原子操作来修改 placed_order，让 read-modify-write 一气呵成，用 compare-and-exchange 正合适：\n\n\n\natomic&amp;lt;bool&amp;gt; placed_order{false};\natomic&amp;lt;bool&amp;gt; beep{false};\nuint64_t noodle;\n\nvoid wong() {\n  bool expected = false;\n  while (!placed_order.compare_exchange_weak(expected, true,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  bool expected = true;\n  while (!placed_order.compare_exchange_weak(expected, false,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n在 amd64 上\n\n\n\n  bool expected = false;\n  while (!placed_order.compare_exchange_weak(expected, true,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n\n\n\n会被 GCC 翻译成\n\n\n\n  movb   $0x0, -0x1(%rsp) ; expected = false\n  mov    $0x1, %edx       ; desired = true\nretry:\n  movzbl -0x1(%rsp), %eax ; expected =&amp;gt; %al\n  lock   cmpxchg %dl, 0x2ee2(%rip)\n  je     while_beep_load\n  mov    %al, -0x1(%rsp)  ; %al =&amp;gt; expected\n  jmp    retry\n\n\n\n因为我们只需要原子操作， 所以这里只用了 std::memory_order_relaxed，它对内存的访问顺序没有限制。但是前面提到，xchg 缺省带有 lock 属性，而 lock 前缀的效果和 mfence 相同。所以用不着专门加入 mfence，我们也能要求处理器顺序访问内存了。否则的话，我们需要这么写\n\n\n\n  placed_order = true;\n  std::atomic_thread_fence(std::memory_order_seq_cst);\n  while (!beep);\n\n\n\n这样 GCC 会产生\n\n\n\n  movb   $0x1,0x2ef2(%rip)\n  lock   orq $0x0,(%rsp)\n  nopl   0x0(%rax)\nretry:\n  movzbl 0x2ed9(%rip),%eax\n  test   %al,%al\n  je     retry\n\n\n\nclang 则会用 mfence 代替 lock orq 指令。效果是一样的。根据查到的文献，两者的性能不分伯仲。\n\n\nload 和 store 一般成对使用：\n\n\n\n\n\n\n\n\nload\nstore\n\n\n\n\nmemory_order_seq_cst\nmemory_order_seq_cst\n\n\nmemory_order_acquire\nmemory_order_release\n\n\nmemory_order_consume\nmemory_order_release\n\n\nmemory_order_relaxed\nmemory_order_relaxed\n\n\n\n\n在 x86 下：\n\n\n\n\n\n\n\n\nC&amp;#43;&amp;#43;\n汇编\n\n\n\n\nload(relaxed)\nmov (mem), reg\n\n\nload(consume)\nmov (mem), reg\n\n\nload(acquire)\nmov (mem), reg\n\n\nload(seq_cst)\nmov (mem), reg\n\n\nstore(relaxed)\nmov reg, (mem)\n\n\nstore(release)\nmov reg, (mem)\n\n\nstore(seq_cst)\nxchg reg, (mem)\n\n\nstore(relaxed)\nmov reg (mem)\n\n\nstore(relaxed)\nmov reg (mem)\n\n\n\n\n其中，load(seq_cst) 和 store(seq_cst) 也可以这么实现\n\n\n\n\n\n\n\n\nC&amp;#43;&amp;#43;\n汇编\n\n\n\n\nload(seq_cst)\nxchg (mem), reg\n\n\nstore(seq_cst)\nmov reg, (mem)\n\n\n\n\n刚才我们用了\n\n\n\n\nmemory_order_relaxed / memory_order_relaxed\n\n\nmemory_order_acquire / memory_order_release\n\n\n\n\n帮老王和老马摆脱了困境。如果要解决 Alpha 处理器的问题的话，可以\n\n\n\nint x = 1;\nint y = 0;\natomic&amp;lt;int*&amp;gt; p = &amp;amp;x;\n\nvoid p1() {\n  y = 1;\n  p.store(&amp;amp;y, memory_order_release);\n}\n\nvoid p2() {\n  int i = *p.load(std::memory_order_consume);\n}\n\n\n\n好在只有 Alpha 处理器这么粗犷，敢于无视数据依赖，用刚从内存里面读出来的数据作为地址，来索引缓存里面的老数据，得到指针指向的数值。其他体系架构都会重视数据依赖问题，在这个数据依赖链条上顺序执行。因为 amd64 不会重新排列 load-load，所以它天生对这个问题免疫。另外，C&amp;#43;&amp;#43;17 说\n\n\n\n\nmemory_order_consume: a load operation performs a consume operation on the affected memory location. [ Note: Prefer memory_order_acquire, which provides stronger guarantees than memory order_consume. Implementations have found it infeasible to provide performance better than that of memory_order_acquire. Specification revisions are under consideration.\n\n\n\n\n看起来要实现依赖链条的分析很麻烦，几家 C&amp;#43;&amp;#43; 编译器都懒得矫情，干脆杀鸡用牛刀，所以性能没提高。标准也向现实低头，那么我们如果一定要把依赖关系写得明明白白，做好跨平台的工作，还是用 memory_order_acquire 吧。图个省事儿，图个放心。等到真的有要求再手写汇编。这背离了设计 memory_order_consume 的 初衷，但是也是现阶段比较实际的办法。\n\n\n前面为了让程序更好懂，这些 memory barrier 都和一个 atomic&amp;lt;&amp;gt; 变量放在了一起了。毕竟活生生的变量对于描述程序的逻辑才是有意义的。memory barrier 只是用来保证执行的顺序而已。它就像 xchg 的前缀一样。但是我们也可以直接加入 memory barrier：atomic_thread_fence。前面如果不能用 CAS 的话，我们可能就只能用这一招了。\n\n\n\nstd::atomic_thread_fence(order)\n\n\n\n这样直接的 memory barrier，能和 atomic&amp;lt;&amp;gt; ，也能和其他 atomic_thread_fence&amp;lt;&amp;gt; 一起使用。效果是相同的。\n\n\n\n\n\n后记\n\n\nCeph 是个分布式的存储系统。它里面客户端访问的数据被叫做 object。我们用 PG 来对 object 分组，把属于同一个 PG 的 object 安排在集群里的一组磁盘上。每个磁盘上都有一个服务，叫 OSD，来管理这个磁盘，同时与集群还有客户端联系。所以客户端在访问自己读写的数据时，就会直接和负责存储的服务用 TCP 进行通讯。\n\n\n一个客户端对 OSD 的读写是原子的，这个是底线。那么我们是不是也可以乱序执行客户端发过来的读写指令呢？如果客户端发过来三个消息\n\n\n\n\nwrite(obj1, data), write_xattr(obj1, xattr), read(obj1), read(obj1)\n\n\nread(obj2)\n\n\nwrite(obj3), write_omap(obj3, omap)\n\n\n\n\n这里还有一些背景，客户端和 OSD 之间是通过 Ceph 自定义的 RADOS 协议联系的。RADOS 协议中一来一回的叫做 message，而用来访问 object 的 message 叫做 MOSDOp。它可以包含一系列的读写访问，但是同一个 MOSDOp 中操作的对象只能是同一个。这很大程度上限制了一个 message 里请求的可能性。因为在执行绝大多数访问 object 的操作之前，OSD 都需要读取这个 object 的 OI，即 object info，获取它的一些元数据，比如这个 object 的大小，版本， 快照信息。有的时候因为操作的 offset 越界，这类操作就被作为无效请求，给客户端返回个错误，或者干脆忽略这个无效的请求。但是不管如何，写请求一般来说仍然是比读请求慢的，对于多副本的数据池，我们要求这些副本是一致的。这里的一致性问题和内存一致性类似，其实也可以展开说我们留着以后聊。对于 erasure coded 的数据池，我们也要求 k+m 都落地了才能返回，这些都很花时间。\n\n\n之前在 crimson 例会上，曾经和同事讨论过 Ceph 是不是能乱序访问，Sam 说 RBD 的访问模式基本不可能有这种情况。因为 librbd 客户端的一个 message 里不会同时出现对同一个 object 的读操作和写操作。是的。块设备的访问模式和内存是完全不一样的。就算使用 RBD 的操作系统或者应用程序把一块内存 mmap 到这个设备，也会把读写尽量 cache 在缓存或者内存里面，除非不得已，比如说上层一定要 fsync。但是即使这样，fsync 所对应的 MOSDOp 也不会包含对所涉及的 object 的读操作。\n\n\n那么我们换个问题，有没有可能，或者说应不应该把对 obj1、obj2 和 obj3 的访问乱序执行呢？先假设这几个 object 都同属于一个 PG，毕竟两个连续的操作的 object 同属于一个 PG 的可能性很小。就像是学校里面上公共选修课，随机点名的时候，你和室友都被抽中一样。\n\n\n我们从有没有可能开始吧。librados 提供了两种操作，一种是同步的，另一种是异步的。同步的操作执行完毕才能返回，也就是说如果这个函数返回了，那么就可以认为集群已经把请求里面的写操作作为一个事务写到磁盘上了。异步的函数调用直接返回，不等待执行的操作落地。另外调用方需要给异步调用一个回调函数，这样 librados 就知道这个操作完成的时候该怎么处理了。前者相当于天然的 sequential consistency。后者就给 OSD 以可乘之机，在保证操作原子性的前提下，有一定的自由度可以调度队列里面的操作。\n\n\n在这个框架下，还有个很强的限制。Ceph 有个测试叫做 ceph_test_rados，它根据配置向集群发送一系列异步的读写操作，每组操作都有个单调递增的编号。异步操作完成的时候，根据这个操作的编号我们能知道它的返回是不是顺序的。如果不是顺序的，这个测试会失败。换句话说，这个测试要求 sequential consistency。如果这个测试是合理的，或者即使不合理也是无法变动的，比如说更高层的客户端，比如说 qemu 的 RBD 插件把这个作为一个协定，并且依赖这个行为，那么我们就必须比 amd64 更自律才行。当然，我们也可以异步返回，然后再在 librados 这一层再让新的调用阻塞在老的调用上，使得它们的返回看上去像是顺序的。\n\n\n为了满足 sequential consistency 的要求，我们有两个选择。\n\n\n\n\n执行上的 sequential consistency。这很明显是最稳妥的办法。把罪恶扼杀在摇篮之中，通讯层甚至可以等 MOSDOp 完成之后再读取下一个 message，但是这和咸鱼完全同步有什么区别呢？客户端之所以选择异步操作就是希望更高的并发啊。当然，即便如此，异步的操作仍然是有意义的。异步往 OSD 发射指令，而在 OSD 上顺序执行可以避免网络上的延迟。\n\n\n表现上的 sequential consistency。这个说法并不严谨，其实表现上的顺序一致也需要执行层面的支持。我们权且把第一个的选择作为最直接了当的、完全阻塞的实现吧。这个选择的执行需要更小心一些。因为执行层面上可能的乱序，我们可能需要考虑下面几种读写序列的乱序\n\n\n\nstore-store\n\n\n\n写同一 object。倘若 object 没有支持快照，只要最终的结果和顺序执行的结果一样，即可以认为这两次操作是顺序的。况且要是能在 object store 层面上实现 write collapsing 或者 write combining，岂不是一桩美事？\n\n\n写不同 object。回到老问题，这两个写操作是不是带有 acquire-release 语义？会不会有老马和老王的问题？\n\n\n\n\n\nstore-load\n\n\n\n读写同一 object。和前文中讨论的 CPU 的读写指令不同，RADOS 里面 store 和 load 指令的操作数都是立即数。所以不存在读写数据本身的数据依赖的问题。但是如果读的 extent 和之前写的 extent 有重叠，那么我们就必须小心了，至少需要先把写指令下发到 object store，然后由 object store 把 cache 修改了，并标记成 dirty 才能算是这个操作提交完成。这样等到执行 load 指令的时候才能读到最新的数据。\n\n\n\n\n\nload-store\n\n\nload-load\n\n\n\n\n\n\n\n十年前，用 mutex 和 condition_variable 就能解决很多多线程的问题。在今天，这些同步原语仍然很重要。但是如果我们对高并发有更高的追求，就需要更深入了解多核系统中的无锁编程，在体系结构上多理解一些 CPU 和内存的交互，这样对工作会更有帮助。\n\n\n"
} ,
  
  {
    "title"    : "longjmp 和 setcontext",
    "category" : "",
    "tags"     : " arch, x86",
    "url"      : "/2020/08/09/setjmp-setcontext.html",
    "date"     : "August 9, 2020",
    "excerpt"  : "\n\n\nlongjmp() 和 setcontext() 之间的性能孰优孰劣？\n\n\n这篇文章起源于 seastar-devel 上的一个 讨论。在开始之前，我们先说一下协程的背景。因为讨论涉及特定的操作系统、处理器系统架构以及调用约定，如果没有特殊说明的话，下面都以 sysv, amd64 和现代的 Linux 为例。\n\n\n\n\n协程的由来\n\n\ncoroutine 或者 cooperative threads，中文常常叫协程。在 Linux 里面，常规的调度单位是 LWP (light wei...",
  "content"  : "\n\n\nlongjmp() 和 setcontext() 之间的性能孰优孰劣？\n\n\n这篇文章起源于 seastar-devel 上的一个 讨论。在开始之前，我们先说一下协程的背景。因为讨论涉及特定的操作系统、处理器系统架构以及调用约定，如果没有特殊说明的话，下面都以 sysv, amd64 和现代的 Linux 为例。\n\n\n\n\n协程的由来\n\n\ncoroutine 或者 cooperative threads，中文常常叫协程。在 Linux 里面，常规的调度单位是 LWP (light weight process)。 NPTL 实现下，LWP 和用户线程在数量上是一对一的对应关系。所以，以 Linux 为例，有这么几个问题:\n\n\n\n\n缺省 8MB 的栈空间。虽然 8M 只是虚拟地址的空间，但是内核里面在分配栈空间的时候必须立即分配对应的页表，这个开销是无法避免的。\n\n\n线程调度的时候必须借助内核。换言之，上下文切换也会引起一些开销。\n\n\n因为内核调度线程的不可预期性，比如一个线程把自己的时间片用完了。内核可能会把它调度出去，把另一个就绪的任务换进来。为了保证数据和逻辑的一致性，在一些可能产生 racing 的地方，必须加锁。而锁的引入进一步影响了性能和并发的粒度。\n\n\n\n\n所以为了避免这些问题，我们引入了协程的概念，在用户态实现 m:n 的映射。让线程自己调度自己。正是因为这种用户态线程是互相协作的，只有当一个线程主动把 CPU 让出来，另一个已经就绪的线程才能继续运行。这也是为什么协程叫做&quot;`协程`&quot;的原因。\n\n\n\n\n协程的基本要素\n\n\n协程要能自己调度自己，需要满足下面几个要求\n\n\n\n\n协程在让出 CPU 的时候，需要保存现场。这样当它以后继续执行的时候，能记得起来之前在做什么，然后继续当时未完成的任务。\n\n\n协程在让出 CPU 的时候，能找到另外一个就绪的协程，恢复它当初保存的现场。帮助它回忆起来之前的事情。\n\n\n\n\n这有点像晚上睡前看完书的时候，大家会在书里面夹一个书签，记住看到哪一页了。下次再翻开书的时候，找到书签的位置就能从上次停下来的地方继续看。只不过一个系统里面可能会有成百上千个线程，每个线程都有自己的&quot;`书签`&quot;。一般来说，协程库提供两个基本的操作：\n\n\n\n\nyield / swap out: 把控制权让出来，保存自己的状态。也就是插书签。\n\n\nresume / swap in: 获取控制权，恢复自己的状态。也就是根据书签的位置，继续读书。\n\n\n\n\n\n\n协程的实现\n\n\n书签和上下文\n\n书签保存的信息只有一个页码。但是对于一个线程来说，它在 CPU 上执行的状态对应着更多的信息。我们先看一个特例&amp;#8212;&amp;#8203;子函数的调用。假设我们在 main() 里面调用之前定义的函数 func()。\n\n\n\nint main()\n{\n  func();\n}\n\n\n\n为了让 func() 返回时，main() 能继续它当时未尽的事业，很明显，它需要\n\n\n\n\n在跳转到 func() 的起始地址之前，保存当下的 %ip。\n\n\n再把 %ip 改成 func() 的地址。\n\n\nfunc() 在返回的时候，需要把 %ip 恢复成之前保存的 版本。\n\n\n\n\nx86 很贴心的提供了 CALL 和 RET 两个指令。前者把 %ip 压栈，再根据 CALL 的参数更新 %ip。要是大家还能回忆相对寻址、绝对寻址的话，CALL 是支持这些寻址方式的。要是目标地址不在一个 %cs 段，它还能把当前 %cs 也一并保存了。RET 执行的是相反的功能。它把栈上的地址恢复回 %cs 和 %ip，如果 RET 还有参数的话，还顺带着把栈上的垃圾清理一下，也就是退栈。通常来说，调用方会把一些参数放到栈上，而参数的个数一般是确定的。所以被调用方在返回的时候，把那些参数从栈上清除也是理所当然的事情。\n\n\n可以说 CALL 和 RET 给了线程订了一张往返票，让它从一个地方走到另外一个地方出个差，然后再回来。 除了 %ip，根据 amd64 或者 x86-64 的 ABI 调用规范，在函数调用的时候，下面的寄存器是调用方负责的:\n\n\n\n\n​%rax\n\n\n​%rcx​\n\n\n​%rdx\n\n\n%rdi\n\n\n%rsi\n\n\n%r8 到 %r11\n\n\n\n\n换句话说，如果调用方觉得它无所谓函数返回之后这些寄存器的状态是否改变了，那么它完全可以选择不保存它们。其中，函数调用的前六个参数保存在 %rdi, %rsi，%rdx，%rcx, %r8d, %r9d。\n\n\n而被调用方则有义务保存：\n\n\n\n\n%rbx\n\n\n%rbp\n\n\n%rsp\n\n\n%r12 到 %r15\n\n\n\n\n也就是说，在函数返回之后，这些寄存器的值应该保持不变。这些要求定义了一个函数调用的行为规范，确保编译器能编译出有效率的代码，而不用花时间分析被调用的函数到底修改了哪些寄存器。所以一般来说，我们的 yield 实现也应该遵守这些基本的规范，保证调用方行为不受到干扰。\n\n\n那么从一个线程到另外一个线程呢？除了函数调用规范要求的那些寄存器，还有哪些状态需要保存呢？\n\n\n\n\npthread(7) 总结了一下。它说，POSIX.1 要求一个进程里面的线程有共同的一系列属性，比如说 process ID、uid、文件描述符以及 signal handler。它们也有自己的独立的属性，比如 errno、signalprocmask 还有 sigaltstack。这些属性有着各自不同的实现方式。\n\n\n\nerrno 它是 libc 实现的接口，让 libc 的函数能告诉调用方具体的错误号。 libc 一般把它保存在 %fs 段里面。但是如果我们不需要:\n\n\n int ret  = fstat(...);\n yield_to(another_thread);\n if (ret != 0) {\n   perror(&quot;fstat failed&quot;);\n }\n\n\n\n那么就没有必要保存和恢复 errno 了。\n\n\n\nsigprocmask 如果调度的线程 sigmask 不一样，那么我们的确需要保存恢复它们各自的 sigprocmask。但是如果它们的 sigmask 都一样的话，就可以不用管这个属性了。sigaltstack 也是类似的。\n\n\n\n\n\n函数调用使用栈来保存返回地址，传递一些参数。而每个线程都有自己的栈。在切换线程的时候，%rsp 和 %rbp 也需要指向新的线程自己的栈。\n\n\n浮点处理器的运行环境。这包括一系列寄存器。可以参考 FSTENV 和 FLDENV 这两个指令。\n\n\n\n\n\nlibc 的书签\n\n我们管这些林林总总的状态叫做&quot;`上下文`&quot;。 为了保存和恢复上下文，libc 提供了\n\n\n\n\nsetjmp() 保存当前的 %rbx, %rbp, %r12, %r13, %r14, %r15, %rsp, %rip 到指定的 jmp_buf 中。\n\n\nlongjmp() 从指定的 jmp_buf 恢复 %rbx, %rbp, %r12, %r13, %r14, %r15, %rsp 中。\n\n\n\n\n可以参考 musl-libc 的实现。可以说 setjmp() 和 longjmp() 是相当简练的。只提供了两个功能，一个是记录当前的位置，另一个是跳转到指定的位置。\n\n\n但是 glibc 的 longjmp 还更啰嗦一些，它在调用平台相关的__longjmp()之前，还调用了\n\n\n\n\n_longjmp_unwind()\n\n\n__sigprocmask()\n\n\n\n\n\nlibc 的 context\n\n虽然 setjmp() 和 longjmp() 很简练。但是它们只能允许我们回到一个已知的地方。这和之前书签的例子很像，如果之前没有用 setjmp() 得到 jmp_buf，那么是无法跳转到 jmp_buf 指示的地方的。如果我们希望实现协程的话。假设我们一开始启动了一个 POSIX 线程，当这个线程执行的函数希望 yield，把执行权交给另一个任务，而这个任务还从没执行过。那么 不手动修改jmp_buf 是无法实现这个功能的。读者可能会说，如果开始这个新任务的函数之前执行过，那么是不是在函数开始的时候用 setjmp()加个书签就可以了呢？这样会导致两个协程互相重用一个栈，导致原来的线程在返回时可能会读到错误的数据，也可能干脆跑飞掉。\n\n\n所以 glibc 干脆提供了下面这几个函数:\n\n\n\nint getcontext(ucontext_t *ucp);\nint setcontext(const ucontext_t *ucp);\nvoid makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);\nint swapcontext(ucontext_t *oucp, const ucontext_t *ucp);\n\n\n\n提供了比 setjmp() 和 longjmp() 更强大的功能。\n\n\ngetcontext() 记录当前的上下文。这个上下文可以作为一个模板，如果我们需要让它使用另一个栈，没问题！如果我们想让调度它的时候，运行 serve_request()，好的！对了，这个函数还应该有几个参数，嗯，我想在这里设置这些参数&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;当然可以！这些函数满足了用户对协程的所有要求。但是它们也带来了一些问题\n\n\n\n\n过于完整的线程支持。setcontext() 和 swapcontext() 除了做了 longjmp() 的工作，还：\n\n\n\n用系统调用设置 sigprocmask\n\n\n设置 %fs，这是段寄存器。TLS 的变量都保存在这里面。\n\n\n\n\n\n不跨平台。 POSIX.1 已经把这几个函数去掉了。musl-libc 干脆[12][不实现他们]。\n\n\n把 context 串起来。调用当初设置的函数，要是执行完了，看看 uc_link，要是还有下一个 context。有的话，再调用 setcontext()，开始执行它。\n\n\n\n\n\nSeastar 的 thread\n\nSeastar 为了避免使用重量级的 swapcontext() 进行上下文切换，只是在开始的时候用 getcontext() 和 makecontext() 来初始化 context，而在平时调度的时候继续用 setjmp() 和 longjmp() 的组合。\n\n\n首先，每个用户态线程都有自己的 context，这个 context 包含\n\n\n\n\n一个 128KB 的栈\n\n\n一个 jmp_buf\n\n\n指向原来的 context 的指针\n\n\n\n\n在这里，ucontext 就像是一个通向 jmp_buf 的跳板。\n\n\n\n\n在初始化用户态线程的时候，Seastar 新建一个 ucontext，让它使用自己的栈，并把它指向一个静态函数 s_main()，这个函数的参数其实就是 thread_context 的地址，所以它得以调用 this-&amp;gt;main()。后者才会调用真正的任务函数。\n\n\n每个线程都用 TLS 保存着自己的 thread_context ，在工作线程调度到新的任务的时候，新的任务对应着新的 thread_context 实例。在这个新的 thread_context 开始运行之前，我们把当前的 context 作为成员变量保存在新的 thread_context 里面。然后用 setjmp() 把当前上下文保存在原来的 context 中。这时保存了原来 context 的上下文。\n\n\n不过我们并不保存这个新建的 ucontext，我们的目标是调度到 this-&amp;gt;main()。接下来用 setcontext() 跳转到这个 ucontext 完成调度。\n\n\n下一次要 yield 就简单很多，只需要 setjmp(this-&amp;gt;jmpbuf)，然后 longjmp(link-&amp;gt;jmpbuf) 就行了。\n\n\n类似的，如果是 resume，则是相反的操作。\n\n\n如果希望销毁这个用户态线程，则直接 longjmp(link-&amp;gt;jmpbuf) 。跳过保留上下文的步骤。\n\n\n\n\n\nBoost::context\n\nBoost::context 用汇编实现了平台相关的 fcontext_t ，它的性能据说比 ucontext_ 高一到两个数量级 。fcontext_ 保存的上下文 有\n\n\n\n\nMXCSR 中的控制字。x86 上 SSE/SSE2 用于保存浮点控制和状态的寄存器\n\n\nFPCR 即 X87 FPU control word。\n\n\n\n\n这两个寄存器状态和 Intel TSX 机制有关系。TSX (Intel Transactional Synchronisation Extensions) 是 Intel 实现的硬件内存事务机制，可以粗略地理解，它使用 L1 cache 跟踪读集合和写集合，如果出现冲突的话，就放弃当前核上的修改，不把它刷到内存里面去，导致不一致。我们可以在另外一篇文章里面继续讨论内存一致性、可见性和多核系统里面乱序执行的问题。不过这里保存它们的原因是因为，如果浮点 TSX 的事务中发现浮点状态字有变化，那么这个事务肯定会 终止。所以为了支持 TSX，Boost 也保存这些浮点寄存器。顺便说一下，内核里面是不能用浮点操作的。所以那边我们不需要关心这种问题。\n\n\n基于这套实现，Boost 实现了自己的协程库。\n\n\nseastar-devel 上的 讨论也是围绕着这一点。 Christian 觉得手工实现 longjmp() 会比较高效。Avi 提到当初他也考虑过 Boost::context。因为它比较简单明了，同时没有 glibc 中 _longjmp_unwind() 和 __sigprocmask() 的开销，所以对于广大的 glibc 用户来说，使用 Boost::context 性能会更好一些。 不过 Boost::context 在 1.55/1.56 中的实现还不成熟。为了精炼版的 longjmp()，只能有两条路，\n\n\n\n\n要求用户使用新版的 Boost\n\n\n把 fcontext_t 使用的汇编代码移植到 Seastar 里面去。\n\n\n\n\n不过 Avi 提到，glibc 中的 longjmp() 在上下文切换操作中占用的时间其实并不算多。所以就没有必要手撸汇编了。\n\n\n\n"
} ,
  
  {
    "title"    : "开场白",
    "category" : "misc",
    "tags"     : " ",
    "url"      : "/misc/2020/08/08/hello-world.html",
    "date"     : "August 8, 2020",
    "excerpt"  : "\n为了记录，也为了提高自己，写个 blog。\n",
  "content"  : "\n为了记录，也为了提高自己，写个 blog。\n"
} 
  
  
  
]
