[
  
  {
    "title"    : "从 metaslabs allocator 说起",
    "category" : "",
    "tags"     : " fs",
    "url"      : "/2021/05/27/alloc-fs.html",
    "date"     : "May 27, 2021",
    "excerpt"  : "\n\n\n系统设计里面有很多开放问题。解决问题的策略是基于经验不断演进的。\n\n\nceph 的 bluestore 有好几种 allocator。其中的 AvlAllocator 基本就是 ZFS 的 df (Dynamic Fit) Block Allocator 的 C++ 移植版。所以要清楚 AvlAllocator，就绕不开 ZFS 的前辈。\n\n\n那么什么是 df allocator 呢？它和 metaslab 又有什么关系呢？故事要从 slab allocator 说起。\n\n\n\n\nsl...",
  "content"  : "\n\n\n系统设计里面有很多开放问题。解决问题的策略是基于经验不断演进的。\n\n\nceph 的 bluestore 有好几种 allocator。其中的 AvlAllocator 基本就是 ZFS 的 df (Dynamic Fit) Block Allocator 的 C++ 移植版。所以要清楚 AvlAllocator，就绕不开 ZFS 的前辈。\n\n\n那么什么是 df allocator 呢？它和 metaslab 又有什么关系呢？故事要从 slab allocator 说起。\n\n\n\n\nslab allocator\n\n\n我们知道 allocator 设计需要解决的问题就是在高效分配内存空间的同时最小化碎片。如果我们使用 first-fit 在空闲列表里面找指定大小的空闲块，搜索是快了，但是它可能产生更大的内部碎片。best-fit 虽然看上去很好，而且它会产生很小的内部碎片，这些碎片就像下脚料一样，很难利用了，因此性能其实也不见得就能改进很多。buddy 算法中所有的内存块都按照二的幂向上取整，这样方便搜索和方便回收，和合并伙伴内存块。但是这样也会造成相当的碎片，而且在频繁地内存分配和回收的时候，积极地合并策略也会浪费 CPU cycle。\n\n\n根据观察，系统里面经常会分配释放特定大小的内存块，比如说一个异步的分布式系统里面可能会分配大量的 mutex 来细粒度地管理它的 inode，每次构造和析构都对应着内存子系统的分配内存和释放内存的操作。有一个解决的办法就是维护一个专门的列表，保存特定大小的 extent。加入刚才说的 mutex 大小是 43 字节，那么我们可能就会用一个列表保存一系列大小为 43 字节的内存块。这样分配和释放这样大小的内存的速度就是 O(1) 的。这种列表根据具体的应用场景可以有好几个，要是 inode 的大小是固定的话，inode 也可以有个专门的列表。但是这样处理也带来了问题，到底应该为这种专用列表分配多少内存呢？还有一个重要的问题，就是内核里面频繁地创建和析构内核对象本身也会耗费大量的 CPU 资源，这种开销甚至比为这些对象分配内存的开销还要高。slab allocator 应运而生。\n\n\nslab allocator 最初是 Jeff Bonwick 为 Solaris 内核设计的，后来这个算法也用到了 zfs 和其他操作系统里面。slab 算法中的每个 slab，都对应着一类固定大小的对象。比如说 slab#1 就专门服务大小为 14 bytes 的对象，slab#2 对应 23 bytes 对象。在这个基础上，我们还有专门类型的 slab，比如专门提供 inode 的 slab，或者专门提供 mutex 的 slab，它们可以省去初始化和销毁对应类型对象的开销。每个 slab 由一个或多个物理地址连续的内存页构成。slab 从这一系列内存页为给定大小的对象分配内存。“专用列表”的思想其实是一种 cache，用来缓存特定大小内存块的分配信息。kmem_cache 中的 slab_partial 是一个 slab 的双向链表，其中每个元素都是一个 slab。当某个 slab 所有的 对象都回收的时候，这个 slab 就从 slabs_partial 移动到了 slabs_free 里面去，如果一个 slab 里面所有的页都分配了，那么这个 slab 就会加入 slabs_full。分配内存的时候先从 slabs_partial 里面找，找不到的时候才看 slabs_free。这样分配对象的时候更高效一些。\n\n\n\n\n\n\n\n如果你在看的是 Linux，很可能你看的版本里面的 slab 已经改进很多，不大一样了。\n\n\n\n\nmetaslab allocator\n\n\nZFS 作为当初所说的终极文件系统，包揽了从文件系统，卷管理系统，到块设备管理的所有工作。它引入了一个概念叫做 zpool，所以不管是裸设备还是 raid 设备都可以一股脑地扔到这个池子里，交给 ZFS 全权管理。所以 ZFS 的 allocator 要分配一个 extent 有三步：\n\n\n\n\n选择设备 (dynamic striping): 目标是让各个设备的空间使用率尽量平均。为了达成这个目标\n\n\n\n稍微倾向于选择使用率低的设备\n\n\n如果其他因素都差不多，那么用 round-robin。但是粒度需要合适。因为如果粒度大了，比如每次都分个 1GB，那么顺序读写的时候，请求都会往一个设备上招呼，设备间的并发性就没法用上了。但是粒度太小也不好，比如说分了 4KB，就找下一个设备了，那么 buffer 和 cache 的效果就会大打折扣。zfs 发现 512K 是个比较合适的值。\n\n\nZFS 的数据在刷到数据盘之前，会先以 ZIL (ZFS Intent Log) 的形式先落盘。这有点像 bluestore 里面 journal 的设计。ZFS 希望能通过引入这个 write cache 的机制，让写操作的数据先保存在比较快的设备 (SLOG) 上，之后再刷到目标设备，这样客户请求可以更快地完成。在需要低延迟低大量写数据时，就会使用 round-robin 调度设备，用类似扫射的方式，充分利用多设备的带宽。\n\n\nstriping 的策略可以根据数据的类型不同而不同。比如大块的顺序访问，小的随机访问，生命周期比较短的数据，比如刚才说的 ZIL，还有 dnode 这种保存 metadata 的数据。其中 dnode 有些类似普通文件系统里面的 inode。这些都是值得进一步挖掘和研究的地方。\n\n\n如果发现有设备性能不好，就应该尽量不使用它。\n\n\n\n\n\n选择 metaslab: 每个设备都被切分成多个的区域，每个区域就是一个 slab。slab 的数量一般在 200 个左右。为什么是 200 个？其实也没有做很多分析。所以这个数字可能不是最优的。metaslab 0 在最靠外的磁道上，metaslab 200 在磁盘最靠里的磁道。每个 metaslab 都有个对应的 space map 用来跟踪 metaslab 的空闲空间。space map 是一个日志，记录着分配和回收的操作。所以分配空间的时候就会在 space map 最后面加一条记录，说明分配了哪个 extent，回收的时候也类似。需要注意的是，如果 space map 还不在内存里面，就需要从硬盘的 space map 日志重建。\n\n\n\n我们假设磁盘的扇区在磁道上分布基本是均匀的，而磁盘转动的角速度是恒定的。所以在外圈柱面 (cyliner) 的数据分布会比内圈的数据分布更密集，比例就是磁道的半径。LBA 的寻址模式下，地址越低的 LBA 地址，对应的柱面就越靠外面。所以为了访问速度考虑，我们更希望用 LBA 地址更低的 metaslab。\n\n\n\n\n\n选择 block: ZFS 确定 metaslab 之后，就会从这个 metaslab 里面分配 block 或者说 extent。它首先从磁盘上读取对应的 space map，然后重放它的分配和回收记录，用来更新内存里面用来表示空闲空间的 b-tree，树里面的节点对应空闲的 extent，树按照 extent 的 offset 排序。有了这个树就可以高效地分配连续的空间。同时它也是一个压缩 space map 的手段。如果分配和回收的操作很多互相抵消了，换句话说，如果树的规模很小，那么 ZFS 会重建硬盘上的 space map，把它更新成内存里面那个更小的版本。space map 的设计有这么几个好处\n\n\n\n不需要初始化。一开始的时候，树里面只有一个 extent，表示整个设备是空闲的。\n\n\n伸缩性好。无论管理的空间多大，内存里面会缓存 space map 的最后一个 block。这一点是 bitmap 望尘莫及的。\n\n\n性能没有痛点(pathology)，即不会因为特定的使用模式造成性能急剧降低。不管是分配和回收的模式怎样，space map 的更新都很迅速。不管是 B-tree 还是 bitmap，在随机回收的时候，对数据结构的更新也是随机的，而且会产生很多写操作。虽然我们可以推迟更新下面的数据结构，把最近释放的 extent 保存在一个列表里面，等到这个列表太大了，再把它排序压缩，写回下面的 B-tree 或 bitmap，以期更好的性能，和写操作的局部性。但是 space map 在这方面基本没有影响，因为它本身就是个 free list。它记录 free 的方式就是写日志。\n\n\npool 很满或者很空的时候，space map 的都很快。不像 bitmap 在很满的时候搜索空闲块会更花时间。\n\n\n\n\n\n\n\n其实还有第四步，如果 metaslab 里面没有能满足的 range，就选择一个新的 metaslab。然是如果根本没有能满足要求的 metaslab，而且也检查过了所有的设备。ZFS 就开始 gang！“gang” 的意思就是把这个大的请求拆解成多个不连续的小的请求，希望它们合起来能满足要求。所谓“gang”也有点三个臭皮匠顶一个诸葛亮的意思。但是这是 allocator 的最后一招了。不到万不得已，allocator 不会 gang，因为这样会产生非常多的碎片。\n\n\n\n\n\n\n\n\n\n\n\n\n\n早先 ZFS 早期使用 AVL 树来保存 space map，但是后来因为 AVL 树太耗费内存了，每个节点都需要额外用 48 byte 保存 AVL 树需要的信息，每个 extent 都有自己的节点，所以对于海量的小 extent，这样的开销是巨大的。所以 ZFS 后来改用了 b-tree。至于为什么一开始选择 AVL。其实也没有什么特别的考虑，主要是作者在实现 metaslab allocator 的时候，Solaris 内核里面已经有 AVL 树了，所以就用了它。理论上说，红黑树也是可以用的。只要它里面的元素是有序的就行。\n\n\n\n\n\nspace map\n\nspace map 在内存里面由 ms_tree 和 ms_size_tree 表示。其中 “ms” 是 MetaSlab 的缩写。两者保存的是同样的信息。\n\n\n\n\nms_tree 中的空闲空间是按照它们的地址排序的。这样方便合并相邻的 extent。\n\n\nms_size_tree 则是按照大小排序的。这样可以根据需要 extent 的大小来搜索。\n\n\n\n\n在 Paul Dagnelie 的 Metaslab Allocation Performance 里面提到，为了减少内存的压力，甚至可以在 ms_size_tree 里面保存部分的 range。因为对于比较小的 alloc 请求来说，顺着 cursor 找，一般来说很容易在放弃之前找到足够大的 extent。所以只要 ms_tree 里面能找到就够了。让 ms_size_tree 保存比较大的 range，那些 extent 才是比较难找到的。\n\n\n\n选择 range/extent/block 的策略\n\n这些策略使用 cursor 记录上次分配的位置，希望下次分配的时候，用 first-fit 的策略从上次分配的位置开始找，希望能紧接着在上次 extent 的后面分配新的空间。这样当大量写入数据的时候，下层的块设备能把这些地址连续的写操作合并起来，达到更好的性能。这对于磁盘是很有效的优化策略，对 SSD 可能也能改进性能。毕竟，谁不喜欢顺序写呢。\n\n\nCF (Cursor Fit) Allocator\n\n这个算法只用了两个 cursor。\n\n\n\n\n根据 ms_size_tree 找到最大的一个 metaslab\n\n\n让 cursor 和 cursor_end 分别指向 metaslab 的两端\n\n\n每次分配新的空间都往前移动 cursor，直到 cursor_end。这表示 slab 里面的空间用完了，这时候就找一个新的 slab。\n\n\n\n\n\nDF (Dynamic Fit) Allocator\n\n所谓 “dynamic” 是指算法会根据具体情况动态地在 best-fit 和 first-fit 两个算法中选择。这个算法用一个 cursor 指向上次分配 extent 结束的地方。\n\n\n\n\n如果 slab 的剩余空间小于设定值，就根据需要 extent 的大小，找够大的就行。\n\n\n如果剩余空间还比较大，为了局部性，首先继续上次结束的地方搜索。搜索的范围由 metaslab_df_max_search 限定，如果超过这个大小还找不到，就退化成按照大小搜索。只要找到和需要大小相同或者更大的 extent 就行。\n\n\n\n\n每次分配到 extent，都会推进 ms_lbas[bits_of_alignment] 让它指向新分配 extent 结束的位置。这样相同对齐要求的 extent 就会从相邻的位置分配出来，不过这并不能防止其他对齐大小的 extent 也出现在同一区域中。\n\n\n\nNDF (New Dynamic Fit / clump) Allocator\n\nclump，即“扎堆”。其实这个名字更能说明这个算法的用意。它希望主动地为请求的大小选择成倍的更大的空间，预期接下来会出现多个相同大小的请求。\n\n\n\n\n先在 ms_tree 里面找 [cursor, cursor+size) 的 extent，如果找到足够大的 extent。就把 cursor 往前移动 size\n\n\n找不到的话，就在 ms_size_tree 里面先找大小为 2metaslab_ndf_clump_shift 倍 size 的 range，等找着了，就把 cursor 指向它，以它作为新的基地，发展成为这种对齐 extent 扎堆的地方。当然，新“基地”的大小是按照当前 slab 的最大空闲空间为上限的。\n\n\n\n\n\n\n\n\nbluestore 里的 Avl Allocator\n\n\nAvlAllocator 基本上是 ZFS 的 DF Allocator 较早版本的 C++ 移植。它继续用 AVL tree 来保存 space map。但是不同之处在于，bluestore 里面的 AvlAllocator 并没有 gang 的机制。所以 AvlAllocator 必须自己实现它。\n\n\n"
} ,
  
  {
    "title"    : "Log-strucutured Filesystem 和垃圾收集",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/05/16/gc-fs.html",
    "date"     : "May 16, 2021",
    "excerpt"  : "\n\n\n垃圾需要分类处理，有用的东西更应该分类。\n\n\n\n\n引子\nZoned Storage 和 degragmentation\nF2FS\n\nGreedy\nCost-Benefit\nCAT\nATGC\n\n\nZoneFS\nBtrfs\nSPDK FTL\nLSM_ZGC\n\n问题\n方案\n\n\nSeaStore\n\nCache\nJournal\nSegmentCleaner\n\n\n\n\n\n\n\n引子\n\n\nSeaStore 是 Crimson 使用的存储引擎。它的目标是\n\n\n\n\n高性能\n\n\n全异步\n\n\n支持 ZNS 和...",
  "content"  : "\n\n\n垃圾需要分类处理，有用的东西更应该分类。\n\n\n\n\n引子\nZoned Storage 和 degragmentation\nF2FS\n\nGreedy\nCost-Benefit\nCAT\nATGC\n\n\nZoneFS\nBtrfs\nSPDK FTL\nLSM_ZGC\n\n问题\n方案\n\n\nSeaStore\n\nCache\nJournal\nSegmentCleaner\n\n\n\n\n\n\n\n引子\n\n\nSeaStore 是 Crimson 使用的存储引擎。它的目标是\n\n\n\n\n高性能\n\n\n全异步\n\n\n支持 ZNS 和高性能的存储介质比如 PMEM\n\n\n支持异构存储\n\n\n兼容 Ceph 现有的 object store 的语义\n\n\n\n\n可以看出来，SeaStore 很像一个文件系统。\n\n\n\n\n文件名就是 object store 里面 object 的 object id\n\n\n文件的内容就是 object 对应的数据\n\n\n文件的 xattr 和各种属性，就类似 object 的 omap 和 xattr\n\n\n当然文件还支持快照，这个和 object 的快照也很相似\n\n\n类似的还有 mount、umount 和 fsck 这类操作\n\n\n和文件系统一样，SeaStore 也有碎片的问题，所以我们也需要 defrag\n\n\n\n\n文件系统的设计可能有好多方面\n\n\n\n\n它像数据库: 需要高效地执行查询和修改的操作。对不同性质的访问模式也可以有不同的优化策略。\n\n\n它像 allocator: 需要有效地管理空间。比如说，分配空闲空间，跟踪使用的空间，释放不用了的区域。\n\n\n它也有 cache: 需要利用不同性质的存储介质，比如说利用低延迟的存储作为缓存，而用大容量的存储保存冷数据。\n\n\n它像调度器: 需要在服务前台请求的同时，也能兼顾后台的任务。所谓磨刀不误砍柴工。\n\n\n\n\n所以一篇文章很难讨论到所有的问题。我们先从垃圾收集说起。为什么？因为笔者正好有一本 《垃圾收集》。有点拿着榔头找钉子的意思吧。\n\n\n\n\nZoned Storage 和 degragmentation\n\n\n先说说“钉子”。目前 SeaStore 主要针对的存储介质叫 Zoned Namespaces SSD。ZNS flash 和 叠瓦盘(SMR) 都属于 Zoned Storage。后者因为读写性能不彰，消费级市场上大家避之不及。但是如果作为冷存储，性价比还是很高的。要是能在应用层结合性能更好的存储介质一起使用，综合下来性价比可能还会更好。但是它最大的问题在于，不支持原地 (in-place) 修改的，所有的修改操作都通过 copy-on-write 实现。整个磁盘被分成好几个区域 (zone)，每个区域都只能添加数据，不能重写已经写入的数据。但要是已经写入的数据被删除了，我们就要回收它们占用的空间。要是需要修改的话，就得复制一份新的。同样，也需要在复制完毕后，回收原来数据占据的磁盘空间。回收的时候，最少必须清除整个 zone。就像用活页笔记本记笔记，每页纸都从头写到尾，如果写坏了，想改一下呢？只能把那一页撕掉，换一张纸重新誊一遍。小块儿的橡皮擦在这里是不能使用的。\n\n\n为了帮助理解问题，还需要提一下 SSD 的访问模式。一块 SSD 板卡上一般有多块 NAND 存储芯片，这些芯片通过一定数量的 channel 连接到控制器芯片。所以 SSD 最小的并发单元就是就是单块 NAND 芯片，最大的并发数就是 NAND 芯片的数量。因为无法向一块 NAND 芯片同时发送多个请求。存储领域我们喜欢说 LUN (logical unit number)，在这里我们也把特定的 NAND 用 LUN 来表示。一个 channel 由多个 LUN 共享。而每个 NAND flash LUN 由高到低分成不同的层级\n\n\n\n\nchannel. channel 之间不共享资源，可以充分并发。\n\n\n\nLUN. 连接到相同 channel 的不同 LUN 之间可能会有数据依赖的问题，这一定程度上影响并发。\n\n\n\nplane: 一个芯片有 2 个 或者 4 个 plane。对某个 page 进行写操作的时候，需要对挂在不同 plane 的相同地址的 page 同时写。换句话说，一个 4k 的 page 事实上是映射到不同 plane 的 page 的。\n\n\n\nblock: 一般是 512 page。它是 flash 擦除操作的最小单位。\n\n\n\npage: 由四个 sector 构成，加上额外 (out-of-band) 的空间，用来保存映射本身的信息。sector 的大小一般是 4 KB。写操作的的时候，必须按照 page 在 block 里的顺序写。 每个 sector 由多个 cell 构成。而每个 cell 按照芯片的不同存储的比特数量也不一样。比如说 SLC 芯片是一个比特，MLC 是两个比特，TLC 三个，QLC 四个。这里需要解释一下 page pairing 的设计。根据 cell 保存比特的数量，由对应个数的 page 瓜分。换句话说，一个 QLC cell 对应着四个 paired page。只有所有的 page 都写好了，这次写操作才能算完成。所以对于一块有 4 个 plane 的 QLC 来说，每次写操作都必须同时写 4 个 plane，每个 plane 都因为 QLC cell 写操作的单位就是\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin_bytes_per_write = 4 /* 4 planes, 1 page per plan */ *\n                      4 /* 4 paired page for each cell */ *\n                      4 /* 4 sectors per page */ *\n                      4_KB /* 4KB per sector */\n                    = 256_KB\n\n\n\n因此，flash 上的物理地址就由 channel, LUN, plane, block, page 和 sector 构成。读的单位是 sector，而写的单位则是 page。\n\n\n顺便说一下，PMEM 的组织就相对扁平，它直接由多个 sector 构成。\n\n\n早在 Zoned Storage 出现之前，因为磁盘的机械特性，大家就已经开始思考怎么把随机写转化为顺序写了，以期提高存储系统的性能。很自然的想法就是把 metadata 和 data 作为 log 顺序地写入磁盘。这也是 log-structured filesystem 中 log 的由来。虽然 LSF 解决了随机写的问题，它也带来了随机读的问题。举个例子，我们在磁盘上保存了一个很大的文件，一开始的时候，文件在磁盘上是顺序写入的，所以它的物理地址是连续的。磁盘在顺序读取整个文件的时候不需要很多次寻道，所以 IO 会很快，带宽仅仅受限于磁盘的转速和磁盘接口的传输速度。但是随着时间流逝，用户先后在文件的不同位置作了一些修改。因为这些修改一样，也是作为 log 顺序写入磁盘的，它们的位置和文件原来的位置差得很远了。所以如果要顺序读取文件的话，\n\n\n\n\n这个读请求就可能会在逻辑地址翻译成物理地址的时候被拆分成为很多小的读请求，这极大影响了顺序访问的性能。\n\n\n更不用说因为地址映射表大小增长带来的额外开销。\n\n\n如果寻址是按照块对齐的，那么大量的数据片也会造成内部碎片。比如说，如果有的数据只有 7k，要是磁盘的块大小是 4k，那么最后那 3k 很可能就浪费掉了。\n\n\n损害了读写的局部性。让系统没有办法根据局部性进行优化。通常文件的读写都有一些局部性，文件系统可能会在应用要求读取某个文件开始的 4k 的时候，就把开始的 4M 都读进来了。它估计你很可能接下来也会读这 4M，索性我都读进来好了。反正\n\n\n\n闲着也是闲着\n\n\n这 4M 的物理地址是连续的，所以干脆一起读了\n\n\n\n\n\n\n\n记得小时候一个乐趣就是看 MSDOS 下面 defrag 程序不断移动的游标和闪动的小砖块。到现在 youtube 甚至还能找到一些怀旧的视频。它的作用差不多就是把同一文件保存在磁盘相邻的块。以减少磁头磁盘寻道的时间，同时通过把数据排列得更紧凑，把内部碎片挤掉，腾出来一些空闲空间来。可以说碎片整理是一种特定的垃圾收集。\n\n\n\n\nF2FS\n\n\nf2fs 的 GC 算法解决的问题就是找出一个牺牲的 segment，把里面的有效块保存下来，然后回收它。f2fs 的 GC 分为前台和后台。只有当空闲空间不够了，才会执行前台 GC。前台 GC 要求短平快，这样能最小限度地减少用户应用的卡顿。后台 GC 则更关注总体的效能，它是内核线程定期唤醒的时候执行的。请注意，f2fs 其实并不会手动迁移有效块，它在选出要回收的 segment 之后，把其中所有的有效块都读取到内存的 page cache 里面，然后把它们标记成 dirty。这样，内核在清 cache 的时候，就会顺便把这些需要保存的有效块也一并写入新的 segment 了。这样不仅能减轻对前台的压力，也可以把小的写请求合并起来。另外，值得注意的是，f2fs 同时使用六个 log 区域，分别用来保存冷热程度不同的数据。它甚至把数据分为 cold, warn 和 hot 数据。它由三种 block\n\n\n\n\ninode block\n\n\ndirect node: 用来保存数据块的地址。它的温度就比 indirect node 高。\n\n\nindirect node: 用来保存 node 本身的 id。这个 id 用来定位另外一个 node。\n\n\n\n\nf2fs 修改数据会更新数据块的地址，为了能让 inode 找到新的数据，它需要更新索引数据块的 direct node，因此 direct node 就是温度更高的 block。它的修改更频繁。\n\n\nf2fs 设计 GC 思路是让牺牲 segment 的代价最小，同时收益最高。评价策略有下面几种。其中 greedy 和 cost-benefit 是很经典的算法。\n\n\nGreedy\n\n有效块的个数。所以有效块最少的 segment 就是牺牲品。当 GC 在前台运行时，f2fs 就使用 greedy 策略来选择回收的 segment，这样需要读写的有效块数量最小，所以对用户请求的影响也最小。\n\n\n\nCost-Benefit\n\ncost-benefit 算法最早是 The Design and Implementation of a Log-Structured File System 一文中提出的。论文中设计的 Sprite LFS 文件系统当空闲 segment 的数量低于给定阈值(一般是几十)的时候就会开始 GC，直到空闲 segment 的总数超过另外一个阈值(一般取50到100)。理想情况下的分布应该双峰形的，两个大头分别是有效数据很少的 segment 和有效数据很多的 segment。前者是热数据，后者是冷数据。有效数据比例靠近 50% 的 segment 很少。这种分布对于 GC 来说是比较省心的。因为在回收的时候不需要迁移很多数据。但是使用 greedy 算法的模拟实验结果出乎意料，和局部性更低的测试相比，局部性高的测试产生的分布更差：大量的 segment 都聚集在中间。论文里面分析，使用 greedy 算法的话，只有在一个 segment 的有效数据比例在所有 segment 中最低的时候，它才会被选中回收。这样几轮 GC 之后，所有 segment 的有效数据比例都会降到回收阈值以下，甚至用来保存冷数据的 segment 的有效数据比例也是如此。但是冷数据 segment 使用率是比较坚挺的，它下降得比较慢。可以类比一个收藏家用来保存藏品的储藏室，除非收藏家突然改变了喜好，否则藏品是很少变化的。而冷数据本身也是有惯性的。所以，含有冷数据的 segment 即使大量保有无效数据，但是因为其稳定的使用率，不会被选中回收。\n\n\n根据这个观察，论文认为，cold segment 里面的空闲空间其实比 hot segment 里面的空闲空间更有价值。为什么呢？我们可以反过来看，因为和那些很快被修改得体无完肤的 hot segment 相比，cold segment 中的无效数据很难迅速增长。它在系统里面会保持相对较高的使用率更长的时间，我们不得已只能去不停地回收那些 hot segment。它们就像离村庄很近的耕地，因为比较近，所以大家都会更喜欢耕种它们。而埋藏在 cold segment 里面的空闲空间，就更难回收。这导致 cold segment 的使用率慢慢地降低，但是无法回收。这些顽固的 cold segment 的比例在一个访问局部性比较强的系统中可能会很高。因为在那种访问模式下，cold segment 中的冷数据的地位更难以撼动。请注意，这里说的局部性强指的是，重复修改的数据只占硬盘中所有数据的一小部分，绝大部分数据是不变的。如果局部性差的话，所有数据被修改的概率基本上是均等的。如果 GC 很积极地回收使用率低的 hot segment 的话，这样虽然当时迁移的成本很低，但是迁移之后当时被迁移的有效数据很快就被修改了，成为了新的无效数据。所以与其不断地迁移这种 hot segment，不如把它放一会儿，等养“肥”了，再 GC 不迟。这样反而效果更好，效率更高。那时候的有效数据的比例会更低。打个比方，就像一条运动裤已经有点脏了，另外一件衣服上面只有一个墨点，如果明天还要踢一场球，那么你说今天是洗裤子还是洗衣服呢？要不今天还是先洗衣服，明天就穿这条裤子踢球，等踢完球再洗裤子吧。\n\n\n为了能让 GC 更积极地回收这些 cold segment，我们必须在政策上倾斜，让 GC 觉得回收 cold segment 是更有利可图的。所以论文里面把 segment 里面的最新的数据的年龄也作为参数一起计算，segment 越老，那么它里面的的空闲空间至少也经历了那么长的时间。我们把它们解放出来的收益就是两者之积。用公式表达就是：\n\n\n\n\\[\\frac{benefit}{cost} = \\frac{(1-u) \\times age}{1 + u}\\]\n\n\n\n其中\n\n\n\n\nu 表示有效块在 section 中所占比例\n\n\nage 表示 section 中所有 segment 中，最近一次修改的时间。这个数字越大，意味着这个 segment 越 &quot;cold&quot;。用这个时间来估计\n\n\n1 - u 表示回收该 section 获得的收益，因为通过这次回收，能得到的空闲空间是 1 - u。\n\n\n1 + u 表示开销。1 表示我们需要读取整个被回收的 segment，u 表示我们需要往另外一个 segment 写入其中 u 那么多的数据。\n\n\n\n\n论文中的模拟实验表示，这样的策略可以使 segment 在使用率上呈现双峰分布或者哑铃状分布。即低使用率的 segment 和高使用率的 segment 都比较多，中间 segement 很少。这样的分布比较适合 GC。如果再能根据冷热数据进行聚类那么 GC 就会更高效。\n\n\nf2fs 在最初的 cost-benefit 上稍加改进，它用来计算 \\(\\frac{benefit}{cost}\\) 的 age 并不是 segment 里面 section 最大的那个，而是里面所有 section age 的平均值。\n\n\n\nCAT\n\nCost Age Times，这个算法基于 cost-benefit，它同时关注 flash block 的 wear leveling 问题。但是 ZNS SSD controller 已经帮我们处理了，所以这里不考虑这类算法。\n\n\n\nATGC\n\nATGC (Age Threshold based Garbage Collection) 是华为的开发者提出的算法，用来改进 f2fs 的 GC 效果 (effect) 和性能 (efficiency)。\n\n\n\n\n如果 segment 的年龄小于预设定的阈值，那么就不再考虑把它回收。因此可以避免回收太年轻的 segement，这种 segment 往往更新更频繁。\n\n\n使用 SSR (slack space recycling) 写日志的时候，尽量选择那些年龄相近的作为源 segment 和目标 segment。这样他们的更新频率可能更相近，有助于保持冷热数据的分离和聚类。\n\n\n\n\n\n\n\n\n\n\nf2fs 除了顺序写日志 (normal logging)之外，还能在空间不够的时候往无效的空间直接写 (threaded logging)，写进去的日志串起来一样用。这样虽然把顺序写变成了随机写，但是可以避免 GC 带来的卡顿，要是选择的 segment 有很大的空闲空间，也能顺序写一阵。这种随机写的做法就叫做 SSR。\n\n\n\n\n\n\n\n\nZoneFS\n\n\n\n\n\nBtrfs\n\n\n因为我们的目标是支持 flash，而 flash 本质上是不支持原地 (in-place) 修改的，所以所有的修改操作都通过 copy-on-write 实现。这也正是 SeaStore 的设计很大程度上受到了 Btrfs 影响的原因。而且最近 Btrfs 也开始加入对 zoned 设备的支持。\n\n\nTODO\n\n\n\n\nSPDK FTL\n\n\nTODO\n\n\n\n\nLSM_ZGC\n\n\n比较原始的 GC 算法可能仅仅关注 zone 里面有效数据的比例，如果一个 zone 里面的有效数据超过一定比例，我们可能就希望保留它，而回收那些充斥着垃圾数据的 zone。LSM_ZGC 一文 提出的 GC 算法希望解决下面几个\n\n\n问题\n\n\n\n冷热数据分离。因为将来在进行另一次 GC 的时候，也会根据数据的性质进行选择 zone。如果一个 zone 里面的冷数据或者热数据的比例是压倒性的多数，那么就可以更容易地决定这个 zone 的处理方式。比如说，如果是绝大多数是冷数据，那么可以放心地把数据搬到冷存储上。要是绝大多数是无效数据，那么这个 zone 就是很好的回收对象。反之，如果 zone 的使用率是 50%，那么做 GC 的时候就难以取舍了。\n\n\nGC 的时候，如果被选中回收的 zone 使用率很高，那么保存有效数据的开销会很大。因为典型的 zone 的大小是 256MB 或者 512MB，所以即使允许用户 IO 抢占后台的 GC 任务，GC 对总体性能产生的影响也会很明显。\n\n\n大量 4k 大小读请求和相对大的读请求相比，后者的性能要比前者要好很多。我们假设后者是 8K 到 128K 的IO。原因是，连续地址的读请求可以充分利用 ZNS SSD 内部的并发能力。因为文中说，一个 zone 里面的数据会被分散到不同 channel 连接的 LUN 上，所以读取更大的读操作就能更好地利用同时使用多个 channel 带来的并发性。但是我认为，使用更大的读操作是一种利用 inter-channel 并发的简便的方式。但是这并不等于说，发送多个分散的小的读操作的并发就不好了。这样做的缺点应该是请求的个数更多了。因为处理多个请求产生的开销也因而增加。但是要得到比较好的性能也需要权衡，如果 64MB 的区间里面，有效的数据只有 4K，那么就没有必要坚持读取所有 64MB 的数据了。\n\n\n\n\n\n方案\n\n按照在文中的设置，一个 zone 大小为 1GB，一个 segment 为 2MB，一个 block 为 4KB。这些设定很大程度上借用了 f2fs 的磁盘布局。为了提高读操作的效率，如果一个 segment 里面有效的 block 个数小于 16，那么就仅仅读取有效数据，否则就读取整个的 segment。\n\n\n我把这个思路叫做“大浪淘沙”。每个 zone 都处于下面四种状态中的一个。刚落盘的数据在 C0，以 segment 为单位统计，如果某一个 segment 的数据使用次数超过事先设定的阈值 thresholdcold，所有保存在这种 segment 中的有效数据都被收集到 C1C_zone，其他 segment 中的有效数据则悉数放到 C1H_zone 中。等到下一次 GC 的时候，无论是 C1H_zone 还是 C1C_zone 中，只要数据仍然有效，我们就把它们当作冷数据，一起放到 C2_zone。因为他们都经历了两次 GC 试炼，并且存活了下来。论文的作者期望通过这样的筛选机制，能够有效地区分不同生命周期的数据。其中，请注意，在这里，“冷数据”并不是指访问频次很低的数据，而是很少被修改或者删除的数据。它们经得起时间的考验，历久而弥坚。我们常说的 WORM (write once read many) 设备保存的就是冷数据。就是而热数据则是那种很快失效的数据，这种数据经常修改，它们生命周期很短，转瞬即逝，如同朝露一般。可以说，CPU 寄存器里面的数据就是热数据。所以我们在第一次 GC 的时候会借助保存数据的机会，先把冷热数据初步分开。这样如果要找热数据富集的牺牲品 zone 的时候，可以更容易地找到这样的 zone。但是第二次 GC 的时候就不再关注它们的使用频次了，而只是单纯地把第一代的幸存者都收集在一起。它们都被搬运过一次，而且顺利地活到了第二次 GC。所以它们完全有资格升级成“二级冷数据”。论文认为，第一代幸存者的生存周期相似，所以它们的空间局部性很可能也更好。比如说 leveldb 里面，同一个 SSTable 里面数据的访问频次可能不同，但是它们的生命周期是相同的，读写模式也一致。\n\n\n\n\n\n\n\n我们还可以更进一步。让经过冷热数据区分后活下来的 C2_zone 数据，升级进入 C3_zone。\n\n\n\n\n\n\n\n这样通过多次淘汰，我们就可以把数据分出三六九等，有的数据经历了很多次 GC 都巍然不动，有的数据最多只能到 C1H_zone 状态就黯然退场。前者都保存在同一个 zone 里面，所以 GC 的时候就不会因为它们和其他热数据挤在一起，而在腾地方的时候被迫迁移它们，因此就减少了不必要的开销。\n\n\n\n\n\nSeaStore\n\n\n因为 SeaStore 当前的目标是支持 ZNS。对它来说，每一张活页纸就是一个 segment。为了理解 SeaStore 怎么做垃圾收集，首先需要知道 SeaStore 里面的 journal 是什么。\n\n\nCache\n\n\n\nJournal\n\nJournal 就是日志，也就是 log-structured filesystem 里面的 log。在任意时刻，SeaStore 总是指定一个特定的 segment 作为当时写 journal 的专用 segment。\n\n\n\n\n\n\n\n\nZNS 是支持同时打开多个 zone 的。这样让我们可以按照写入数据的不同特性，选择不同的 zone，这样可以避免因为不同生命周期的数据相互交错，导致在 GC 的时候投鼠忌器，难以权衡。但是 SeaStore 现在为了简单起见，还没有利用这个特性。\n\n\n\n\n\n\nSegmentCleaner\n\nGC 的时机\n\n\n\nmount 的时候，会扫描 journal 映射的地址空间。这确定了空闲空间的大小，借这个机会，就会看看是不是应该运行 GC。\n\n\n在 IO 事务提交完成时。这时，事务产生的 journal 会减少可用空间。所以也可能需要进行 GC。\n\n\n\n\n先定义几个 ratio:\n\n\n\n\nreclaimable ratio: 可回收空间和无效空间之比。\n\n\n\n可回收空间指的是非可用空间除去被有效数据占用的，剩下的那部分。\n\n\n非可用空间就是总空间减去可用空间。\n\n\n\n\n\navailable ratio: 即可用空间和总空间的比例。可用空间是下面几项之和\n\n\n\n空闲 segment 的总大小。也就是空闲 segment 的个数 * segment 的大小\n\n\n当前 segment 的剩余空间。正在用来记 journal 的 segment 就是所谓的“当前” segment。\n\n\nGC 扫描的进度。SegmentCleaner 在 GC 时候会逐一扫描 journal 的所有记录块，它认为扫描过的块都是恢复“自由身”了的可用空间。\n\n\n\n\n\n\n\n\nGC 的条件，只要满足下面的条件之一，就触发 GC\n\n\n\n空闲空间不够了。需要同时满足下面的条件，才能称为空间不够\n\n\n\navailable_ratio &amp;lt; available_ratio_gc_max\n\n\nreclaimable_ratio &amp;gt; reclaim_ratio_gc_threshhold\n\n\n\n\n\n\n\n\nGC 的手段\n\n在上面提到的 GC 时机，seastore 会判断是否满足 GC 的条件，当条件满足的时候，就触发 GC，这时 Segment::gc_reclaim_space() 会扫描以往 journal 分离其中的有效数据，把它们作为 transaction 写到新的 journal 中去。为了避免长时间地阻塞客户端请求，每次扫描的空间大小由 reclaim_bytes_stride 限制，而且我们维护着一个 cursor 记录着上次扫描结束的位置。每次扫描都从上次结束的地方继续。\n\n\n\nextents, self.scan_cursor =\n    journal.scan_extents(victim_segment.body,\n                         start=self.scan_cursor,\n                         step=self.config.reclaim_bytes_stride)\ntxn = Transaction()\nfor extent in extents\n    if not extent.alive:\n        continue\n    txn.rewrite_extent(extent)\nif self.scan_cursor.is_complete:\n    txn.release_segment(self.scan_cursor.segment)\nself.txn_mgr.submit_transaction(txn)\n\n\n\n\nGC 的问题\n\n从改进 GC 效率和性能的角度出发，可以从这么几个方面改进\n\n\n\n\n记录数据在产生、访问和修改删除过程中产生的统计信息。\n\n\n\n跟踪有效数据和无效数据。能迅速地枚举一个 segment 中所有的有效数据。\n\n\n辨别冷热数据。这个需求是上一个的强化形式。即能保存数据块被修改的时间。如果数据经历多次 GC 并存活至今，那么也需要能记录它被 GC 的次数及其年龄。\n\n\n按照数据在应用层面的属性重排或者聚类，提高读写性能。比如说，如果一个对象被分成多个块，那么这些块的物理地址最好也是连续的。\n\n\n\n\n\n按照数据的特性分开保存。即能同时写多个 journal。\n\n\n如何更有效地迁移有效数据。\n\n\n\n\n\n\n"
} ,
  
  {
    "title"    : "ceph::common::PerfCounter 和 seastar::metrics",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/05/10/metrics.html",
    "date"     : "May 10, 2021",
    "excerpt"  : "\n\n\n成年人还是得做选择。\n\n\n\n\nseastar::metrics\n\n\nseastar::metrics 是 seastar 提供的一套机制，用来监控系统的动态指标。\n\n\nlabel\n\n在 seastar::metrics 里面，每个指标都有自己的标签。举个例子吧，假设 seastore 能够管理多个存储设备，每个设备都有自己的 IO 队列。而作为一个存储系统，我们可能会关心好多指数\n\n\n\n\ntotal_bytes: 写入设备的数据量\n\n\ntotal_operations: 读写请求的总...",
  "content"  : "\n\n\n成年人还是得做选择。\n\n\n\n\nseastar::metrics\n\n\nseastar::metrics 是 seastar 提供的一套机制，用来监控系统的动态指标。\n\n\nlabel\n\n在 seastar::metrics 里面，每个指标都有自己的标签。举个例子吧，假设 seastore 能够管理多个存储设备，每个设备都有自己的 IO 队列。而作为一个存储系统，我们可能会关心好多指数\n\n\n\n\ntotal_bytes: 写入设备的数据量\n\n\ntotal_operations: 读写请求的总个数\n\n\nqueue_length: 当前的读写队列长度\n\n\ndelay: 总的延迟\n\n\n\n\n问题在于，每个设备我们都需要监控这同一组数据。如果按照面向对象的思路，那么就是每个对象都有一组属性，而且每个对象都有自己的名字或者索引。\n\n\n另外对于 seastar 应用来说，每个 shard 都是一个相对独立处理的单元，独立结算，自负盈亏。如果我们希望监控一个 sharded service，那么这个服务在每个 shard 都有一组自己的数据。Sesastar 甚至为每一个 metric_definition_impl 都强制加上了 shard_label，所以每个 metric 从一出生，它标签上的&quot;shard&quot;就是当时 reactor 的 shard id，如果当时 reactor 还没有运行，那么 shard 就是 &quot;0&quot;。但是如果要自己设置 shard_label 的话，也可以用 seastar 提供的 shard_label。\n\n\n\nlabel shard_label(&quot;shard&quot;);\n\n\n\n其中 label 是一个 functor 类，它可以用来构造 label_instance 实例。后者才是真的 label。\n\n\n\ntemplate&amp;lt;typename T&amp;gt;\ninstance operator()(T value) const {\n    return label_instance(key, std::forward&amp;lt;T&amp;gt;(value));\n}\n\n\n\n通过为 metrics 贴上多个标签，我们可以更方便地管理和查询这些指数。而且不用因为把数据聚合起来而丢失重要的信息。seastar 在把 metrics 导出到监控系统的时候，也把 label 一起导出了。它\n\n\n\n\n使用 prometheus labels\n\n\n把 label 编码成 collected 的 type instance 字段里面。\n\n\n\n\n将来，如果我们决定使用 seastar::metrics 的话，甚至可能会用不同的 label 在不同的维度来标记同一个监控的指数\n\n\n\n\n不同 CPU shard 上的监控数据\n\n\n不同 pg 的监控数据\n\n\n不同存储设备的监控数据\n\n\n不同网卡或者网络设备的监控数据\n\n\n不同网络连接的监控数据，比如说连接到 peer osd 的心跳或者网络延迟。\n\n\n\n\n比如说，当我们注意到出现多个 slow request 的时候，首先会看这些请求在不同维度上的相关性，如果所有的请求都和某个 replica osd 有关，那么我们可能就能猜测 primary 和这个 osd 的连接是不是有问题。\n\n\n\n名字\n\nmetric 的名字由 group 和 name 构成。一组逻辑上相关的 metric 组成一个 group，比如说 seastore 的所有指数的 group 可能就是 &quot;seastore&quot;。加上一个名字空间方便管理。内存方面的监控则用 &quot;memory&quot; 作为 group 的名字。\n\n\n\n数据的类型\n\nseastar::metrics 大体是按照 Prometheus 的几种指标的类型 和 collectd 的数据类型 来实现的。\n\n\n它定义了下面几类指标：\n\n\n\n\ncounter: 单调递增的整数。要是发现某个 counter 变小了，唯一的解释就是它溢出了。比如说，从启动到现在 cache miss 的请求数量。\n\n\ngauge: 测量值。和 counter 不同，guage 支持浮点，它的值允许减小。比如说\n\n\n\n系统的音量\n\n\n某个队列的总延迟\n\n\n当前 onode 的缓存大小\n\n\n\n\n\nderive: 和 gauge 相比，derive 更像 counter 一些。它仅仅支持整型，但是它允许读数减小。它叫 &quot;derive&quot; 的原因并不是这个指标是由其他指标导出 (derive) 的，而是因为，很多时候我们关心的是这个数值的变化量 (derivative)，或者说读数对时间的导数。\n\n\n\n当前正在处理的请求个数\n\n\n\n\n\nhistogram: 一个指标的直方图。比如说，\n\n\n\n请求的延迟的分布\n\n\n请求大小的分布\n\n\n\n\n\n\n\n为了和 collectd 的类型 对应上，seastar::metrics 还定义了一些方便的函数，用来设置基于这些类型。比如，make_total_bytes()，它的功用就是在为 collectd 导出监控数据的时候，为对应的指标设置 total_bytes 的数据类型。\n\n\n\n数据的来源\n\nmake_gauge() 这些函数让我们提供一个变量的引用，或者给出一个函数返回要监控的数据。事实上前者也是通过包装一个 lambda 来实现的。\n\n\n\n\n\nceph::common::PerfCounters\n\n\nceph::common:PerfCounters 使用 PerfCounters 来管理一组 perf counter，\n\n\n名字\n\n因为 ceph::common::PerfCounters 不是统一管理的，每个 perfcounter 在构造的时候都设定了一个字符串，作为它的名字。在导出 perfcounter 的时候，把所有的 perfcounter 被放在以 PerfCounters::get_name() 为名的大对象里面。每个 perfcounter 分别打印自己的信息。\n\n\n\n数据的类型\n\n\n\n\n\n\n\n\n\n\nnone\nu64\ntime\n\n\nlongrunavg\n\n\n\n\n\nhistogram\n\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n数据的来源\n\n每个 PerfCounters 都有一个 std::vector&amp;lt;&amp;gt; ，用于保存对应的 perf counter，通过预先定义好的索引来更新和访问 vector 里面对应的值。\n\n\n\n\n\n\n\n\nceph\nseastar\n\n\nPerfCountersCollection\nmetric group\n\n\nadd_u64_counter()\nmake_counter()\n\n\nmake_derive()\n\n\nmake_gauge()\n\n\nPerfCountersBuilder\nmetric_groups\n\n\nPerfCounters\nNone\n\n\nidx\nmetric_id?\n\n\n\n\n总体上 PerfCounters 和 metrics 两个功能相当，而且前者内置了一些功能\n\n\n\n\n对 counter 加上优先级。优先级有点像 Python 的 logging level。它决定了不同情况下，输出 perfcounter 的详尽程度。如果是 CRITICAL 的 perfcounter 的话，一般来说都会打印出来，或者发给 prometheus, influxdb 这些 mgr module。\n\n\n支持设置自定义的字符串作为监控指标的单位，在打印 perfcounter 的时候，可以打印自定义的单位。\n\n\n如果一个 perfcounter 有 LONGRUNAVG 属性，那么还会统计平均值。\n\n\n\n\n但是 PerfCounters 缺少 label 的支持，而且其实现是基于 std::atomic&amp;lt;&amp;gt; 的，在读写 perfcounter 的时候对性能也有负面的影响。\n\n\n\n"
} ,
  
  {
    "title"    : "QoS 和 dmClock",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/04/24/dmclock.html",
    "date"     : "April 24, 2021",
    "excerpt"  : "\n\n\nQoS，而且是分布式的。\n\n\n\n\nQoS\n\n\n如果一个服务有多个服务对象，那么就得分轻重缓急。但是也不能因为有些服务对象不重要，就完全不顾它们。举个例子，就算平时工作再忙，上个月的报销单也应该及时填啊。那么怎么实现 QoS 呢？还是用报销单的例子，那么我们定个规矩吧：每天至少用半小时时间填报销，要是做不完的话，那么就明天继续做。在执行之前，我们需要先引入几个概念:\n\n\n\n\n任务：也就是要做的事情。每件事情都有它的优先级，还有它要花费的时间。\n\n\n任务列表：要是任何时候，手里面只有一件...",
  "content"  : "\n\n\nQoS，而且是分布式的。\n\n\n\n\nQoS\n\n\n如果一个服务有多个服务对象，那么就得分轻重缓急。但是也不能因为有些服务对象不重要，就完全不顾它们。举个例子，就算平时工作再忙，上个月的报销单也应该及时填啊。那么怎么实现 QoS 呢？还是用报销单的例子，那么我们定个规矩吧：每天至少用半小时时间填报销，要是做不完的话，那么就明天继续做。在执行之前，我们需要先引入几个概念:\n\n\n\n\n任务：也就是要做的事情。每件事情都有它的优先级，还有它要花费的时间。\n\n\n任务列表：要是任何时候，手里面只有一件事情，我们可能就不用操心 QoS 了。但是现实是，很多时候别说不同优先级的，就是同一优先级的任务都可能同时有好几件。我们一般按照先到先服务的原则，把它们放到队列里面，挨个处理。\n\n\n分类：把要做的事情按照优先级分成几种。\n\n\n计量：每个任务需要的时间都不一样，如果只是处理会议的邀请，那么可能只需要几秒钟就能做出回应。但是如果是分析一个新的编译错误，可能就得花半个小时。\n\n\n调度：接下来该从哪个队列里做哪个任务呢？\n\n\n\n\n我们还需要继续细化刚才的规定，这半个小时时间是排在什么时候呢？要不下午一点开始吧。对应到刚才的概念就是\n\n\n\n\n任务：每天的工作，比如处理信件，填报表单，当然还有调试程序。\n\n\n任务列表：每件工作按照先后顺序记到小本本上。做完一件就划掉一件。有新的工作就记在最后面。\n\n\n分类：工作分成两类：一类是平时的工作，另一类是低优先级的维护工作，比如说报销。\n\n\n计量：对列表里的工作根据工作量一一进行评估。\n\n\n调度：现在到一点了吗？还没到，那么看看“正事儿“列表里面下一项是什么？到了的话，看看另外一个列表有没有东西，没有的话，就先做正事儿。反过来也是这样，就是如果今天正好没有其他事情的话，即使没有到一点钟，那么也不妨把积攒的报销单填了。\n\n\n\n\n所以对我们来说，每当要开始下一个任务的时候，要回答的问题就是，我应该看哪个任务列表呢？刚才的设计定义的算法是一种比基础的优先级队列 （priority queue）调度更复杂的算法。它不仅仅为不同性质的任务定义了具有两个优先级的队列，而且为了防止低优先级任务得不到执行，还为它们保留了最小的带宽。\n\n\n\n\nClock\n\n\n在说 mClock 之前，我们先说说几个指标：\n\n\n\n\nweight (share) ：这个类型的任务能使用多少比例的资源。如果一个系统里面需要进行数据迁移，那么它可以占用的带宽比例就可以很高。\n\n\nreservation：这个类型的任务最少能保证得到多少资源。如果系统里面需要有低延迟的应用，比如说远程桌面或者交互式的网络游戏，那么就必须保证最低的带宽。但是这种应用需要的 weight 可能就相对低一些。\n\n\nlimit: 使用资源的上限。比如说刚才说的数据迁移的应用，它可能就需要设定一个上限，在资源吃紧的情况下，防止它挤占其它应用的带宽。\n\n\n\n\n刚才的算法里面只支持前两个指标。mClock 的目标是同时支持这三个参数。论文前面列举了一些算法，这些算法解决的问题分为三类：\n\n\n\n\n按照应用指定的权重分配带宽\n\n\n除了按照比例分配带宽之外，还能照顾对于延迟比较敏感的应用\n\n\n除了按照比例分配带宽，照顾延迟，还能预留最低的带宽\n\n\n\n\n但是 mClock 不仅仅能支持这些功能之外，还支持指定上限，并且能根据资源的容量动态地进行调整。比如说，一个系统它接入的是一个网络存储，因为分布式的存储系统和本地存储相比有着更多的变数和不确定性，那么这个系统能获得的存储的带宽可能就是动态的。根据网络系统的设置，以及存储系统提供给它的带宽而定。\n\n\nmClock 先从 tag-based 调度算法开始。 标签是指每当调度器看到一个新的请求，就会给它贴一个标签。标签上有几个之后要用到的数字。这个很像是去饭馆吃饭时，拿到的预约单号。当调度器需要决定谁是下个幸运儿的时候，它就检查每个请求上标签。每个客户端都有自己的权重 \\(w_i\\) 。我们用权重的倒数就可以构成一个等差数列，\n\n\n\n\\[1/w_i, 2/w_i, 3/w_i, .., n/w_i\\]\n\n\n\n在服务端，当请求到达的时候，就根据请求所属的客户端为请求打上标签。同一客户端发来的请求构成了这个数列。权重越大客户端对应的序列分布越密集。下图中有蓝黄和绿三个客户端，它们的权重分别为 1/2，1/3 和 1/6。\n\n\n\n\n\n\n\n如果标签小于当前时间就按照先小后大的顺序依次处理，那么在单位时间里面，客户端得到处理的请求个数所占比例也就是它分到的权重。\n\n\n我们还可以从完全公平调度（Complete Fairness Scheduling）算法的角度来进一步理解。CFS 的调度对象是进程。它按照 RUNNABLE 进程的虚拟时钟从小到大排序，时间靠前的进程先调度，靠后的后调度。每个进程都有自己的虚拟时钟。内核定期按照物理时钟的节拍推进所有的虚拟时钟。各进程时间流逝的速度是不一样的，优先级高的进程时钟就走得慢，低的走得快些。这样，CFS 就能直接调度排在最前面的进程。顺便说一下，Linux 中的 CFS 调度器是用红黑树来管理这些 RUNNABLE 进程的，树的键就是虚拟时钟的值。\n\n\n另外，由于时间早的会先调度，所以如果谁的时钟停摆很久，如果它的标签一开始就比它优先级高的其他人低好多，那么一旦开始发送请求，接下来就会因为自己的时钟读数比其他人都小而连续抢占资源，导致发生饥饿。因此 mClock 论文里面说的 global virtual time 或者 virtual time 也就是为了解决这个问题。全局时钟要求所有的客户端时钟不管客户端是不是正在发出请求，都不断地往前推进。确保不会有人中途加入，打乱其他人的步调。\n\n\n\n\nmClock\n\n\nmClock 把这个想法推而广之，既然我们要支持 reservation 和 limit，为什么不能用他们来计算 tag 呢？我想这也是 mClock 的名字的由来，multiple clocks。于是在处理每个请求的时候，都会给他们设定三个 tag：\n\n\n\n\n\\(\\mathit{R_i}^{r}\\)\n\n\n\\(\\mathit{L_i}^{r}\\)\n\n\n\\(\\mathit{P_i}^{r}\\)\n\n\n\n\n其中，\n\n\n\n\nR 代表 reservation\n\n\nL 代表 limit\n\n\nP 则是 priority, weight 或者是 proportional\n\n\n\n\n以 \\(\\mathit{R_i}^{r}\\) 为例，用下面的递推公式计算：\n\n\n\n\\[\\mathit{R_i}^{r} = max \\{\\mathit{R_i}^{r-1}+\\frac{1}{\\mathit{r_i}}, \\mathit{t}\\}\\]\n\n\n\n其中， \\(\\mathit{r_i}\\) 就是第 i 个客户端的 reservation 值。相邻 tag 的距离就是 \\(\\frac{1}{r_i}\\)。而 \\(\\mathit{t}\\) 是当前的时间。\n\n\n那么刚才说的 global virtual time 的问题怎么解决呢？因为新客户端的上一个标签无据可查，而且枚举 所有 的客户端，定时遍历每个人的时钟，挨个更新它们的三个 tag，对系统可能也是个负担。论文采取的办法是，把所有人的 P 和当前时间对齐。\n\n\n写成 C++ 代码，可能就是这样：\n\n\n\nvoid request_arrival(request_t r, time_t t, vm_t i)\n{\n  if (vm[i].was_idle()) {\n    // tag adjustment\n    if (!P_tags.empty()) {\n      auto min_P_tag = std::min_element(std::begin(P_tags), std::end(P_tags));\n      for (auto&amp;amp; vm : active_vm) {\n        P_tags[vm.id] -= *min_P_tag - t;\n      }\n    }\n  }\n  // tag assignment\n  R_tags[i] = std::max(R_tags[i] + 1 / reservation[i], t);\n  L_tags[i] = std::max(L_tags[i] + 1 / limit[i], t);\n  P_tags[i] = std::max(P_tags[i] + 1 / weight[i], t);\n  schedule_request();\n}\n\n\n\n为什么对 \\(\\mathit{P_i}^{r}\\) 特殊处理呢？我们假设直接使用当前的时间作为新人或者刚开始 active 的客户端的  \\(\\mathit{P_i}^r\\)。那么对其他客户端来说，它们发出第一个请求的时候，\\(\\mathit{P_i}^{r}\\) 也是当前的系统时刻，但是当客户端持续地发出请求，随着时间推移，根据各自的权重不同，你我的时间开始差得越来越多，贫富差距慢慢显现。但是新来的客户端横空出现打破了这个均衡，它的 \\(\\mathit{P_i}^{r}\\) 不是根据之前的 \\(\\mathit{P_i}^{r-1}\\) 推算出来的，而是直接使用的当前时间。虽然它的优先级可能并不高，但是它在一段时间之内凭借它的暂时的“后发优势“，无缘无故地打败了很多甚至优先级比他更高的老前辈，直到它的权重慢慢地把一开始的 P 慢慢抵消，一切恢复正常。mClock 算法为了解决这个问题，转而以最新的系统时钟调整其他老革命的 P，让所有的 P 按照时间轴平移，令最小的 P 等于系统时间。这样新加入的 P 就不会干扰现有的秩序了。因为 P 标签有累计的效应，所以这里仅仅调整它。\n\n\n有了三个 tag，那么到底以谁为准呢？调度器有两种决策模式，并根据当前情况在两者之间切换：\n\n\n\n\n基于约束的决策：调度器先看看有没有人的 R 小于当前时间，要是有的话，就直接调度最小 R 的请求。一旦所有的 R tag 大于当前时间，就脱离基于约束的决策模式，进入基于权值的模式。\n\n\n基于权值的决策：这时候所有人的 reservation 都已满足。调度器开始按照权重来分配资源。它先把资源用量还没有超过上限的人找出来，他们的 L 比当前时间小。然后从中找出 P 最小的。调度 \\(vm_{i}\\) 的请求的时候，除了让它的最前面的请求出列，还需要把 \\(vm_{i}\\) 其他还在队列里面的请求的 R 都减去 \\(1/r_{i}\\)。这样可以保持相邻 R 的差仍然是 \\(1/r_{i}\\)。否则，我们想象一下，如果一个客户端很长一段时间它的请求都是用基于权值的决策调度的，那么它的 R tag 就会非常大。一旦系统的资源吃紧，它会立即得不到应该有的 reservation。为什么？只是因为它一直因为权重得到了很多服务，但是这笔账不应该算在 reservation 头上。所以我们每次因为权重调度请求，都需要把这个客户端的还没调度的请求的 R tag 都往前移动一格。确保这个客户端的 reservation 不会受到影响。\n\n\n\n\n\n\n\n\n\nmClock 为存储系统做的一些改进\n\n突发情况\n\n有些客户端可能会稳定地发送读写请求，但是也有那种平时不动声色，突然狮子大开口的角色。有时候请求会陡然增加，我们叫做 burst。比如说有的客户端每个晚上会为文件建立索引，但它白天却悄无声息，这时候我们希望感谢它之前高风亮节为大家节省资源，给它个行个方便，让它一开始的 \\(\\mathit{P_i}^{r}\\) 小一些。\n\n\n\n\\[\\mathit{P_i}^{r} = max \\{\\mathit{P_i}^{r-1}+\\frac{1}{\\mathit{r_i}}, t - \\frac{\\sigma_{i}}{w_i}\\}\\]\n\n\n\n这个 \\(\\sigma_{i}\\) 可以每个人都不一样，我们暂且把它叫做&quot;先人后己奖励奖&quot;吧，专门用来补偿把带宽让给别人的人，让他们在有急需的时候也能感受到 QoS 的温暖。论文后面也提到，如果这个奖金太高，会因为扰乱权重分配的决策，导致细水长流式的客户的高延迟。\n\n\n\n读写有别\n\n在存储系统里面，写的延迟往往比读请求要高。但是我们没办法取巧，把同一个客户端的请求乱序执行。比如，把读请求放到写请求之前乱序执行未导致读到不一致的数据。\n\n\n\n大小有别\n\nIO 请求有的大，有的小，不能等同视之。因为我们不追求绝对的延迟数值，比如说一个 4k 的读请求需要多少毫秒。我们希望得3到的是一个比例，即大小为 S 的 IO 请求产生的延迟相当于多少个单位大小的 IO 请求。\n\n\n\n\\[1 + \\frac{S}{T_{m} + B_{peak}}\\]\n\n\n\n论文大概计算了一下，其中 \\(T_{m}\\) 表示机械动作产生的延迟，假设每次随机读写都要求机械磁盘的悬臂产生移动到对应的磁道，磁盘都需要转动到需要读写的扇区。而 \\(B_{peak}\\) 是磁盘最高的读写速度。\n\n\n\n\n\n\ndmClock\n\n\n分布式的场景下，每个服务器需要了解两件事情\n\n\n\n\n你从所有服务器总共获得了多少服务\n\n\n其中，你通过 reservation 获得了多少服务\n\n\n\n\n为了让服务器知道客户端的服务情况，客户端在发送请求的时候也会顺带着发送\n\n\n\n\n\\(\\rho_{i}\\) 最近请求到当前请求之间，因为基于约束的决策获得的服务数量。其实也就是因为 reservation 获得的服务数量。\n\n\n\\(\\delta_{i}\\) 最近请求到当前请求之间，获得了多少服务。\n\n\n\n\n\n\\[\\mathit{R_i}^{r} = max \\{\\mathit{R_i}^{r-1}+\\frac{\\rho_{i}}{\\mathit{r_i}}, \\mathit{t}\\} \\\\\n\\mathit{L_i}^{r} = max \\{\\mathit{L_i}^{r-1}+\\frac{\\delta_{i}}{\\mathit{l_i}}, \\mathit{t}\\} \\\\\n\\mathit{P_i}^{r} = max \\{\\mathit{P_i}^{r-1}+\\frac{\\delta_{i}}{\\mathit{w_i}}, \\mathit{t}\\}\\]\n\n\n\n下面是之前介绍的单机版递推公式：\n\n\n\n\\[\\mathit{R_i}^{r} = max \\{\\mathit{R_i}^{r-1}+\\frac{1}{\\mathit{r_i}}, \\mathit{t}\\}\\]\n\n\n\n可以发现，我们把 1 换成了 \\(\\rho_i\\)。这个思路和之前是一脉相承的。有点像一个大型的合作性的公寓，每家都有个户主负责向提供交各种费用水费、电费、煤气费。但是户主们并不是各自为政，只要交的总金额足够支付整个公寓的账单就行，当然，家里面有的时候没有流动资金，所以紧张的话，有的人可以少交有的人也可以多交。但是户主和户主之前缺少有效的沟通方式，好在自来水公司它们都有明细账，所以户主在缴费的时候可以查看之前的账目。这里户主就像分布式系统里面提供服务的节点，各项费用的账单就像不同性质的客户请求。缴费的过程就是处理客户请求。借用刚才的示意图，我们以 100 块钱为单位，如果公寓上个月加起来交了 200 块钱电费，那么电力公司这次就应该把付账的进度条往前推进 2 个单位。所以图里面的有两个请求就用虚线表示了，它们代表在其他服务器处理过的请求。\n\n\n\n\n\n\n\n公式里的 \\(\\rho_{i}\\) 和 \\(\\delta_{i}\\) 就是这里的&quot;2&quot;。这两个系数表示因为不同原因，自从上次从这个服务器处理请求，这个客户端一共从不同服务器获得了多少服务。很明显，\\(\\delta_{i}\\) 应该总是大于等于 \\(\\rho_{i}\\)。如果是单机的话，两个参数就退化成 1 了。\n\n\n\n\ndmClock 在 Ceph 中的应用\n\n\n因为 dmClock 是一个通用的算法，Ceph 并没有把直接集成在自己的 repo 里面，而是单独实现了高度模板化的 dmClock 库。这样其他应用也能使用它。dmClock 库基本忠实地实现了论文中的算法。开始之前，请大家注意，目前 Ceph 正在使用的并不是 dmClock 而是 mClock。\n\n\nOSD 中的 QoS\n\nOSD 需要处理多种请求，有的请求优先级比较低，比如后台的数据恢复，有的请求优先级比较高，比如说前台客户发来的读写请求。而 OSD 的处理能力有限，又希望有一定的 QoS 能力。就需要设计一个能兼顾不同优先级需求的调度器。我们把不同类型的请求看成不同的客户端，在 OpSchedulerItem 就定义了下面几种请求\n\n\n\n\nclient_op\n\n\npeering_event\n\n\nbg_snaptrim\n\n\nbg_recovery\n\n\nbg_scrub\n\n\nbg_pg_delete\n\n\n\n\n以 bg 开头的请求都是后台的请求，它们保证系统的正常运行，但是优先级相对于前面两类请求就要低一些。而且，每个 OpSchedulerItem 都有自己的 priority 和 cost。所以调度器调度的对象就是 OpSchedulerItem 了。但是可能和大家猜测的不同，OSD 用来实现 QoS 的调度器却不是全局唯一的。它是 OSDShard 的成员变量。而 OSDShard 则是 OSD 的执行单位，它维护着一个队列。队列里面的元素就是被安排执行的请求。每个 shard 都负责一个或者多个 PG，每当有请求到达，都会用请求对应的 PG 作为 key 找到对应的 shard，让 shard 决定什么时候执行它。而这个决定就是由调度器做出的。所以有多少个 OSDShard 就有多少个调度器，它们分别为各自负责的一组 PG 调度请求。\n\n\n我们有两个调度器\n\n\nClassedOpQueueScheduler\n\n这个调度器很像 Low-latency queuing。它基于 WeightedPriorityQueue 实现，简称 WPQ。它的设计和大家熟知的 Weighted Fair Queueing 调度器很像。WPQ 维护着多个子队列，每个队列有自己的优先级。在调度的时候，队列按照优先级享有对应的权重，被选中的机会就是权重的大小。选好队列之后，再随机选择队列里面的请求。请求的 cost 越低，被选中的可能性越大。但是这个设计可能太“公平“了，但是对于低延迟的请求响应可能就不够及时。所以除了这个为普通优先级服务的加权公平队列之外，调度器还另外定义了一个单独的 WPQ，为低延迟的应用提供了严格优先级的服务。只有严格优先级队列里面的请求处理完了，它才会开始检查普通优先级的队列。\n\n\n\nmClockScheduler\n\n前面提到一个 OSD 有多个调度器，但是它们共享除了系统线程之外所有的资源，而且缺少有效的隔离措施。所以在设置预留值的时候是按照假设的介质提供最大带宽按照 shard 的数量平均下来计算的。和 ClassedOpQueueScheduler 类似，mClockScheduler 定义了一个 &quot;immediate&quot; 队列，它提供为高优先级的客户端先进先出的服务。只有这个队列没有元素的情况下，才会转用基于 mClock 的队列。为了方便测试，现在预定义了三种 QoS 模式，分别为三大类请求设置了对应的参数：\n\n\n\n\n\n\n\n\n\n\n\nQoS模式\n服务类型\n预留\n权重\n上限\n\n\n偏重客户性能\nclient\n50%\n2\ninf\n\n\nrecovery\n25%\n1\n100%\n\n\nbest effort\n25%\n2\ninf\n\n\n均衡型\nclient\n40%\n1\n100%\n\n\nrecovery\n40%\n1\n150%\n\n\nbest effort\n20%\n2\ninf\n\n\n集中精力 recovery 型\nclient\n30%\n1\n80%\n\n\nrecovery\n60%\n2\n200%\n\n\nbest effort\n1%\n1\ninf\n\n\n\n\nmClockScheduler 中，很重要的一个函数是 mClockScheduler::ClientRegistry::get_info()，它负责把请求按照他们的 get_scheduler_class() 分门别类，套用上面配置的 res, wgt 和 lim 参数。\n\n\n\n\nMOSDOp:\n\n\n\nCEPH_MSG_OSD_OP 或者 CEPH_MSG_OSD_BACKOFF: client。这一类所有的请求都用 default_external_client_info\n\n\n其他: immediate\n\n\n\n\n\nPG 操作\n\n\n\npg delete: background_best_effort\n\n\npg scrub: background_best_effort\n\n\npg snaptrim: background_best_effort\n\n\npg peering: immediate\n\n\npg recovery:\n\n\n\n高优先级的就是: immediate\n\n\n低优先级就是: background_recovery\n\n\n\n\n\n\n\n\n\n\n\n\n客户端和服务端协作的 dmClock\n\n社区也曾推进 dmClock 在 Ceph 的应用。在 2018 年的 Cephalocon 上， SK 电信的工程师向我们分享了他们做的 工作。甚至他们的改进曾经进入了 master，但是后来被 revert 了，剩下的 PR 到现在三年多过去了，没有进展。\n\n\n\n"
} ,
  
  {
    "title"    : "大批量发送增量 osdmap 对性能的影响",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/03/29/flatten-osdmaps.html",
    "date"     : "March 29, 2021",
    "excerpt"  : "\n补丁太多了的话，加起来大小可能会比最终的版本更大。\n\n\n在 Ceph 里面，osdmap 是一个很重要的数据。比如说，\n\n\n\n\n集群的拓扑\n\n\n集群里每个数据池的 crush 规则，甚至还有\n\n\n一个屏蔽列表，集群会拒绝向在列表里面的客户端提供服务\n\n\n\n\n但是正因为 osdmap 包含了太多信息，在集群里面传递完整的 osdmap 会耗费很多带宽，而且编解码完整版本的 osdmap 也加大了对 CPU 的压力。为了缓解这个压力，我们选择仅仅发送变化的那部分。在 monitor 上，每次...",
  "content"  : "\n补丁太多了的话，加起来大小可能会比最终的版本更大。\n\n\n在 Ceph 里面，osdmap 是一个很重要的数据。比如说，\n\n\n\n\n集群的拓扑\n\n\n集群里每个数据池的 crush 规则，甚至还有\n\n\n一个屏蔽列表，集群会拒绝向在列表里面的客户端提供服务\n\n\n\n\n但是正因为 osdmap 包含了太多信息，在集群里面传递完整的 osdmap 会耗费很多带宽，而且编解码完整版本的 osdmap 也加大了对 CPU 的压力。为了缓解这个压力，我们选择仅仅发送变化的那部分。在 monitor 上，每次 osdmap 有变化，我们不仅仅保存了最新完整版本的 osdmap，也会保存它的增量部分&amp;#8201;&amp;#8212;&amp;#8201;我们用专门的对象保存这个部分，即 OSDMap::Incremental，有时候干脆叫它 inc map。所以当客户端找 monitor 要 osdmap 的时候，也会告诉对方自己手里面 osdmap 的版本 m，如果 monitor 的最新版 osdmap 的版本是 n，那么它就会把 m..n 的所有 inc map 都发给客户端。\n\n\n但是有时候也会适得其反，因为积少成多，要是有很多的 inc map，为了发送这些 inc map，对 monitor 甚至客户端，累加起来的开销和发送一个完整 osdmap 比起来可能会更高。而且，需要注意的是，Monitor::ms_dispatch() 是在一个全局大锁里面执行的。很多其他操作也需要这个锁。所以我们应该尽量避免长时间地持有它，否则会造成很高的延迟。\n\n\n要解决这个问题，有下面几个思路：\n\n\n\n\n在 monitor 一侧\n\n\n\n减少 monitor 对 osdmap 的更新频次。primary osd 会根据情况要求产生 pgtemp，但是 monitor 也可以主动地批次生成 pgtemp。\n\n\n分期分批地发送 inc map。这样可以缓解因为长时间占用全局锁造成的延迟。\n\n\n\n\n\n在 osd 一侧\n\n\n\n减少 osd 对 osdmap 的请求。如果 osd 发现自己落后太多，就直接找 monitor 要完整的最新版 osdmap。而不是要求获得增量版本。减轻 monitor 的负担。\n\n\n\n\n\n\n\n但是 n 版的 osdmap 是不是真的能替代 m 版 osdmap 加上中间的 m..n 的 inc map 呢?\n"
} ,
  
  {
    "title"    : "方程式赛车和读写请求",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/03/14/lap-n-race.html",
    "date"     : "March 14, 2021",
    "excerpt"  : "\n请求 client.io-42 在第三圈超过了 client.io-17！\n\n\n前阵子在思考在 Ceph 里面为什么要顺序执行读写。简单说，和 CPU 的顺序执行模型一样，这个模型更容易理解和使用。顺便跑题一下，对于&quot;人工智能是什么&quot;一直有个争论。其中有两派:\n\n\n\n\n弱人工智能，即表现上的人工智能。 中文房间 中文房间和 图灵测试 就是两个比较有名的测试，通过它们就是一定程度上的弱人工智能。这种人工智能能解决一个特定问题领域上的问题。 现在说的机器学习就是一种弱人工智能。\n\n\n强人工智...",
  "content"  : "\n请求 client.io-42 在第三圈超过了 client.io-17！\n\n\n前阵子在思考在 Ceph 里面为什么要顺序执行读写。简单说，和 CPU 的顺序执行模型一样，这个模型更容易理解和使用。顺便跑题一下，对于&quot;人工智能是什么&quot;一直有个争论。其中有两派:\n\n\n\n\n弱人工智能，即表现上的人工智能。 中文房间 中文房间和 图灵测试 就是两个比较有名的测试，通过它们就是一定程度上的弱人工智能。这种人工智能能解决一个特定问题领域上的问题。 现在说的机器学习就是一种弱人工智能。\n\n\n强人工智能，即机理上的人工智能，或者说通用人工智能。这种人工智能需要具有知识表示，推理，规划，学习，自然语言处理等等能力，并把它们整合起来。强人工智能和弱人工智能比起来要难很多。前年看的 The Book of Why: The New Science of Cause and Effect 说明了因果关系和推理在人工智能的重要地位。\n\n\n\n\n和人工智能一样，存储系统的顺序性也有两种\n\n\n\n\n表现出来是顺序的\n\n\n严格在执行层面上就是顺序的\n\n\n\n\n当然，限制越多，性能越不好。所以在实现 crimson 的时候，每当要求顺序执行的时候，我都会多问几次&quot;为什么&quot;。Ceph 集群里面，一个读写请求在一生中，至少有下面几个阶段\n\n\n\n\n从客户端到 primary OSD\n\n\n从 primary OSD 分配到对应的 PG\n\n\n生成事务，并写入本地磁盘。事务包含 pglog 和对应的用户数据。\n\n\nreplica OSD 也回应了请求，确认持久化完成。\n\n\n\n\n客户端请求就像方程式赛车一样，\n\n\n\n\n必须遵守一定的规则，比如只有在持有要求的 osdmap 的时候才能开始处理它，不能依据老版本的 osdmap 行事。\n\n\n需要跑完一定的圈数，\n\n\n而且，对于 Ceph 来说，特定客户端发来的多个请求必须顺序完成。好像来自一个车队的选手必须按照出场顺序完赛一样。\n\n\n\n\n在 crimson 里面，满足前两个要求都比较简单直接。问题在于第三个。如果 object store 能保证先进先出，那么是不是只要保证发送请求到 object store 的顺序满足第三个要求就行了呢？\n\n\n答案是不行。在中间有的阶段需要有 write barrier，比如，为了保证 osd 在恢复时有个参考，pglog 保存了 PG 最近的一系列操作，它们保存在 pglog 里面。在构造 transaction 的时候，会和封装用户数据的 journal 一起刷到磁盘上。但是 pglog 是一个链表，链表上每个环节都是一次对 PG 的修改操作。为了逻辑上简单一些，我们需要 pglog 上的操作序列也按照按照客户请求的顺序安排。这样在副本不一致的时候，就可以按照顺序恢复了。不用担心因为乱序执行带来的数据依赖的问题。所以在生成 pglog 的时候，需要保证顺序。\n\n\n类似的，在写磁盘的时候也需要保证顺序，因为相同的读写操作以不同顺序发送到后端存储，得到的结果是不一样的。这个道理和之前《多核和顺序》一文中讨论的问题类似。所以在写磁盘的时候也需要保序。我们使用流水线的设计解决这个问题。\n"
} ,
  
  {
    "title"    : "永不消逝的 failure 报告",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2021/03/11/failure-reports.html",
    "date"     : "March 11, 2021",
    "excerpt"  : "\n有时候，failure 报告会化身 SLOW_OPS，并获得永生。\n\n\nCeph 集群里面，OSD 和 OSD 会定时互相发送心跳。如果有人很长时间不回应，那么另外一方就会打个小报告给 monitor。告诉 monitor，对方很可能已经停摆了。当然，这只是一家之言，monitor 不会因此就断定那个不回消息的人失联了。但是如果收到多个不同来源的报告，都指向同一个 OSD，那么 monitor 就会认为该 OSD 的确是有问题的。而且，本着认真负责的精神，monitor 把所有的小报告的...",
  "content"  : "\n有时候，failure 报告会化身 SLOW_OPS，并获得永生。\n\n\nCeph 集群里面，OSD 和 OSD 会定时互相发送心跳。如果有人很长时间不回应，那么另外一方就会打个小报告给 monitor。告诉 monitor，对方很可能已经停摆了。当然，这只是一家之言，monitor 不会因此就断定那个不回消息的人失联了。但是如果收到多个不同来源的报告，都指向同一个 OSD，那么 monitor 就会认为该 OSD 的确是有问题的。而且，本着认真负责的精神，monitor 把所有的小报告的原件都保存了起来。\n\n\n\n\nmonitor 这边\n\n\n\n如果 osdmap 已经把失联的 OSD 标记成了 down，它会逐个联系打报告的 OSD，给他们发送最新版的 osdmap。告诉相关人士，&quot;众卿这下可以放心了&quot;。如果自己变成了 peon，也会把这些小报告重新处理一遍，小报告会被转发给新的 leader，让它定夺。\n\n\n如果打小报告的 OSD 不在 osdmap 里面了，它会直接把这个 OSD 发送的报告全部删除。\n\n\n\n\n\n报告人\n\n\n\n如果打小报告的 OSD 准备重启，或者因为网络故障打算停止运行，它也会主动发消息，让 monitor 取消自己的报告。\n\n\n另外，报告人一旦和失联 OSD 取得了联系，它也会取消自己的报告。\n\n\n或者它不再和失联 OSD 互发心跳了，那么它同样会把之前的报告撤销。\n\n\n\n\n\n失联的人\n\n\n\n如果失联的人其实并没有脱离组织，但是它发现 monitor 已经把自己从 osdmap 除名。那么它会向 monitor 发消息澄清。monitor 收到它的请求，会把所有关于它的报告作废。\n\n\n\n\n\n\n\n另外一方面，为了跟踪集群里面的性能问题，Ceph 设计了一个机制，叫 TrackedOp。我们认为每个请求都有它的生命周期。如果集群出现问题的话，可能会导致请求有很长时间的延迟。这就是我们常说&quot;hang&quot;。最极端的情况就是一个请求永远得不到相应。这里所说的请求包罗万象，从客户端发来的 IO 请求，到 ceph 命令行发到 monitor 的查询，甚至刚才提到的&quot;小报告&quot;都是所谓的 TrackedOp。每个对自己服务质量有要求的 daemnon 都有个注册表叫做 OpTracker。每收到一个 TrackedOp 请求，都会把它记录在案。在 OpTracker 的析构函数里面，则会把自己从注册表里面注销掉。所以这个表里面保存着当前所有的 TrackedOp 的引用，如果一个 TrackedOp 收到之后很快就销毁了，那么就认为这个 op 的处理是及时的。但是如果有请求躺在注册表里很长时间，都没有把自己注销，那么很可能它就&quot;hang&quot;住了。处理这个请求的服务在定期汇报健康状况的时候，会把这个问题也一起报告给 monitor。monitor 会进一步汇总，把这类问题一起报告给管理员。这种问题叫做\nSLOW_OPS。\n\n\n所以，只要可能失联的 OSD 仍然在 osdmap 里面，那么 leader monitor 就会把针对它的小报告一直保留下来。直到 leader 收集到足够的证据，或者举报的人主动撤销或者自己出局。\n\n\n但是也有可能证据一直不够充分，那么 leader 无法做出判断。而这些报告经 prepare_failure() 处理后，就一直放在 OpTracker 里面，变成了 SLOW_OPS 的一员。对于系统管理员来说，它们成了很碍眼的报警信息。\n\n\n这里其实有两个问题：\n\n\n\n\nprepare_failure() 处理过了小报告，是不是就可以说，monitor 完成了这个请求？从而把它从 OpTracker 里面注销？\n\n\nmonitor、报告人和疑似失联者三方都尽心尽力对报告负责。但是如果报告人因为异常重启没有撤销报告，那么这个它发送的报告就永远无法撤销了。\n\n\n\n\n解决这个问题可能有下面几个方案，他们相对独立：\n\n\n\n\n处理完毕失联报告，就直接销毁对应的请求，让 OpTracker 不再跟踪它。这样即使这个报告在短时间之内没有举报成功也不会出现在 SLOW_OP 里面。但是问题在于\n\n\n\n如果失联报告暂时没有下文，我们能说处理完毕了吗？换言之，如果一个疑似失联的人没有被及时澄清，或者没有踢出 osdmap，那么这个集群的设计很可能是有问题的。它表示这个集群很可能有严重的网络分裂问题，导致一部分 OSD 之间无法互相通信，但是另外一部分 OSD 之间的网络是联通的，导致疑似失联的 OSD 没有办法在短时间之内获得足够数量的报告。所以把这个问题以 SLOW_OPS 的方式暴露出来也是一个办法，让管理员觉察到问题，虽然这个错误信息比较隐晦。\n\n\n报告人这里对报告也有周密的跟踪机制，它把还未发出去的报告保存在 failure_queue 里面，已经发出去的报告则保存在 failure_pending 里面。 如果发出的报告没有下文，也就是说如果它没有撤销，而新版的 osdmap 也没有把失联的 OSD 标记成 &quot;down&quot;，那么这些报告会一直保存在 OSD 里面。\n\n\n\n\n\n在 monitor 这边加入衰退的机制，如果失联报告很长时间没有其他方的证据坐实这个问题，那么就取消报告。但是在大规模的集群里面，如果集群有突发情况，比如产生大规模的网络分裂，那么短时间之内 failure_info 中会出现大量积压的 failure 报告，如果在持有 Monitor::lock 大锁的时候遍历数据它，可能会加重 monitor 的负担，提高 monitor 处理消息的延迟。请注意，Monitor::timer 是一个 SafeTimer，而这个 &quot;safe&quot; 是由 Monitor::lock 保障的。而 Monitor::ms_dispatch() 也是在 Monitor::lock 里面处理消息的。为了缓解这个问题，可以引入一个 int 型的 &quot;指针&quot;，每次在 tick() 里面仅仅处理一定数量的 failure，如果超过这个数量，就把工作留到下次 tick() 再做。这个 &quot;指针&quot; 就用来保存下次开始的位置。如果在某次处理部分报告的时候，正好错过了某些刚加入的信息也没关系，以后就会轮到的。除非 failure_info 增长的速度大大超过每次 tick() 处理的速度。\n\n\n因为 OSD 会定期发送 osd_beacon 给 monitor，所以如果有疑似的失联 OSD 发送 beacon 给 monitor，那么是不是也能说明这个 OSD 并不是真正的脱离了组织呢？但是，如果因为网络的问题，这个 OSD 只是不能和举报的 OSD 联系，那么我们也需要把它踢出 osdmap。但是因为 osd_beacon 的发送周期非常长，达 300 秒之久。所以经过这么长的时间，monitor 都无法做出裁决。大概率也能说明集群很难有效地在指定时间内有效地标记失联 OSD 了，至于原因可能是当初发送报告的 OSD 重启，或者干脆是严重的网络分裂问题。但是如果是 OSD 重启的话，我们不应该继续保留 SLOW_OPS 了，它逗留的时间已经够久了。\n\n\n"
} ,
  
  {
    "title"    : "Crimson 里面的流水线",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2020/11/17/pipeline-ceph.html",
    "date"     : "November 17, 2020",
    "excerpt"  : "\n在处理性能问题的时候，需要知道客户请求都堵在哪儿了。\n\n\n具体说，有这么两个需求\n\n\n\n\n能够知道客户请求阻塞在什么地方了。比方说，最好能够列出当前还没有完成的请求都是什么状态。是在等 osdmap，还是在等 PG，还是在等它希望访问的 object？\n\n\n提供一个机制，能阻塞客户请求。我们有时候甚至希望有计划有策略地阻塞客户端请求。实现一些 QoS 的功能。\n\n\n\n\nCrimson 为了解决这些需求，引入了一组概念\n\n\n\nOperation\n\n代表一种操作。比如说刚才提到的客户发来的请...",
  "content"  : "\n在处理性能问题的时候，需要知道客户请求都堵在哪儿了。\n\n\n具体说，有这么两个需求\n\n\n\n\n能够知道客户请求阻塞在什么地方了。比方说，最好能够列出当前还没有完成的请求都是什么状态。是在等 osdmap，还是在等 PG，还是在等它希望访问的 object？\n\n\n提供一个机制，能阻塞客户请求。我们有时候甚至希望有计划有策略地阻塞客户端请求。实现一些 QoS 的功能。\n\n\n\n\nCrimson 为了解决这些需求，引入了一组概念\n\n\n\nOperation\n\n代表一种操作。比如说刚才提到的客户发来的请求就是一种 Operation。\n\nBlocker\n\n代表处理操作的一个特定的阶段。像刚才提到的 “等 osdmap”，就可以作为一个 Blocker。\n\nPipeline\n\n代表系列 Blocker。在做限流或者 QoS 的时候，pipeline 也是一个资源。\n\n\n\n\n为了集中跟踪当前的 Operation ，我们定义了 ShardServices::start_operation() ，用一个全局的数据结构分门别类记录各种操作。这里以简化版的 ClientRequest::process_op() 为例，解释一下用法\n\n\n\nseastar::future&amp;lt;&amp;gt; ClientRequest::start(PG&amp;amp; pg)\n{\n  return with_blocking_future(handle.enter(connection_pipeline().await_map)\n  ).then([&amp;amp;] {\n    return with_blocking_future(osd.osdmap_gate.wait_for_map(req.min_epoch));\n  }).then([&amp;amp;] {\n    return with_blocking_future(handle.enter(connection_pipeline().get_pg));\n  }).then([&amp;amp;] {\n    return with_blocking_future(osd.wait_for_pg(req.spg));\n  }).then([&amp;amp;] {\n    return with_blocking_future(handle.enter(pg_pipeline(pg).wait_for_active));\n  }).then([&amp;amp;] {\n    return with_blocking_future(pg.wait_for_active_blocker.wait())\n  }).then([&amp;amp;] {\n    return with_blocking_future(handle_enter(pg_pipeline(pg).recover_missing));\n  }).then([&amp;amp;] {\n    return maybe_recover(get_oid());\n  }).then([&amp;amp;] {\n    return with_blocking_future(handle.enter(pg_pipeline(pg).get_obc));\n  }).then([&amp;amp;] {\n    return pg.with_locked_obc(get_op_info(), [&amp;amp;] {\n      return with_blocking_future(handle_enter(pg_pipeline(pg).process)).then([&amp;amp;] {\n        return process(req);\n      }).then([&amp;amp;](auto reply) {\n        return conn-&amp;gt;send(reply);\n      });\n    });\n  });\n}\n\n\n\n处理一个请求有多个步骤，它们构成一条流水线\n\n\n\n\n\n\n\n其中，绿色和蓝色分别代表一条小流水线。\n\n\n\n\n绿色的是 connection pipeline。每个客户端来的链接都有一条。它分两个阶段\n\n\n\nwait until osd gets osdmap\n\n\nwait for PG\n\n\n\n\n\n蓝色的是 PG pipeline。每个 PG 都有一条。\n\n\n\nwait until PG gets osdmap\n\n\nwait for active\n\n\nwait for object recovery\n\n\nwait for object context\n\n\nwait for process\n\n\n\n\n\n\n\n一条流水线就像博物馆的一层楼。流水线分多个阶段，每个阶段就像一层楼里面的一个个展厅。第一次去一个博物馆或者艺术馆，我一般会拿着小册子，按照顺序，一个展厅一个展厅地逛。因为不会分身大法，所以也没办法同时在几个展厅参观。这个设定和 Ceph 处理客户端请求是很像的。但是即使是在疫情期间，博物馆的展厅也能同时容纳不止一个人。那么这些 pipeline 呢？在同一时刻，我们允许多个请求停留在同一阶段吗？\n\n\n答案是“不可以”。\n\n\n\nblocking_future&amp;lt;&amp;gt;\nOrderedPipelinePhase::Handle::enter(OrderedPipelinePhase &amp;amp;new_phase)\n{\n  auto fut = new_phase.mutex.lock();\n  exit();\n  phase = &amp;amp;new_phase;\n  return new_phase.make_blocking_future(std::move(fut));\n}\n\n\n\n其中 new_phase 就是即将进入的新展厅：\n\n\n\nclass OrderedPipelinePhase : public Blocker {\n  // ...\n  seastar::shared_mutex mutex;\n};\n\n\n\n要是不熟悉 seastar::shared_mutex，可以把它理解成和 std::shared_mutex 有类似语义的共享锁。但是这里调用的是霸气的 lock() 而不是 lock_shared() 。所以在别人离开展厅之前，你是没法进去的。同样，要是你在里面，别人只能止步。\n\n\n这个气氛好像很不和谐。就像好多人希望去一个很大的展厅，他们想看的展品其实都不一样，但是因为里面已经有\n一个人 了，所以大家只能依次在门外排队，等出来一个人，才能进去一个人。这个情形倒很像是高峰时段的厕所。\n\n\n如果有多个不同的请求正好访问同一 PG，即使它们对应的是不同的 object，也不得不互相保持二十米的距离，挨个等待 PG.client_request_pg_pipeline.process 。前面一个人不完事儿，后面的人必须等着。\n\n\n这不又是 PG 大锁嘛。我在例会上提出来这个问题。Sam 提醒我。可以从另外一个角度看这个问题。每个 OSD\n都有上百个 PG，而一个集群有成百上千个 OSD。如果 PG 的分布很均匀，那么每个 PG 同时需要处理的请求其实是不多的。而且，我一直有个误解，就是觉得 PG 大锁是现有 OSD 性能不彰的原因之一。但是 Sam 告诉我，问题不在于 lock contention，而在于 CPU 的使用率居高不下。这才想起来，官方的文档建议一个 OSD 一般需要配备两个核。\n\n\n不过还是有点放不下心，最好我们能在各个“展厅”前面设置一个计时器，看看大家都在门外面等了多久。这也是流水线机制设立的一个初衷。\n"
} ,
  
  {
    "title"    : "seastar::future 中的 ownership",
    "category" : "",
    "tags"     : " c++, seastar",
    "url"      : "/2020/10/16/ownership-in-seastar.html",
    "date"     : "October 16, 2020",
    "excerpt"  : "\nseastar::future 在 get() 之后就像被掏空了一样。\n\n\n最近有同事说，在测试 Release build 的时候发现 crimson::do_until() 会产生 segfault。重现的代码很简单：\n\n\n\n  future&amp;lt;&amp;gt; test()\n  {\n    return crimson::do_until([this]() -&amp;gt; future&amp;lt;bool&amp;gt; {\n      if (i &amp;lt; 5) {\n        ++i;\n   ...",
  "content"  : "\nseastar::future 在 get() 之后就像被掏空了一样。\n\n\n最近有同事说，在测试 Release build 的时候发现 crimson::do_until() 会产生 segfault。重现的代码很简单：\n\n\n\n  future&amp;lt;&amp;gt; test()\n  {\n    return crimson::do_until([this]() -&amp;gt; future&amp;lt;bool&amp;gt; {\n      if (i &amp;lt; 5) {\n        ++i;\n        return ertr::make_ready_future&amp;lt;bool&amp;gt;(false);\n      } else {\n        return ertr::make_ready_future&amp;lt;bool&amp;gt;(true);\n      }\n    });\n  }\n\n\n\n看了下，的确如此。祭出 seastar-addr2line\n\n\n\n?? ??:0\nseastar::internal::future_base::detach_promise() at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1169\n (inlined by) seastar::internal::future_base::schedule(seastar::task*, seastar::future_state_base*) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1175\n (inlined by) seastar::future&amp;lt;bool&amp;gt;::schedule(seastar::continuation_base&amp;lt;bool&amp;gt;*) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1372\n (inlined by) void seastar::future&amp;lt;bool&amp;gt;::schedule&amp;lt;seastar::internal::promise_base_with_type&amp;lt;&amp;gt;, crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}, seastar::future&amp;lt;bool&amp;gt;::then_impl_nrvo&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;, crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; &amp;gt;(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)::{lambda(seastar::internal::promise_base_with_type&amp;lt;&amp;gt;&amp;amp;&amp;amp;, a_basic_test_t::test()::{lambda()#1}&amp;amp;, seastar::future_state&amp;lt;bool&amp;gt;&amp;amp;&amp;amp;)#1}&amp;gt;(seastar::internal::promise_base_with_type&amp;lt;&amp;gt;, crimson::errorated_future_marker&amp;lt;&amp;gt;&amp;amp;&amp;amp;, seastar::future&amp;lt;bool&amp;gt;::then_impl_nrvo&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;, crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; &amp;gt;(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)::{lambda(seastar::internal::promise_base_with_type&amp;lt;&amp;gt;&amp;amp;&amp;amp;, a_basic_test_t::test()::{lambda()#1}&amp;amp;, seastar::future_state&amp;lt;bool&amp;gt;&amp;amp;&amp;amp;)#1}&amp;amp;&amp;amp;) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1391\n (inlined by) crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; seastar::future&amp;lt;bool&amp;gt;::then_impl_nrvo&amp;lt;crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}, crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; &amp;gt;(crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1571\n (inlined by) crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; seastar::future&amp;lt;bool&amp;gt;::then_impl&amp;lt;crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}, crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; &amp;gt;(crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1605\n (inlined by) seastar::internal::future_result&amp;lt;a_basic_test_t::test()::{lambda()#1}, bool&amp;gt;::future_type seastar::internal::call_then_impl&amp;lt;seastar::future&amp;lt;bool&amp;gt; &amp;gt;::run&amp;lt;crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}&amp;gt;(seastar::future&amp;lt;bool&amp;gt;&amp;amp;, crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1234\n (inlined by) crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; seastar::future&amp;lt;bool&amp;gt;::then&amp;lt;crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}, crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;&amp;gt; &amp;gt; &amp;gt;(crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}) at /var/ssd/ceph/build-release/../src/seastar/include/seastar/core/future.hh:1520\n (inlined by) auto crimson::errorator&amp;lt;crimson::unthrowable_wrapper&amp;lt;std::error_code const&amp;amp;, crimson::ec&amp;lt;(std::errc)22&amp;gt; &amp;gt; &amp;gt;::_future&amp;lt;crimson::errorated_future_marker&amp;lt;bool&amp;gt; &amp;gt;::_then&amp;lt;crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}&amp;gt;(crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1})::{lambda(a_basic_test_t::test()::{lambda()#1}&amp;amp;&amp;amp;)#1}) at /var/ssd/ceph/build-release/../src/crimson/common/errorator.h:676\n (inlined by) auto crimson::do_until&amp;lt;a_basic_test_t::test()::{lambda()#1}&amp;gt;(a_basic_test_t::test()::{lambda()#1}) at /var/ssd/ceph/build-release/../src/crimson/common/errorator.h:68\nseastar::noncopyable_function&amp;lt;void ()&amp;gt;::direct_vtable_for&amp;lt;seastar::async&amp;lt;a_basic_test_t_0_basic_Test::TestBody()::{lambda()#1}&amp;gt;(seastar::thread_attributes, std::decay&amp;amp;&amp;amp;, (std::decay&amp;lt;a_basic_test_t_0_basic_Test::TestBody()::{lambda()#1}&amp;gt;::type&amp;amp;&amp;amp;)...)::{lambda()#1}&amp;gt;::call(seastar::noncopyable_function&amp;lt;void ()&amp;gt; const*) at /var/ssd/ceph/build-release/../src/test/crimson/test_errorator.cc:22\nseastar::noncopyable_function&amp;lt;void ()&amp;gt;::operator()() const at /var/ssd/ceph/build-release/../src/seastar/include/seastar/util/noncopyable_function.hh:201\n (inlined by) seastar::thread_context::main() at /var/ssd/ceph/build-release/../src/seastar/src/core/thread.cc:297\n\n\n\n这个问题的特点是 Release 版本才有。注意到 Seastar 中 future::schedule() 的实现 (文中把过长的行折成多行，方便阅读)\n\n\n\n    template &amp;lt;typename Pr, typename Func, typename Wrapper&amp;gt;\n    void schedule(Pr&amp;amp;&amp;amp; pr, Func&amp;amp;&amp;amp; func, Wrapper&amp;amp;&amp;amp; wrapper) noexcept {\n        // If this new throws a std::bad_alloc there is nothing that\n        // can be done about it. The corresponding future is not ready\n        // and we cannot break the chain. Since this function is\n        // noexcept, it will call std::terminate if new throws.\n        memory::disable_failure_guard dfg;\n        auto tws = new continuation&amp;lt;Pr, Func, Wrapper, T SEASTAR_ELLIPSIS&amp;gt;(std::move(pr),\n                                                                           std::move(func),\n                                                                           std::move(wrapper));\n        // In a debug build we schedule ready futures, but not in\n        // other build modes.\n#ifdef SEASTAR_DEBUG\n        if (_state.available()) {\n            tws-&amp;gt;set_state(std::move(_state));\n            ::seastar::schedule(tws);\n            return;\n        }\n#endif\n        schedule(tws);\n        _state._u.st = future_state_base::state::invalid;\n    }\n\n\n\n其中对 Debug 版本有特殊的处理，如果 future::_state 当时就有，那么调用 continuation_base::set_state() 把 _state 搬到新建的 tws 里面。\nfuture_state::move_it() 是 future_state(future_state&amp;amp;&amp;amp;) 的具体实现。它比较直接，把值 move 或者 memmove 到自己手里面。\n\n\n但是 Release 版则会调用 future_base::schedule(tws, &amp;amp;tws&amp;#8594;_state)\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n    promise_base* detach_promise() noexcept {\n        _promise-&amp;gt;_state = nullptr;\n        _promise-&amp;gt;_future = nullptr;\n        return std::exchange(_promise, nullptr);\n    }\n\n    void schedule(task* tws, future_state_base* state) noexcept {\n        promise_base* p = detach_promise();\n        p-&amp;gt;_state = state;\n        p-&amp;gt;_task = tws;\n    }\n\n\n\n\nsegfault 发生在第 2 行，所以说 _promise 在那个时候已经是个空指针。这是谁干的呢？我们回过头看看 do_until() 的实现\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\ntemplate&amp;lt;typename AsyncAction&amp;gt;\ninline auto do_until(AsyncAction action) {\n  using futurator = \\\n    ::seastar::futurize&amp;lt;std::result_of_t&amp;lt;AsyncAction()&amp;gt;&amp;gt;;\n\n  while (true) {\n    auto f = futurator::invoke(action);\n    if (!seastar::need_preempt() &amp;amp;&amp;amp; f.available() &amp;amp;&amp;amp; f.get()) {\n      return futurator::type::errorator_type::template make_ready_future&amp;lt;&amp;gt;();\n    }\n    if (!f.available() || seastar::need_preempt()) {\n      return std::move(f)._then(\n        [ action = std::move(action)] (auto &amp;amp;&amp;amp;done) mutable {\n          if (done) {\n            return futurator::type::errorator_type::template make_ready_future&amp;lt;&amp;gt;();\n          }\n          return ::crimson::do_until(\n            std::move(action));\n        });\n    }\n    if (f.failed()) {\n      return futurator::type::errorator_type::template make_exception_future2&amp;lt;&amp;gt;(\n        f.get_exception()\n      );\n    }\n  }\n}\n\n\n\n\n思路很简单，就是递归调用，直到 f 返回真。因为递归是通过 post message 风格的调用实现的，所以不需要担心栈的大小问题。其中最可疑的地方就是 ._then() 了，它其实就是 future::then()。后者分情况讨论，如果 future 的 state 是立等可取的，那么就直接 futurator::invoke() 了，否则调用 then_impl_nrvo()。接下来则是 future::schedule()。schedule() 会把 future\n的 _promise 取走，留下一个空指针。这下子就和前面的 backtrace 对上了。但是稍等，为什么要调用\nschedule() 呢？test() 里面都返回的 future 的 state 都是 available 的啊。\n\n\n我们再看看 future::get() 吧\n\n\n\n    [[gnu::always_inline]]\n    value_type&amp;amp;&amp;amp; get() {\n        wait();\n        return get_available_state_ref().take();\n    }\n\n\n\n这个 take() 很奇怪。get() 和 take() 的语义是不一样的。一个是返回拷贝或者引用，一个则是从所有者手中 夺走，然后返回抢到的东西。果不其然：\n\n\n\n    T&amp;amp;&amp;amp; take() &amp;amp;&amp;amp; {\n        assert(available());\n        if (_u.st &amp;gt;= state::exception_min) {\n            std::move(*this).rethrow_exception();\n        }\n        _u.st = state::result_unavailable;\n        return static_cast&amp;lt;T&amp;amp;&amp;amp;&amp;gt;(this-&amp;gt;uninitialized_get());\n    }\n\n\n\n所以 take() 之后，future 里面原来的 state 成了 unavailable 的状态。难怪 do_until()\n回过头再看 f 的时候，它已经变成了 unavailable，所以就傻乎乎地去调用 _then() 了。\n\n\n再看看 get_available_state_ref()\n\n\n\n    [[gnu::always_inline]]\n    future_state&amp;amp;&amp;amp; get_available_state_ref() noexcept {\n        if (_promise) {\n            detach_promise();\n        }\n        return std::move(_state);\n    }\n\n\n\n原来 _promise 是在这里被拿走的，罪魁祸首并非 schedule()。人家只是受害者。调整一下顺序，最后再 get()，问题就解决了。\n\n\n这两天学 Rust。现炒现卖一下，用 Rust 来写这个有 bug 的 do_until，就是\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nstruct Future {\n  state: State,\n}\n\nimpl Future {\n  fn get(self) {\n    // takes the ownership of self\n  }\n  fn _then(self) {\n    // also takes the ownership of self\n  }\n}\n\nloop {\n  let mut f = futurator::invoke(action);\n  if (!seastar::need_preempt() &amp;amp;&amp;amp; f.available() &amp;amp;&amp;amp; f.get()) {\n    return now();\n  }\n  if (!f.available() || seastar::need_preempt()) {\n    return f._then( /* */);\n  }\n}\n\n\n\n\n编译的时候 rustc 就会出错：\n\n\n\nerror[E0382]: use of moved value: `f`\n --&amp;gt; src/main.rs:19:9\n   |\n16 |     if (!seastar::need_preempt() &amp;amp;&amp;amp; f.available() &amp;amp;&amp;amp; f.get()) {\n   |                                                      -- value moved here\n17 |       return now();\n18 |     }\n19 |     if (!f.available() || seastar::need_preempt()) {\n   |          ^^ value used here after move\n   |\n  = note: move occurs because `f` has type `Future`, which does not implement\n  the `Copy` trait\n\n\n\n顿时有弃暗投明的冲动。\n"
} ,
  
  {
    "title"    : "求值的顺序",
    "category" : "",
    "tags"     : " c++, seastar",
    "url"      : "/2020/10/12/order-of-evaluation.html",
    "date"     : "October 12, 2020",
    "excerpt"  : "\n用 Seastar 的时候，常常需要推迟一个对象的析构。于是问题来了。\n\n\n平时，我们这样写程序：\n\n\n\nvoid scan(func_t&amp;amp;&amp;amp; f)\n{\n  Node root = get_root();\n  return root.scan(std::move(f));\n}\n\n\n\n但是用 Seastar 的话，因为 get_root() 可能会阻塞，我们可能可以把代码写成下面这样：\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;...",
  "content"  : "\n用 Seastar 的时候，常常需要推迟一个对象的析构。于是问题来了。\n\n\n平时，我们这样写程序：\n\n\n\nvoid scan(func_t&amp;amp;&amp;amp; f)\n{\n  Node root = get_root();\n  return root.scan(std::move(f));\n}\n\n\n\n但是用 Seastar 的话，因为 get_root() 可能会阻塞，我们可能可以把代码写成下面这样：\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;amp; f)\n{\n  return get_root().then([f=std::move(f)](Node&amp;amp;&amp;amp; root) {\n    return root.scan(std::move(f));\n  });\n}\n\n\n\n那么既然 get_root() 是异步的，那么等到调用 then() 的时候，f 会不会已经析构了呢？我们是不是应该这么写？\n\n\n\nseastar::future&amp;lt;&amp;gt; scan(func_t&amp;amp;&amp;amp; f)\n{\n  return seastar::do_with(std::move(f), [this](auto&amp;amp; f) {\n    return get_root().then([&amp;amp;f](Node&amp;amp;&amp;amp; root) {\n      return root.scan(f);\n    });\n  });\n}\n\n\n\n这里就是例子：\n\n\n\nstruct foo_t {\n  foo_t&amp;amp; func(int i) {\n    cout &amp;lt;&amp;lt; &quot;func(&quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;)&quot; &amp;lt;&amp;lt; endl;\n    return *this;\n  }\n};\n\nint gen(int i) {\n  cout &amp;lt;&amp;lt; &quot;gen(&quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;)&quot; &amp;lt;&amp;lt; endl;\n  return i;\n}\n\nint main()\n{\n  foo_t foo;\n  foo.func(1).func(gen(2));\n}\n\n\n\n的输出是:\n\n\n\nfunc(1)\ngen(2)\nfunc(2)\n\n\n\n由此可知，gen(2) 是在 func(1) 返回之后才调用的。 而 get_root() 里面的代码如果是异步调用的话，可能在 scan() 返回的时候也还没有“完成”。因为异步调用返回的是一个 future ，如果 future state 当时还没有准备好，那么 .then(func) 则会把 func 包装成一个 task 等待调度。\n\n\n但是从 C&amp;#43;&amp;#43; 的角度来说呢？简化版本的代码中，有这么个表达式\n\n\n\nfoo.func(1).func(gen(2));\n\n\n\n它用来模拟 Seastar 里面 .then() 的调用，方便理解求值的先后顺序。很明显，这里的 AST 的树根是个函数调用。\n\n\n\n\n\n\n\nn4659 中， expr.call 一节说道\n\n\n\n\nA function call is a postfix expression followed by parentheses containing a possibly empty, comma-separated list of initializer-clauses which constitute the arguments to the function.\n\n\n\n\n接着标准规定了函数参数的求值顺序\n\n\n\n\nThe postfix-expression is sequenced before each expression in the expression-list and any default argument. The initialization of a parameter, including every associated value computation and side effect, is indeterminately sequenced with respect to that of any other parameter.\n\n\n\n\n用通俗易懂的话，就是：\n\n\n\n\nWhen calling a function (whether or not the function is inline, and whether or not explicit function call syntax is used), every value computation and side effect associated with any argument expression, or with the postfix expression designating the called function, is sequenced before execution of every expression or statement in the body of the called function.\n\n\n\n\n所以，我们这个例子里面 .then() 有两个参数，在真正调用 .then() 之前，我们必须先对这两个参数求值。\n\n\n\n\n一个是 this，它的值由 get_root() 返回，为了得到这个参数必须对 get_root() 求值。\n\n\n另一个是一个 lambda 表达式，它的值由 capture list 和后面的函数体决定。但是请注意，要对这个表达式求值并不需要执行这个 lambda 表达式。它的值就是一个 lambda 表达式。\n\n\n\n\n所以在调用 .then() 之前，f 的值就被稳妥地保存在第二个参数里面了，并且因为我们是 capture by move，所以第二个参数析构的时候，f 也会随之而去。我们并不需要为它专门做一个 seastar::do_with() 用智能指针保存它的值，延长其生命周期。\n\n\n回到一开始的 foo_t 的那个例子，其实它有些许误导。我们按照结合律，可以把这个表达式拆成这么几个\n\n\n\n\n\n\n\n所以对第二个 .func() 求值，我们必须先对 foo.func(1) 和 gen(2) 求值，当然它们的顺序不一定。然后再调用 foo.func(2)。\n\n\n但是和前文 scan() 的例子不一样，scan() 的第二个参数是个 lambda 表达式，为了对它求值，我们必须初始化 lambda 表达式中的 capture 列表。所以看上去好像有“写在后面的代码反而在之前执行了”的错觉。但是如果把语法关系理清楚，这个问题就迎刃而解了。\n"
} ,
  
  {
    "title"    : "Read the Docs 之路",
    "category" : "",
    "tags"     : " ceph, ci",
    "url"      : "/2020/10/02/road-to-rtd.html",
    "date"     : "October 2, 2020",
    "excerpt"  : "\n\n\n最近为了支持多单词的准确搜索，把 Ceph 的文档编译和 host 转移到了 Read the Docs 上。但是还有一些问题。\n\n\n\n\nCeph 里的 API 文档\nSphinx 的搜索\n假的 librados\n方案\n\nSphinx 能看见的预处理结果\n加入 stub 函数\n浏览器能看到的\n\n\n后记\n\n\n\n\n\nCeph 里的 API 文档\n\n\nCeph 用 Sphinx 编译文档。它作为一个平台提供 librados，让大家可以用它写程序。librados 有各种语言的绑定，其中一...",
  "content"  : "\n\n\n最近为了支持多单词的准确搜索，把 Ceph 的文档编译和 host 转移到了 Read the Docs 上。但是还有一些问题。\n\n\n\n\nCeph 里的 API 文档\nSphinx 的搜索\n假的 librados\n方案\n\nSphinx 能看见的预处理结果\n加入 stub 函数\n浏览器能看到的\n\n\n后记\n\n\n\n\n\nCeph 里的 API 文档\n\n\nCeph 用 Sphinx 编译文档。它作为一个平台提供 librados，让大家可以用它写程序。librados 有各种语言的绑定，其中一些有对应的文档：\n\n\n\n\nC: 用 Breathe 的 autodoxygenfile directive\n\n\nC&amp;#43;&amp;#43;: 还没有加上去。不过我觉得用 Breathe 和 Doxygen 的组合应该就够了\n\n\nPython: Sphinx 有内置的支持，即 sphinx.ext.autodoc 扩展。我们主要用它的 automethod directive\n\n\nOpenAPI: 用 sphinxcontrib.openapi 提供的 openapi directive\n\n\n\n\n\n\nSphinx 的搜索\n\n\nSphinx 内置有搜索功能，它也支持多词搜索，但是通常我们希望搜索 source code 的话，Sphinx 会返回所有包含 source code 的文档，即使文档里面出现的是 &quot;code source&quot; 或者 &quot;source of code&quot;。换言之，它不是我们习惯上的多词搜索。这个问题其他开发者也碰到了，在 Sphinx 上有相关的 issue。我研究了一下，这个问题在于 Sphinx 搜索的实现比较直接。它分这么几步\n\n\n\n\n分词。每种语言的分词都不一样。值得一提的是，中文分词用的是“结巴”分词\n\n\n逐词预处理，用对应语言的 stemming 规则把词归一化。英语的实现可以参考这里\n\n\n\n去掉后缀。比如说 apples 这个词就会变成 apple。civilize 则会变成 civil。\n\n\n去掉常见的介词、连词和代词。比如说 at、and 和 they 就会被去掉。\n\n\n\n\n\n把索引关系加入倒排表。组织成一个大数据结构，保存在磁盘上。\n\n\n在搜索的时候，javascript 会直接从这个表里面找。\n\n\n\n\n所以可以想见，如果我们搜索 &quot;fuse support&quot; 想看看 Ceph 对 FUSE 的支持，它会返回 mount.fuse.ceph 的 manpage。这虽然也不算离谱，但是里面出现的 support 是这么一句话\n\n\n\n\nThe old format /etc/fstab entries are also supported:\n\n\n\n\n通篇没有出现 &quot;fuse support&quot; 这个序列。搜索返回了 32 篇文章，后 31 篇 文章被检索到的关键字就是 unsupported。这个很可能不是我们想要的。而 RTD 的多词搜索的结果要好很多。对于不挑剔的读者基本上够用了。\n\n\n那为什么要纠结多词搜索呢？因为我们很多命令是多个单词构成的。比如说\n\n\n\nceph df\n\n\n\n要是用户想搜索 &quot;ceph df&quot;，多词搜索要是能精确匹配，问题不就能解决了吗？那有没有其他办法呢？\n\n\n\n\ngoogle 的站内搜索。但是 google 是一个商业公司。有的人可能会浑身不自在，如果他用一个广告公司的搜索。虽然在这个世界上，我们和商业公司有千丝万缕的联系，但是，哎。让我们留一点理想主义的念想吧。\n\n\n直接用 Read the Docs 的一揽子方案。它和 travis 这些服务很像，不仅内置了 CI 的功能，也能帮着 host 这些静态页面。对我们很合适。但是它的 build 流程是很死的。看看它的配置文件就知道了。这是为一个纯 Python 项目度身定制的。我们后文分析这个限制的影响。\n\n\n其他 sphinx search plugin。找了一圈，没有不收费的。功能比较好的也需要自己搭建 Elasticsearch。RTD 开源了他们的方案。但是一想到要挠我们实验室小哥的门，我就知趣地把爪子收起来了。\n\n\n\n\n\n\n假的 librados\n\n\nRead the Docs 的搜索不错，但是它的限制也很明显。\n\n\n\n\n只能通过 requirements.txt 安装第三方依赖。那么 requirements.txt 到底是啥呢？它是 pip 用来给 pip install 传参数的。 文档说得明白。\n\n\n也可以用 setuptools 或者 pip 安装源码里面的 Python 项目。\n\n\n没有预处理阶段。pip 装好了，直接就是 sphinx-build。\n\n\n\n\n我们回到各种语言的绑定：\n\n\n\n\nC: Breathe 其实本身并不能解析 C 代码里面的注释，也不能理解头文件。它事实上担当的角色是 Doxygen 产生的 XML 文件到 Sphinx 中间的桥梁。但是如果这些 XML 不存在，巧妇难为无米之炊。所以它会调用用 Doxygen 预处理一下指定的文件。但是问题来了，doxygen 怎么安装呢？它是一个 C++ 的项目。\n\n\nPython: automethod 读取制定方法的 docstring，产生 Sphinx 的文档。最近一部分代码用上了 PEP484 风格的标注，所以我们也用 sphinx_autodoc_typehints 来把这些标注变成 Sphinx 文档。这两种办法都要求 Sphinx 的 Python 环境能访问被处理的 Python 扩展 (模块)。\n\n\nOpenAPI: openapi 读取的是一个 yaml 文件。我们目前解决这个问题的办法是直接把这个文件放到了 repo 里面。但是大家都知道这个 yaml 文件其实是从代码产生的。把预处理的结果放到 repo 里面显然不是一个最好的方案，在现阶段这是一个折中。\n\n\n\n\n但是对于 Python API 来说，以 python-rados 为例，它是用 Cython 编写的 Python 扩展，它的底层则是 librados C API。我们编译文档的时候其实并不需要一个功能上完备的 librados，我们只需要让 sphinx 能导入 python-rados 就行了。sphinx 并不会真正运行 python-rados 的函数，它只会读取代码里面的元数据。所以 Ceph 里面用了一个比较取巧的方法。\n\n\n\n\n为 lib/librados.so 建立一个空链接，指向 lib/librados.so.1\n\n\n用 GCC 编译一个空的 lib/librados.so.1\n\n\n用 pip 安装 python-rados，pip 会自动执行 setup.py 脚本，后者会\n\n\n\n调用 Cython 编译对应的 rados.pyx，生成 C 代码，然后\n\n\nGCC 继续用指定源代码里的头文件目录，刚才生成的空动态链接库，生成 rados 的 Python 扩展。\n\n\n\n\n\n至此，rados 的 python 扩展编译好了。但是它链接的 librados.so 只是个空壳子。如果有人希望 import rados ，一定会出错。因为那些符号都不存在呢。所以我们用 nm 分析这个 Python extension，找出它引用的所有符号，看看它有没有 librados  API 的前缀。把这些符号，其实也是函数，统一写成 void func(void) {} 的样子，用管道交给 GCC 生成新的 lib/librados.so.1 。虽然它是假的，但是至少 import 的时候就不会出错了。\n\n\n\n\n\n\n\n\n\nOpenAPI 文档的 yaml 文件的产生过程要简单很多，但是也需要使用我们自己编写的 python 脚本。但是 RTD 的 requirements.txt 没法实现这么复杂的预处理逻辑。\n\n\n\n\n方案\n\n\nSphinx 能看见的预处理结果\n\n为了能有一个 librados，我们可以在 PyPI 注册一个项目，让 Ceph 发布新版本的时候也更新它。同时，我们的文档编译流程也能直接从 PyPI 安装 python-rados。openapi.yaml 其实也可以放在这里面。具体说就是\n\n\n\n\n注册 python-rados 项目。其他 Python 绑定也同理，比如 cephfs、rgw、rbd。\n\n\n一旦修改任何 Python 绑定的 pyx，就需要发布一个新版。\n\n\n让 ceph/admin/doc-read-the-docs.txt 安装 python-rados， python-cephfs 等。\n\n\n\n\n\n加入 stub 函数\n\n在编译文档的时候，在 rados.pyx 中实现所有使用到的 C 函数。不过需要注意，这些函数也应该暴露出来给 python-cephfs 它们用。当然，只有在编译文档的时候才这么做。\n\n\n\n浏览器能看到的\n\n另外一个办法就是保留我们的 CI 流程，让它编译 API 相关的文档，然后让 RTD 的文档引用我们自己编译的文档。这需要\n\n\n\n\n新建一个域名，专门用来保存 API 文档。题外话，它也可以用来保存 CI 产生的文档。\n\n\n修改文档里面所有引用 API 文档的超链接，加入条件：\n\n\n\n如果是 RTD 编译的话，就链接到刚才的域名\n\n\n其他情况，就使用相对路径\n\n\n\n\n\n\n\n\n\n\n后记\n\n\n最后采用的是 stub 函数的方案。毕竟用 Cython 写个假的实现相对比较容易。同时，因为我们有很多 tell 命令，它们中有的是 C 实现的，有的是 Python 实现的。前者有固定格式的头文件来描述命令的参数，后者是用 type annotation 来标记参数类型。之前为了产生对应的文档，我们有专门的 Python 脚本。但是因为 Read the Docs 不支持这样的流程。所以为了能从 C 代码和 Python 代码生成文档，专门写了一个 sphinx 扩展。好在 sphinx 允许在扩展里面直接插入 reStructuredText 风格的 markup，这样搭配 Jinja 就方便多了。有点 PHP 的感觉，吼吼。\n\n\n"
} ,
  
  {
    "title"    : "std::error_code",
    "category" : "",
    "tags"     : " c++",
    "url"      : "/2020/09/30/error-code.html",
    "date"     : "September 30, 2020",
    "excerpt"  : "\nC++11 中的错误处理的核心是 std::error_code。它的引进满足了错误处理可扩展的需求。\n\n\n\n\nvalue() 返回一个 int，这个是 error code 本身。\n\n\ncategory() 则是“可扩展的关键”。它允许在不同上下文或者说 domain 里面，有相同 value 的 error_code 代表不同的意思。不过，value 为 0 一般来说，就是没有错误。这个是约定俗成的。\n\n\n\n\nerror_code 有三个构造函数，其中， template&amp;lt;cl...",
  "content"  : "\nC++11 中的错误处理的核心是 std::error_code。它的引进满足了错误处理可扩展的需求。\n\n\n\n\nvalue() 返回一个 int，这个是 error code 本身。\n\n\ncategory() 则是“可扩展的关键”。它允许在不同上下文或者说 domain 里面，有相同 value 的 error_code 代表不同的意思。不过，value 为 0 一般来说，就是没有错误。这个是约定俗成的。\n\n\n\n\nerror_code 有三个构造函数，其中， template&amp;lt;class ErrCodeEnum&amp;gt; error_code(ErrCodeEnum e) 是最有意思的。它调用的是 make_error_code() 。看上去，似乎它让我们能从自定义的类型构造出 error_code。是的，不过我们需要用\n\n\n\nstd::is_error_code_enum&amp;lt;ErrorCodeEnum&amp;gt;::value == true\n\n\n\n来明确地规定这个行为。只有为这个类型定义特化的模板，才能让该类型能转换到 error_code。\n\n\n请注意，std::error_code 定义了多个 operator==()，它不仅仅可以和相同类型的 std::error_code 比较，而且可以和另一个 std::error_condition 比较。只要后者和前者 equivalent 或者反之，那么就把他两个视为相等。\n\n\n\nbool operator==(const error_code&amp;amp; __x, const error_condition&amp;amp; __y) _NOEXCEPT\n{\n    return __x.category().equivalent(__x.value(), __y)\n        || __y.category().equivalent(__x, __y.value());\n}\n\n\n\n具体分析，如果 Seastar 底下抛出来的是一个 std::system_error(ret, std::system_category())，我们希望看看这个 exception 是不是匹配我们的 conditon。第一个判断是\n\n\n\n\nsystem_category.equivalent(ret, condition) &amp;#8658; default_error_condition(code) == condition\n\n\n\nsystem_category() 返回的是一个 system_error_category 的 singleton，它的 default_error_condition(int ev) 的定义类似\n\n\nreturn error_condition(ev, generic_category())\n\n\n\n\n\n\n\n\n\n但是如果 ELAST 这个宏定义了的话，当 ev &amp;gt; ELAST, 返回的 category 则是 system_category() 而非 generic_category()\n  所以，如果 == 的右边是一个 error_condition，而且 val 也是 ret，category 也是 generic_category 的话，那么就是匹配的。\n\n\n\n\n\n\n如果根据左边 category 判断不匹配，我们需要看看右边的意见。\n\n\n\n\n\nconditon.category.equivalent(const error_code&amp;amp; code, int condition)\n\n\n\n如果是我们自定义的 condition 的话，就根据自定义的实现来判断。\n\n\n否则就看缺省的实现\n\n\n*this == code.category() &amp;amp;&amp;amp; code.value() == condition;\n\n\n\n所以，首先要求 y.category() == x.category()，即，两者同属一个 category；而且要求 x.value() == y.value() 即两者的 value 一致。\n\n\n\n\n\n\n\n\n这就是为什么下面的代码是有效的原因：\n\n\n\ntry {\n  // ..\n} catch (const std::system_error&amp;amp; e) {\n  if (e.code() == std::errc::invalid_argument) {\n    //..\n  }\n}\n\n\n\n请注意，std::errc::invalid_argument 本身并不是一个 std::error_condition。它只是标准库用来表示 error_condition 的一系列 errc 枚举中的一个。 标准库不希望用平台或是领域相关的 errno 来直接表示错误，而是想把不同平台和 domain 的比较方式统一到一个框架下面，通过各自定义的 error_condition 来进行比较。std::errc 就是这个框架下面的一个例子，它表示平台无关的各种系统错误，通过 error_condition 得以用来于判断和比较 std::error_code。和 std::error_code 类似，error_condition 本身也应该是可以扩展的，它的几个构造函数和 error_code 如出一辙，第三个构造函数是\n\n\n\ntemplate&amp;lt;class ErrorConditionEnum&amp;gt; error_condition(ErrorConditionEnum e)\n\n\n\n和 error_code 也一样，它要求下面的特化来限定这个行为，只有满足它，error_condition 的这个构造函数才会去调用 std::make_error_condition(errc)：\n\n\n\nstd::is_error_condition_enum&amp;lt;T&amp;gt;::value == true\n\n\n\n不过和 std::error_code 不一样，std::error_condition 是平台无关的。std::error_code 用来表示一个平台相关和领域相关的错误，而 std::error_condition 专门用来处理\n\n\nsystem_error 实例的 code() 返回的是一个 std::error_code() 引用，所以，它可以用多态的方式来比较。这里，std::errc::invalid_argument 是一个 std::errc 的 enum class 成员。按照 C&amp;#43;&amp;#43;11 它可以自动转换成为 error_condition 以方便 error_code 来比较。而标准库规定了 std::is_error_condition_enum&amp;lt;std::errc&amp;gt;::value == true 的特化。所以，当我们比较 std::error_code 和 std::errc 的时候，C&amp;#43;&amp;#43; 会把后者自动转换成对应的 std::error_condition。不过，也不全然是“自动”的。因为这里调用的 make_error_condition(errc) 其实也是一个特化，否则谁知道这个 error_condition 里面的 value 和 category 应该是什么样子呢？\n\n\n错误处理的三驾马车分别是 error_code, error_condition 和 error_category。error_category 是一个多态的类型。他负责\n\n\n\n\n比较 (int code, error_condition&amp;amp;), 或者 (error_code&amp;amp;, int condition) 这两个 equivalanet() 正是 error_code operator==() 所使用的两个条件。\n\n\nname 自报家门，我是哪个 domain 的。\n\n\nmessage(int condition) 这个 error condition 是什么\n\n\ndefault_error_condition(int code) 根据 error code 构造一个 condition\n\n\n\n\n可以说， error_category 是衔接 error_code 和 error_condition 的枢纽。它既知道平台相关 error_code，又了解平台无关的 error_condition。有了它，我们才能知道代码抛出的 code 是不是我们所关心的 condition。\n"
} ,
  
  {
    "title"    : "RADOS 里的顺序",
    "category" : "",
    "tags"     : " ceph",
    "url"      : "/2020/09/20/out-of-order-in-ceph.html",
    "date"     : "September 20, 2020",
    "excerpt"  : "\n\n\n乱序和并发的问题无处不在，Ceph 里面也是这样。\n\n\nRADOS 是 Ceph 的基础。而它在不同的上下文中有不同的含义\n\n\n\n\n集群。比如说数据保存在 RADOS 集群里。\n\n\nAPI。比如说，大家提到 librados，可能就是说用来访问 RADOS 集群的 C API。\n\n\nRADOS 协议。librados 客户端要和集群通信，就基于这个协议。它包含着身份验证，鉴权，控制面的操作，以及 I/O，等等。\n\n\n一个叫做 rados 命令行工具。它可以说是 Ceph 的瑞士军刀。...",
  "content"  : "\n\n\n乱序和并发的问题无处不在，Ceph 里面也是这样。\n\n\nRADOS 是 Ceph 的基础。而它在不同的上下文中有不同的含义\n\n\n\n\n集群。比如说数据保存在 RADOS 集群里。\n\n\nAPI。比如说，大家提到 librados，可能就是说用来访问 RADOS 集群的 C API。\n\n\nRADOS 协议。librados 客户端要和集群通信，就基于这个协议。它包含着身份验证，鉴权，控制面的操作，以及 I/O，等等。\n\n\n一个叫做 rados 命令行工具。它可以说是 Ceph 的瑞士军刀。\n\n\n\n\n这次我们从协议出发，讨论一下 OSD 对读写顺序的处理。它和前面的 多核和顺序 提到的乱序访问内存的问题非常相似。\n\n\n\n\nlibrados 的访问顺序\n\n\n客户端如果要访问集群中的数据，就需要向相关的 OSD 发送 MOSDOp 请求，OSD 则会用 MOSDOpReply 进行回应。它们两个是 RADOS 协议用来传输数据的很重要的消息类型。每个 MOSDOp 都包含\n\n\n\n\n指定要访问的 hobject_t。它包含 pool ，对象的名字，对象的 snapid。\n\n\n一系列 OSDOp，这些 op 就像是一系列指令，不过它们的操作的对象是同一个。要是有写操作，那么写入的数据也放在这个 MOSDOp 里面，按照先后顺序，放在同一个 payload 里面，解码的时候分给各自的 op。\n\n\n\n\nlibrados 为客户端提供了两种调用方式\n\n\n\n\n同步调用。\n\n\n异步调用。\n\n\n\n\n其中，同步调用很好理解。客户端在发送完请求之后，就开始等待，直到收到 OSD 的回应。但是如果客户端选择使用异步调用的话，它就可以同时发送多个 MOSDOp，而不用等待之前的请求返回。从 OSD 的角度来看，它有机会在同一时刻看到同一客户端先后发来的多个请求。不管如何，OSD 都有义务顺序执行这些请求，让客户端收到 MOSDOpReply 的顺序和它当初发送对应 MOSDOp 的顺序一致。Ceph 有个专门的测试，叫做 ceph_test_rados。这个测试会检查客户端收到的回应的顺序是不是对应请求发出的顺序。换言之，如果某个客户端的请求序列是\n\n\n\n\nreq(write(offset=601750, len=535546, payload))\n\n\nreq(write(offset=1929910, len=271840, payload))\n\n\nreq(setxattr(&quot;header&quot;, payload), truncate(size))\n\n\nreq(read(offset=0, len=1))\n\n\n\n\n那么，OSD 的返回序列也应该是\n\n\n\n\nack(write)\n\n\nack(write)\n\n\nack([setxattr,truncate])\n\n\nack(read)\n\n\n\n\n虽然 Ceph 也可以不坚持顺序。我的理解是，sequential consistency 是便于客户端编程的一个模型，所以 Ceph 选择顺序返回是很自然的事情。后来想了一下，对于 RBD 来说，如果系统先发了一个写请求，然后再发一个读请求，那么我们一定要先完成写请求吗？答案是&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;哪有那么巧啊！&quot;`我们`&quot;在这里其实是某个特定的 OSD，这两个时间上相邻的请求，如果不是访问同一个对象，那么怎么会这么巧，这两个对象都被分配到同一个 OSD，而且先后被访问到。就好像你和大学室友在市郊的加油站排队的时候遇见了。如果是同一个对象，那么问题来了。RBD 的使用者为什么会有这种需求呢？先写一块数据，然后不等写完，开始读同一块数据，而且不关心这个读到的数据是不是之前写下去的。RBD 使用者一般来说 Linux 的本地文件系统，它们对一致性还是有要求的，所以一般不会忍受这种脏数据。所以，按照 RBD 的需求分析，这种 load-store 的乱序执行是没有意义的，而且是错误的。\n\n\n\n\nOSD 里的内存屏障\n\n\n在 OSD 的这一边，它可能会看到多个 MOSDOp 同时访问一个对象。这些请求可能来自同一客户端，也可能来自不同的客户端。在保证原子性的前提下，为了提高并发度，我们要区别对待这些 MOSDOp 对应的事务。事务可以分为三种：\n\n\n\n\n只读。很明显这些事务不会改变对象本身，所以它们可以同时进行。\n\n\n只写。提到并发写，有人可能会有点犹豫。不过请不用担心，\n\n\n读然后写或者写然后读。\n\n\n\n\n如果好几个连续的 MOSDOp 都只进行读操作，那么我们是可以同时处理它们的。只需要按照顺序把它们依次发送给 object store 就可以了。 我们把这叫做 pipelined read:\n\n\n\n\n\n\n\n我们甚至可以把这些读请求一起发给下面的 object store，让它酌情同步处理。\n\n\n和 pipelined write:\n\n\n\n\n\n\n\nobject store 自然会把它们序列化，依次处理。因为在 object store 里面也需要多个步骤才能完成一个写操作，所以把它当成一个流水线，提交多个操作更有利于提高并发。\n\n\n但是如果某个请求里面既有写，又有读呢？感受一下：\n\n\n\n\n\n\n\n第二个事务中所有的请求都是读操作。在客户端发送消息的顺序上，这个只读事务排在第一个只写的事务之后。 TCP 保证了 OSD 也是按照发送的顺序收到这两个消息的，但是在 OSD 这边，如果不加以限制的话，或者允许乱序执行的话，就会出现 store-load 重排的情况。因为在 Ceph 里面，读操作一般来说比写操作要快，因为读操作运气好的话，直接在缓存里面就能找到想要的数据，直接返回了。最不济，通过本机读几次磁盘，也能找到想要的数据。但是写操作不仅仅需要读本机磁盘，获得对象的元数据，还需要写本地磁盘，提交分布式事务，让其他副本也持久化，所以它的延迟比读请求是相对高一些的。如果我们放任自流，让两者的延迟决定谁先返回，不仅仅返回的顺序不对，返回的数据也可能是不正确的。如果我们希望实现一个严格的 sequential consistency 的系统，那么 read.2 就有义务体现 write.1 的结果。最简单的办法就是加上一个 sfence，保证 read.2 之前的写操作的事务提交完成。\n\n\n\n\n\n\n\n解决了 store-load 重排，那么 load-store 呢？我们允许在 read.2 仍然进行的时候，开始执行 write.3 吗？这取决于下面 object store 的处理顺序。我们假设这里使用的是 seastore。根据现在 seastore 的设计，要读取某个对象的指定 extent，需要\n\n\n\n\n先根据索引 onode block 的 b+ 树，找到这个对象 onode 所在的 block\n\n\n每个对象自己又有一个 b+ 树管理各自的 extent，如果运气好的话，b+ 树所有的叶子节点就内置在 onode 的 block 里面，但是如果这个对象比较大，或者 extent 的 b+ 树还没有来得及压缩，那么它就会有一些 extent 是需要再查询几个中间节点才能知道具体的逻辑地址的\n\n\n其实上层根据逻辑地址访问下面的物理介质，都需要先把逻辑地址翻译成物理地址，这个过程也需要查索引，也就是要用 LBA 树来查找。而 LBA 树的节点也是不一定都在内存里面。\n\n\n\n\n而 write.3 所对应的 extent 相关的索引信息说不定就在内存里面，可以很快的找到，从而开始写日志。同时呢，read.2 虽然身为读操作，有可能就没那么好运，需要读多次磁盘，才能找到对应的物理地址。所以我们无法保证读操作肯定是比写操作先完成的，即使读操作比写操作先开始。而且，这里的 read.2 和 write.3 都各自包含了多个操作，任何一个操作都会成为瓶颈。所以在某种极端情况下可能会是这样\n\n\n\n\n\n\n\n在这个捏造的例子里面，read.2.1 拖慢了整个事务的后腿，read.2 是在 write.3 之前开始的，但却在 write.3 之后完成。这对于期望 sequential consistency 客户端显然无法接受。同时，我们还能想象一个更复杂的场景，因为每个读请求都会指定一个区间，告诉 OSD 自己希望读的偏移量和长度。但是这个区间可能会映射到对象的多个 extent，而每个 extent 的读延迟可能会不一样。倘若 read.2.1 指定的区间正好映射到某个 extent，而这个 extent 又正好和 write.3.1 所写的 extent 有重合呢？而且，请注意，例子里面 write.3 先结束，它的事务提交的时候，刷新了 OSD 内存里面所有相关的 extent 对应 block 的 cache。所以 read.2.1 有可能读到的是 write.3 所写的内容。更可怕的是，因为 read.2 读的是多个 extent，返回的 extent 中有的可能是新的，有的则是则是老的。所以这里还有一致性的问题。\n\n\n\n\n\n\n\n简单粗暴的办法就是在 read.2 之后直接加一个 lfence，确保所有的读请求都完成，防止乱序的发送，也避免读到不一致的数据。\n\n\n\n\n\n\n\n对于 erasure coded pool 这个问题更复杂一些。如果对象保存在 erasure coded pool 里面，Ceph 在往里面写数据的时候，会\n\n\n\n\n把数据拆开成 k 等份\n\n\n再根据选择的算法计算出 m 个校验块\n\n\n再把这些数据发往 m + k 个 OSD\n\n\n\n\n倘若写操作的偏移量不是 m x chunk size 对齐的，那么这个写操作就会升级成 rmw (read modifiy write) 操作，因为它需要把自己少的那部分先读出来，解码，然后再和自己的没对齐的部分拼起来再重新拆分编码。\n\n\n\n\n\n\n\n在上图中，在编码的时候产生了 6 块数据，其中 4 块是原始数据，2 块是校验数据。为了修改这个对象，而修改的位置正好落在了 3 里面，我们必须把整个数据都读进来，然后再把写请求的数据嫁接到 3 的对应位置，重新编码。得到被修改过的 3 和全新 4，以及融合了老数据和新数据的 5 和 6。正因为 erasure coded 的写操作事实上包含了\n\n\n\n\n相邻区域的读操作\n\n\n指定区域的写操作\n\n\n\n\n所以它无法和其他的写操作在对象层面上同时进行。除非我们实现了更细粒度的访问隔离控制，确保事务的独立性。当然我们目前没有这么做并不意味着不可能，而是因为这样会比较复杂。因为每个写的事务都会涉及多个 extent。extent 可能会含有多个 stripe。两个写事务之间没有读写依赖的话，那么完全可以一起执行。也就是说，如果事务 A 不会写到事务 B 读取的数据，反之亦然，那么我们就可以认为两者是独立的。然是这需要在往下发送写请求之前，先把这些关系先分析清楚才能决定。这个可能太复杂了。而且得不偿失，以 RBD 为例，允许并发写一个 block 的请求的可能并不大。所以我们还是选择直接加 lfence。\n\n\n在 crimson 里面使用了一个 shared_mutex 的变形 tri_mutex 来解决这个问题。常规的 shared_mutex 是一个读写锁，允许多个读者，或者单个写者。tri_mutex 借用了 mutex 的名字，其实它实现的是自动添加 sfence 和 lfence 的功能。它维护着一个等待者的队列，如果有新的请求进来，tri_mutex 就看看这个请求和当前的请求是不是能一起执行，如果不能的话，就进入队列，等到现在所有正在执行的请求结束之后才能开始；如果可以的话，就直接放行。从前面的讨论，可以知道我们有下面这个规则：\n\n\n\n\n读操作可以和读操作并行\n\n\n写操作可以和写操作并行\n\n\nRMW 不能和任何操作并行\n\n\n\n\n"
} ,
  
  {
    "title"    : "博客维护指南",
    "category" : "",
    "tags"     : " jekyll",
    "url"      : "/2020/09/05/my-blog.html",
    "date"     : "September 5, 2020",
    "excerpt"  : "\n工具链\n\n\n我对 web 开发和 ruby 不熟悉，磕磕碰碰用下面的技术和工具把博客搭起来了：\n\n\n\nAsciiDoc\n\n标准化的 markup 语言。和各家 markdown 不一样，它有标准化的工具链。\n\nAsciiDoctor\n\nAsciiDoc 的参考实现。\n\njekyll-asciidoc\n\njekyll 的 AsciiDoc 插件。通过它就能用 AsciiDoc 写网站了。\n\njekyll-text-theme\n\n一开始用的 minima 主题虽然开箱即用，但是这个主题的红色...",
  "content"  : "\n工具链\n\n\n我对 web 开发和 ruby 不熟悉，磕磕碰碰用下面的技术和工具把博客搭起来了：\n\n\n\nAsciiDoc\n\n标准化的 markup 语言。和各家 markdown 不一样，它有标准化的工具链。\n\nAsciiDoctor\n\nAsciiDoc 的参考实现。\n\njekyll-asciidoc\n\njekyll 的 AsciiDoc 插件。通过它就能用 AsciiDoc 写网站了。\n\njekyll-text-theme\n\n一开始用的 minima 主题虽然开箱即用，但是这个主题的红色主题一下子让人有些亲近感。忍不住还是套用上了。\n\n\n\n\n\n\n一些补丁\n\n\n另外做了一些修改：\n\n\n\n\njekyll-asciidoc 没法支持一些插件。\n至少用这个补丁是修好了。\n测试的时候如果用自己本地的 jekyll-asciidoc 的话，需要参考 bundler 的文档，\n让 bundler 用本地 repo，而不是 RubyGems 上的版本。这也是为什么我用 Gemfile 管理 jekyll\n插件的原因。感觉这样更接近 Ruby 一些，方便理解 bundler 是如何找到插件的。\n\n\nasciidoctor_cjk_breaks 是 asciidoctor 的扩展。因为有时候一句话太长了，就希望分成几行写。但是和英文不一样，中文的字和字之间一般是不加空格的，asciidoctor 看到 linebreak 就当成了空格处理，输出的 HTML 中行尾和行首中间就加了个空格。读者看起来就很变扭，所以这个扩展就把空格去掉了。这个问题在 asciidoctor 也有报告，但是还没得到解决。这个扩展很久以前写的，可能这两年没有更新，所以改了一下，让它能和比较新的 asciidoctor 2.x 一起用。\n\n\njekyll-TeXt-theme 自带的 archive.html 页面如果用 prettier 处理的话，它会报错。用上这个补丁就没问题了。\n\n\n\n\n\n\n一些设置\n\n\n\n\nCNAME:\n\n\n\n配置 GitHub 让我的域名成为 canonical domainname\n\n\n配置我的域名服务商加了 CNAME 指向 GitHub\n\n\n\n\n\nGitHub Actions: 因为贪图用新的的 jekyll。GitHub Pages 提供的版本就有点旧了。所以用他家的 Actions 来做 CI，自动编译页面。用了下面几个 action\n\n\n\nactions/checkout\n\n\nruby/setup-ruby\n\n\nlimjh16/jekyll-action-ts\n\n\npeaceiris/actions-gh-pages\n\n\n\n\n\njekyll-TeXt-theme: 把它的配置抄了一堆。\n\n\nCSS: rouge 的语法高亮在 jekyll-text-theme 中无法生效，因为后者没有定义一些 rouge 要的 CSS 规则。\n所以从 minima 拷贝了一份 _syntax-highlighting.scss 然后按照个人喜好改了一下，放到了 _sass/custom.scss。网上也能找到好些好看的 rouge 语法高亮的主题。因为 rouge 高亮的 HTML 输出和 pygments 是兼容的，所以那些 pygments 的主题也可以拿来用。\n其实之前并不知道应该用这个名字，只是觉得 text-theme 缺了这些定义，应该补上，是后来分析了它的 assets/css/main.scss，看着 custom.scss 的名字，找到它，发现这是个空的文件。才猜测这是给用户自定义的一个 stub。\n\n\n\n\n\n\n写作要注意的事项\n\n\n多余的空格\n\n加链接的时候，如果里面的文本是中文。通常我们不希望在链接前面有空格。所以只能用\n\n\n\n这个行星上最大的link:https://github.com/[交友网站]\n\n\n\n\n结果就像这样\n\n这个行星上最大的交友网站\n\n\n\n\n要是用\n\n\n\n这个行星上最大的 https://github.com/[交友网站]\n\n\n\n\n结果就像这样\n\n这个行星上最大的 交友网站\n\n\n\n\n“交友网站”前面多了个空格。\n\n\n\n嵌入\\(LaTeX\\)公式\n\nAsciiDoc 的文档里写得很清楚。但是为了方便查找，还是自己记一下。\n\n\ninline 的 \\(LaTeX\\) 公式用\n\n\n\ninline 的 latexmath:[$LaTeX$] 公式用\n\n\n\nblock 的可以用\n\n\n\ne = mc^2\n\n\n\n结果是：\n\n\n\n\\[e = mc^2\\]\n\n\n\n记得在 AsciiDoc 文件头里加入\n\n\n\n:page-mathjax: true\n\n\n\n虽然也可以在 _config.yml 里加\n\n\n\nmathjax: true\nmathjax_autoNumber: false\n\n\n\n但是我觉得会减慢页面加载的速度，毕竟 MathJax.js 还有一些支持的字体体积也不小。\n\n\n\n\n\n常规维护\n\n\n另外，如果以后要加个插件或者其他 ruby 包，先修改 Gemfile，然后\n\n\n\nbundle install\n\n\n\n就行了。\n\n\n"
} ,
  
  {
    "title"    : "TCP 长连接的最大个数",
    "category" : "",
    "tags"     : " networking",
    "url"      : "/2020/08/23/tcp-connections.html",
    "date"     : "August 23, 2020",
    "excerpt"  : "\n\n\n提个问题，单机单网卡最大对某个特定服务器 TCP 长连接的最大个数是多少？\n\n\n和平时一样，我们假设客户端是 Linux。这可能不是一个生造出来的问题，想一想，如果我们希望设计一个支持长连接的代理服务器呢？如果有海量的客户端希望连接被代理的服务器呢？或者说我们希望为用户提供实时的消息服务呢？这是一种 c1000k 问题。\n\n\n我们从 TCP 协议开始，慢慢往外推，到操作系统直到硬件。看看一路上都有哪些限制。我们可以假设服务器是个怪物，它在 TCP 的框架下面可以有无穷的计算能力和带宽...",
  "content"  : "\n\n\n提个问题，单机单网卡最大对某个特定服务器 TCP 长连接的最大个数是多少？\n\n\n和平时一样，我们假设客户端是 Linux。这可能不是一个生造出来的问题，想一想，如果我们希望设计一个支持长连接的代理服务器呢？如果有海量的客户端希望连接被代理的服务器呢？或者说我们希望为用户提供实时的消息服务呢？这是一种 c1000k 问题。\n\n\n我们从 TCP 协议开始，慢慢往外推，到操作系统直到硬件。看看一路上都有哪些限制。我们可以假设服务器是个怪物，它在 TCP 的框架下面可以有无穷的计算能力和带宽，各种资源取之不尽用之不竭。那么问题到了 TCP。\n\n\n\n\nTCP\n\n\n数据库问题\n\n有人说，这是个数据库问题。因为 TCP 实现为了区别 TCP 连接，用了个四元组标记每个 TCP 报文\n\n\n\n\nsource ip: 32 bit for IPv4\n\n\nsource port: 16 bit\n\n\ndestination ip: 16 bit (fixed)\n\n\ndestination port: 32 bit for IPv4 (fixed)\n\n\n\n\n所以这四个数字合起来，就是数据库的复合主键。其中，目标服务的 IP 和端口都是固定的，所以我们只能从客户端这边发掘潜力：\n\n\nsource ip\n\n用 iproute2 可以为同一块网卡添加多个 IP 地址。理论上说，这就是 232 个地址。\n\n\n\nip addr add &amp;lt;ip&amp;gt;/&amp;lt;network&amp;gt; dev &amp;lt;interface&amp;gt;\n\n\n\n但是要细究的话，IPv4 有很多特殊的地址段是不能使用或者使用上有限制的。如果服务器是对公网开放的，那么我们作为客户端就不能使用外部地址，只能用那些本地的地址，比如 192.168.x.x 或者 127.x.x.x 这些。如果使用 NAT/PAT 这类技术在内部实现 IP 复用，那么就需要把 NAT 设备的限制考虑进去了。不管怎么样，数量级差不多是这个。\n\n\n\nsource port\n\n对于特定目标地址，本地端口可以选择的区间是由 net.ipv4.ip_local_port_range 决定的。\n\n\n\n$ cat /proc/sys/net/ipv4/ip_local_port_range\n32768\t60999\n\n\n\n对于给定的目标地址，以及给定的本地 IP，可以发出的连接数量就是本地端口区间的大小。所以单个本地 IP 最多可以产生 65535 个 TCP 连接。为了打破这个限制，我们必须为网卡添加多个虚拟 IP。满打满算，这就是 232+16 个链接，约为 281万亿。打个比方，我想开个公司，先从员工的工号的编码方式开始计划！嗯，就用 IPv4 的地址和 16 位的端口号来吧，所以，我的公司最多支持 281万亿个员工。这个思路扩展性很好，很强大！但是每个人都得发工资啊，我陷入了沉思&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;\n\n\n\n\nTCP 的运行时开销\n\n什么是 TCP 连接\n\n我们熟知的三次握手就能建立一个 TCP 连接\n\n\n\n\nSYN\n\n\n等待对方回应 SYN/ACK\n\n\n最后回答 ACK\n\n\n\n\n一旦双方完成这个规定的礼仪，就可以说这个连接建立了。一旦两边接上头，剩下的事情就是运行时的开销。\n\n\n\n系统 TCP 协议栈\n\n如果服务使用系统的 TCP 协议栈，那么每个连接都需要占用一个文件描述符。回忆一下 send() 和 recv()，它们的第一个参数是 socket，而socket 可不就是个 fd 嘛。所以操作系统文件描述符的最大值，这个全局的 设置 是 fs.file-max。在我的 RHEL8 上，它的值是 19603816，接近两千万了。如果我们希望用单进程实现这个服务，还需要改 ulimit -n 的限制。当然，如果内存够大，多操作系统或者用容器化的实现，以及用多进程的实现都可以越过这些限制。代价就是更多的额外开销。\n\n\n另外，还需要关注协议栈用到的缓冲区，看看 net.ipv4.tcp_wmem 和 net.ipv4.tcp_rmem，在 RHEL8 上，它们的缺省大小分别是 85K 和 16K。设置都有三组数字，分别是下限、缺省值和上限。以及 net.ipv4.tcp_mem，它控制着整个系统中所有 TCP 缓冲区的总大小的上限。这个设置表示的是内存页的数量，有三组数字，分别是下限、警戒值和上限。如果 TCP 缓冲空间总使用量达到上限之前，TCP 就会开始减少每个 TCP 连接缓冲区的分配。一旦达到上限，TCP 实现就开始丢包，希望减轻对内存系统的压力。\n\n\n\n$ grep . /proc/sys/net/ipv4/tcp*mem\n/proc/sys/net/ipv4/tcp_mem:2295903\t3061204\t4591806\n/proc/sys/net/ipv4/tcp_rmem:4096\t87380\t6291456\n/proc/sys/net/ipv4/tcp_wmem:4096\t16384\t4194304\n\n\n\n所以缺省设置下，最多使用 17G 内存，可以同时支持二百万以上的长连接。\n\n\nLinux 下新的防火墙是使用 netfilter 实现的，如果开启了防火墙那么还需要关注\n\n\n\n\nnet.ipv4.ip_conntrack_max\n\n\nnet.ipv4.netfilter.ip_conntrack_max\n\n\n\n\nnetfilter 维护着一张哈希表用来跟踪所有的 TCP 连接，所以如果这张表放不下新的 TCP 连接，TCP 就会开始丢包。\n\n\n\n用户态 TCP 协议栈\n\n\n\n带宽\n\n需要估计所有连接中活跃的比例，并且需要了解活跃连接需要的带宽是多少。\n\n\n\n内存\n\n前面如果使用系统的 TCP/IP 栈，就需要为每个连接保证 tcp_rmem.min + tcp_wmem.min 的空间。\n\n\n\n\n"
} ,
  
  {
    "title"    : "多核和顺序",
    "category" : "",
    "tags"     : " arch, x86",
    "url"      : "/2020/08/10/memory-ordering.html",
    "date"     : "August 10, 2020",
    "excerpt"  : "\n\n\n性能和易用性之间常常会有矛盾。\n\n\n晚上到住处，有人可能会先放水，休息一下，简单吃点东西，然后呢，看看水温，差不多了洗个澡。用文艺的说法，这是生活的智慧。用体系结构的话说，这是乱序执行，使用了简单的调度算法。对单身汉来说，打乱计划，用不一样的顺序安排生活可以获得更高的效率。但是对多核程序来说，这其实不一定是好事。Jim Keller 打了个 比方，他说计算机是在顺序的手法说一个故事，书里面有很多段落，段落是由句子构成的。读者可以画一个示意图，看看哪些段落和句子读的时候可以打乱顺序，而...",
  "content"  : "\n\n\n性能和易用性之间常常会有矛盾。\n\n\n晚上到住处，有人可能会先放水，休息一下，简单吃点东西，然后呢，看看水温，差不多了洗个澡。用文艺的说法，这是生活的智慧。用体系结构的话说，这是乱序执行，使用了简单的调度算法。对单身汉来说，打乱计划，用不一样的顺序安排生活可以获得更高的效率。但是对多核程序来说，这其实不一定是好事。Jim Keller 打了个 比方，他说计算机是在顺序的手法说一个故事，书里面有很多段落，段落是由句子构成的。读者可以画一个示意图，看看哪些段落和句子读的时候可以打乱顺序，而不改变表达的意思。比如\n\n\n\n\nHe is tall and smart and &amp;#8230;&amp;#8203;\n\n\n\n\n可以改成\n\n\n\n\nHe is smart and tall and &amp;#8230;&amp;#8203;\n\n\n\n\n但是这个句子如果打乱顺序的话，&quot;red&quot; 修饰的对象可能就不对了\n\n\n\n\nTall man who is wearing a red shirt\n\n\n\n\n句子里面的元素之间有依赖关系。\n\n\n\n\nmutex 的问题\n\n\n在多核环境下，要实现无锁编程或者尽量减少锁的使用，就不能用 mutex。其实，为了最大程度上优化，内核里的 mutex 在加锁的时候为了避免不必要的开销甚至分了 三种情况：\n\n\n\n\n用 lock cmpxchg() 指令，检查 mutex 的 owner 是否为空。\n\n\n如果 mutex 的所有者正在运行，那么用 spin lock 等待它。\n\n\n把希望得到锁的线程阻塞，挂到等待队列。等到锁释放的时候，再调度自己。\n\n\n\n\n这个机制叫做 futex。在 Linux、NetBSD、FreeBSD、Windows 以及 Zicron 中都实现了它。在 FreeBSD 中甚至有明确的 spin/sleep mutex 和 spin mutex 的 概念。而在 Linux 中，futex 则是一个 系统调用。\n\n\n不过，这些原语有两个问题\n\n\n\n\n系统调用是个原罪。\n\n\n\n鉴权。\n\n\n切换栈。即使 x86 使用 syscall 来实现系统调用，因为内核的地址空间和用户态程序不同，进入内核要求修改段寄存器，相应的带来 TLB 的刷新。\n\n\n缓存的刷新。\n\n\n\n\n\nPOSIX.1 要求 mutex 同步内存。\n\n\n\n\n前者大家做了很多实验，说明不管如何系统调用都比 longjmp 的开销都要大很多。而今天我想把后者展开说一下。\n\n\n\n\n内存一致性\n\n\n首先，什么叫同步内存？在多核系统里面，每个核都有自己的一个小天地，为了加快对内存的访问，CPU 核在内存中间有多层的 cache。这个设计有点像现在大家说的朋友圈。根据一些权衡，这个朋友圈中的 cache 根据亲疏远近又分了三六九等，即 L1、L2 和 L3 缓存。通常每个核都有自己的 L1 cache，L1 其中又分数据 cache 和指令 cache。L2 不分这么细。L3 缓存由多个核共享。CPU 的片内总线设计很大程度就是各个核和片内缓存的关系网的拓扑。 和硬盘一样，cache 对内存的映射也是有最小单位的，硬盘的单位是 block，而 cache 的单位叫做 cache line。所以，从硬盘往内存读数据是以页为单位，通常大小是 4K。从内存往缓存读数据以 cache line 为单位，大小一般是 64 字节。另外，还有一种特殊的缓存叫 TLB，它是用来缓存线性地址到物理地址映射的页表。每个进程都有自己的地址空间，所以每个进程的 TLB 表项也各自不同。这里涉及内存的寻址、分配和管理，我们可以另外说。\n\n\n既然是缓存，就会有缓存的一系列问题\n\n\n\n\n替换策略。比如大家耳熟能详的 LRU。再啰嗦一句，替换的最小单位也是 cache line。\n\n\n写策略。\n\n\n\nwrite through 写数据的时候，数据不仅写进 cache，而且也同时刷新内存。\n\n\nwrite back 写数据的时候，数据仅仅写到 cache 里面，把相应的 cache line 标记成 dirty。在真正需要刷内存的时候再把数据&quot;`写回`&quot;去，一旦内存和缓存同步，这个 cache line 又是 clean 得了。因为 write back 性能比较好，缓存通常用它。\n\n\n\n\n\n一致性。既然每个核读写都是通过自己的 cache，而不是直接访问内存，那么怎么保证各个核看到的数据是一致的？\n\n\n\n\nMESI\n\n这里主要就写策略和一致性展开来说一下。为了解决一致性的问题，CPU 的设计者会用 MESI 的某种改进版来保证缓存之间的同步问题。\n\n\n\n\nModified 我的这份数据是被修改过的最新版。这个数据所在的 cache line 被标记成 dirty。这个状态要求其他人的状态是 I。别人想要读这个数据，必须等我把它写回内存。一旦写回去，状态成为 S 了。\n\n\nExclusive 我的这个 cache line 别人都没有缓存，所以如果修改它的话，不会产生不一致。\n\n\nShared 我的版本和别人的版本是一样的，我们的版本都是最新的。不过，我们都是&quot;`读`&quot;者，如果要写的话，得先获得排他锁。\n\n\nInvalid 我的版本比较老了。对于一个缓存来说，一个 cache line 如果是 I 的状态，那就相当于它不存在。要是内核希望读它的话，会得到一个 cache miss。\n\n\n\n\n假设 core#0 想写 0x1347 地址，它写的不仅仅这个地址对应的内存空间，它写的是这个地址映射到的整个 cache line。\n\n\n\n\ncore#0 告诉内存说，请把 0x1347 所在的 cache line 交给我。\n\n\n内存说，好的，这里是 0x1347 所在 cache line 的 64 字节数据。\n\n\ncore#0 告诉其它核，你们的 cache 里要是有这个 cache line，立即把它作废掉。因为它的值就快过时了。\n\n\n其它核听到这个消息纷纷回应\n\n\n\n好的，我把那个 cache line 给作废了。或者干脆清除，或者把它标记成 I。\n\n\n好的。虽然我这里没有那个 cache line。不过你可以放心了。\n\n\n\n\n\ncore#0 找到 cache line 中对应 0x1347 的字节，改成自己想要的值，把那个 cache line 标记成 M。\n\n\ncore#1 想读 0x1347，但是它对应的 cache line 是 I 状态。\n\n\ncore#1 问内存，请把 0x1347 所在的 cache line 交给我。\n\n\ncore#0 不得已，把那个 cache line 写回内存。core#1 立即读到了最新的 cache line，这时他们缓存对应 cache line 的状态都改成了 S。\n\n\n\n\n\n硬件重排序\n\n这带来了另外一个问题，MESI 协议里面有两种操作会比较慢。\n\n\n其中，写一个 cache line 需要好几步。如果 cache line 不在本地缓存，或者是 I 状态。这就是个 cache miss。那么这种情况下，还需要读内存。然后为了获得 cache line 的排他锁，还需要得到其它核的确认。要是 core#1 的缓存在收到 invalidate 消息的时候正在忙其它事情呢？这会 core#0 的写操作。更何况现代处理器的核那么多，core#0 的写操作的瓶颈之一是最慢的一个核对 invalidate 请求的回应。所以 core#0 的 invalidate 最好能立即返回。所以我们在每个核的缓存前面放一个 invalidation queue，让这个操作成为异步的。core#0 只要把消息放在队列里面，就可以继续执行下一条指令。等 core#1 的缓存忙完了手里的事情，就会检查它的队列更新对应 cache line 的状态。CPU 的设计者没有就此止步，因为只是读内存，往每家的队列里面投递消息也很耗时，core#0 的流水线还有余力做其他工作，它不希望因为这个 cache miss 就干等着。最好能并行地多做几件事情。所以我们在本地的核边上也加了一个队列，叫做 store buffer 或者 write buffer。把写操作扔到 store buffer 里面，就可以立即返回。而本地缓存一旦做完那些准备工作，它就会从 store buffer 里面拿到要修改的数据，更新自己的 cache line。反之，要是等待本地缓存和其他各方把所有这些步骤完成再循规蹈矩往下执行下一条指令，就太慢了！\n\n\n\n\n\n\n\n而读一个 cache line 也不容易。类似的，要是 cache miss 的话，那么当前核就会要求另外一个核把它的数据先刷到内存。这将引起一个内存事务。\n\n\n但是这样引入了一个问题--------内存读写操作的乱序执行。这不仅让单核的顺序执行成为一个有前提的表象，更让多核的环境下的内存一致性和顺序执行更加错综复杂。对于特定的内核来说，可能会在一个写操作完成之前，就开始执行下一条指令。而对于其他内核来说，读指令可能会得到一个事实上过时 (invalid) 的数据。因为即使是写操作的发出者也还没有真正完成这个写操作，它只是把这个操作提交给了 store buffer。不过和其他内核相比，它是可以读到最新的数据的，在它执行读指令的时候，可以先检查 store buffer，如果 store buffer 里面没有对应的数据，再检查缓存。这个叫做 store buffer forwarding。因为它在当前核通过 buffer 把数据&quot;转交&quot;给将来要执行的读指令。这个设计保证了数据依赖和控制依赖，也就是单核上下一个操作的结果如果依赖上个操作的副作用，那么下个操作必须能看到上个操作的副作用。换句话说，如果从单核的角度出发，看不出这种&quot;`依赖`&quot;问题，那么 CPU 就认为它可以把读写操作重新排列，以此获得更高的并发度。另外，store buffer 的存在也催生了另外一些优化，如果有两个写操作修改的是连续的内存地址，在刷内存的时候，这两个写操作就可以合并成一个大的写操作，从而减轻内存总线的负担。这个技术叫做 write combining。 write combining buffer 就是处在 store buffer 和系统总线中间的地方。如果有往同一地址的写操作，那么时间顺序上后面操作就会覆盖前面的操作，这个技术叫 write collapsing。\n\n\n这种读写指令的乱序执行破坏了严格意义上的顺序一致性。对很多人来说，如果你要的是咖啡加奶，那么做法应该是先加咖啡再加奶，但是对一个追求效率的人来说，可能就会应该先做咖啡，在咖啡机哼哧的时候，把奶加进去，等咖啡机好了，再把咖啡倒进去。不过要是有原教旨主义者看到这个顺序可能会很不高兴，他说顺序和比例一样重要！简单说，顺序对自己可能不那么重要，但是旁人可能会很在意。\n\n\n但是甚至在不对齐写的情况下也会造成不一致的结果。说到 store buffer forwarding，之前 Linus 举了一个 例子。\n\n\n假设有个系统有三个核，开始的时候 dword [mem] 的内容是 0。执行下面的程序\n\n\n\nxor %eax, %eax\ncmpxchl $0x01010101, (mem)\n\n\n\n\nmovl $0x01010101, %eax\ncmpxchl $0x02020202, (mem)\n\n\n\n\nmovb $0x03, (mem),\nmovl (mem), reg\n\n\n\n程序结束的时候，dword [mem] 可能是 0x02020203 ，但是有趣的是，这时第三个核上 reg 里面则会是 0x01010103。因为 MESI 协议保证了 cache coherency，dword [mem] 的值先后是 0 -&amp;gt; 0x01010101 -&amp;gt; 0x02020202 -&amp;gt; 0x02020203。因为最后一次第三个核的 mov 也获得了排他锁，然后把整个 cache line 刷到了内存里面。但是第三个核的寄存器为什么读到了一个奇怪的值。这个值甚至在 cache line 里面没有缓存过。原因是第三个核会这样解释：\n\n\n\nmovb $0x03, store_buffer[mem] ; 把 [mem] &amp;lt;- 0x03 的操作放到，store buffer，写操作比较慢。先继续执行读操作\nmovl (mem), reg              ; 把 [mem] 的内容读出来\nmovb store_buffer[mem], reg  ; 读操作也会查看一下 store_buffer，看看手里面最新的数据\n\n\n\n所以第三个核寄存器中看到是一个脏数据。这个数据从来没有在内存中出现过。它有两个来源：高 24 位是第一个核写进去的，低 8 位是自己写的。而按照 cmpxch 的原子操作的语义，这个过程中是不可能有这样的不一致出现的。这也是为什么 amd64 不能保证非对齐写操作的原子性的原因。\n\n\n话说回来，不仅仅是写数据上的核可能看到脏数据，也因为 store buffer 的存在，使得各个核看到的内存并不一样 (coherent)。如果某个核的对某个 cache line 的修改存在 store buffer 里面，那么这个 cache line 在其它核眼中则是旧的数据。另外，就算本地缓存检查了 store buffer，发送了 invalidate 消息给其他核。但是在其它核在检查 invalidation queue 之前，仍然会认为那个 cache line 是有效的。有人可能会说，其他内核可以在读缓存之前看看 invalidation queue 啊，可能是因为 invalidation queue 只是个 queue，内核在读缓存之前不会去检查 invalidation queue。所以如果多个内核共享一块内存，那么某个核上读写顺序重新排列会导致程序有不同的执行的结果。有的时候我们不在乎，但是有的时候这种不一致的结果是致命的。再举个例子，在餐馆吃饭。有的餐馆在顾客点菜之后会给一个电子闹钟，等闹钟响了，就可以去自助取餐。以此为背景，我们想象有两个核分别代表等餐的顾客老王 (wong) 和面馆老马 (mars)：\n\n\n\nbool placed_order = false;\nbool beep = false;\nchar meal[128];\n\nvoid wong() {\n  placed_order = true;\n  while (!beep);\n  claim(meal)\n}\n\nvoid mars() {\n  while (!placed_order);\n  cook(meal);\n  beep = true;\n}\n\n\n\n要是平时写这个程序，大家可能会很自然地用 atomic&amp;lt;bool&amp;gt; 来定义 placed_order 和 beep。但是既然 amd64 保证了 单字节数据访问的原子性\n\n\n\n\nCacheable, naturally-aligned single loads or stores of up to a quadword are atomic on any processor model, as are misaligned loads or stores of less than a quadword that are contained entirely within a naturally-aligned quadword.\n\n\n\n\n所以 placed_order 的读写都是原子的。那么我们为什么还要用 atomic&amp;lt;bool&amp;gt; 呢？所以上面的代码就直接用 bool 了。接下来，我们在老王和兰州拉面的互动中加入 store buffer，看看会发生什么：\n\n\n\n\n老王来到面馆，大碗牛肉面！于是更新 placed_order。但是 placed_order 是在内存里面，写内存太慢了。先更新自己桌上的的 store buffer 吧。等会儿结账的时候再一起更新 placed_order 好了。\n\n\n老王看着桌上的闹钟，焦急地等待。beep 啊，你怎么还是 false 呢？都十秒钟过去了。\n\n\n面馆的马老板看着老王，这个人没有下单，眼神呆滞，从一坐下来就盯着桌上的闹钟不动。怕是昨晚加班到三点，还没缓过劲？\n\n\n又过了十秒钟&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;两个人都隐约觉得有点不对，但是不知道出了什么问题。\n\n\n\n\n对老王和老马来说，这都是个僵局。而这个僵局是 store-load 重排造成的。所以即使从单核的角度看，数据依赖和控制依赖是能够保证的，多核环境下也无法确保程序的&quot;`顺序`&quot;执行。换言之，cache conherence 不等于 sequential consistency。后者的语义需要引入更强约束。但是因为后者的约束太强了，我们在实际工作中往往会采用一些折中。\n\n\n另外，如果文献中提到 load buffer 或者 load queue，它是用来保存读请求的。比如说，如果处理器预测某个写请求之后会读取地址 X，它会把这个请求放到 load buffer 里面。一个读请求的地址计算出来之后，这个请求也会保存在 load cache 里面。对于那个写请求，它在写内存之前则会检查 load buffer，如果发现命中的话，就会让读取 X 的请求返回写请求要写入的值。load buffer 可以让内存读取批次化，使得 cache miss 的处理更有效率。\n\n\n\n\n\n一致性模型\n\n\n不同体系结构在 consistency 这个问题上有着不同的答案，这些答案就是不同的一致性模型：\n\n\n\n\nsequential consistency: 顺序一致，简称 SC。这是最死板的一致性模型。即使看上去没有危险，每个核也会以完全忠实原著的方式执行，除了缓存，不加入任何可能产生乱序的设计。所以 store buffer 和 invalidation queue 这种东西是禁止的。这种简单粗暴的限制对 CPU 的自尊心和性能是一种强烈的伤害。\n\n\nweak consistency: 弱一致。在一定程度上允许重排序，受到 memory barrier 的约束。\n\n\nrelaxed consistency: 处理器完全可以乱来乱序。\n\n\n\n\n大家对性能都有自己的坚持，没有一个有追求的处理器是顺序一致的。或者说，做到高性能的严格的顺序一致会非常困难。不过 amd64 是最接近的。它只会把代码里面的 store-load 顺序打乱，变成 load-store。像刚才老王吃面的例子里面，本来老王先点面，再看闹钟，被处理器一乱序，优化成了先看闹钟，再点面。完全乱了套。\n\n\n除此之外，还有下面几种排列。对于它们，amd64 就完全按照脚本执行了。\n\n\n\n\nstore / store\n\n\nload / store\n\n\nload / load\n\n\n\n\n在各种架构里面，amd64 是比较保守的。其他架构就比较放飞自我，比如对于 aarch64 中的 ARMv8-A 架构， 它的文档提到\n\n\n\n\nThe ARMv8 architecture employs a weakly-ordered model of memory. In general terms, this means that the order of memory accesses is not required to be the same as the program order for load and store operations. The processor is able to re-order memory read operations with respect to each other.\n\n\n\n\n而 Alpha 处理器则是另外一个极端。有这么一个 例子\n\n\n\nint x = 1;\nint y = 0;\nint* p = &amp;amp;x;\n\nvoid p1() {\n  y = 1;\n  mb();\n  p = &amp;amp;y;\n}\n\nvoid p2() {\n  int i = *p;\n}\n\n\n\n在处理器两个核分头运行完这个程序，i 竟然可能是 0！ 可以这样解释\n\n\n\n\np2 开始前就缓存了 y，它知道 y 的地址保存的值是 0\n\n\np1 执行 y = 1 ，发了一个 invalidate 消息给 p2，然后立即返回了。\n\n\np2 收到了 y 的 invalidate 消息，但是它并不急着处理，人家前面又没有 mb() 催着，于是这个消息在 invalidation queue 里躺着。\n\n\np1 这边因为 invalidate 消息立即返回，满足了 mb() 的要求，所以程序得以继续往下执行 p = &amp;amp;y。\n\n\np2 为了得到 *p 的值，先读取 p。读 p 并不要求刷 invalidation queue，所以它得到了 y 的地址。\n\n\np2 根据这个地址，索引到了自己的缓存。缓存里面有，为什么不用呢？\n\n\np2 把原来缓存的 y 的值 0 赋给了 i。\n\n\n\n\n这里，Alpha 没有根据数据依赖来刷 invalidation queue，因为为了得到 *p 读了两次内存。分别是\n\n\n\n\nmov p, %reg\n\n\nmov (%reg), reg\n\n\n\n\n这里有一个数据依赖的关系，因为第二次的输入是第一次的输出。本来很明显，最后 reg 的值至少应该是一致的。也就是说，不会出现历史上 *p 从来没有过的值。就像这个夏天你一直喝啤酒，从没喝过汽水。但是年前和一个朋友吃饭的时候，他说你们七月份在日本玩儿的时候，一起还喝过可乐。这一定是个错觉。你会觉得他记错了，把你记成另外一个人了。并不是说你从没喝过汽水，你小时候还挺喜欢喝。而是你和这个朋友才认识一年，你这一年的确没喝过汽水啊。\n\n\n不过这些选择并没有高下之分。如果只允许重排一两种读写序列，好处是程序员可以按照直觉编写多核程序，而不用太关心读写重排的问题。问题在于处理器的设计会有一些限制。要是需要同时有高并发，和严格顺序，那么处理器就必须把这些读写序列组织成一个个内存事务，如果处理器发觉因为乱序执行破坏了事务，那么就必须把乱序执行的操作取消掉。这使得高性能的并行处理器的设计变得更复杂了。如果处理器遵循的内存模型允许处理器做很多类型的重排序，那么处理器的设计会有很高的自由度，能无所顾虑地应用一些提高并发性的技术，来提高访问内存的效率，比如\n\n\n\n\nout-of-order issue\n\n\nspeculative read\n\n\nwrite-combining\n\n\nwrite-collasping\n\n\n\n\n如果处理器不需要保证访存的顺序，在相同性能指标下，功耗也低一些。在保证数据依赖和控制依赖的前提下，处理器有最大的自由度重新排序读写指令的顺序。但是对程序员的要求就更高了。他们需要再需要顺序的地方安插一些指令，手动加入 memory barrier，让处理器在那些地方收敛一下。这些 memory barrier 要求当前的内核把自己的 invalidation queue 里所有的 invalidate 消息都处理完毕，再处理读写请求。而程序员也可以帮助处理器做一些猜测，比如说 prefetch 和 clflush 具体影响处理器的 cache 行为。\n\n\nmemory barrier 和 lock\n\nlfence, sfence, mfence 是 SSE1/SSE2 指令集提供的指令：\n\n\nAMD64 Architecture Programmer&amp;#8217;s Manual 卷 2，7.13 ：\n\n\n\n\nThe LFENCE, SFENCE, and MFENCE instructions are provided as dedicated read, write, and read/write barrier instructions (respectively). Serializing instructions, I/O instructions, and locked instructions (including the implicitly locked XCHG instruction) can also be used as read/write barriers.\n\n\n\n\n\nlfence\n\nLoad Fence: 即 read barrier。以 lfence 调用的地方为界，定义了读操作的偏序集合。保证系统在执行到它的时候，把之前的所有 load 指令全部完成，同时，在其之后的所有 load 指令必须在其之后完成，不能调度到它的前面。换句话说，它要求刷 invalidation queue，这样当前核所有的 invalidate 的 cache line 都会被标记成 I，因此，接下来对它们的读操作就会 cache miss，从而乖乖地从内存读取最新数据。\n\nsfence\n\nStore Fence: 即 write barrier。以 mfence 调用的地方为界，定义了写操作的偏序集合。保证系统在执行到它的时候，把之前的所有 store 指令全部完成，同时，在其之后的所有 store 指令必须在其之后完成，不能调度到它的前面。它要求刷 store buffer，这样当前核所有积攒的写操作都会发送到缓存，缓存刷新的时候会发送 invalidate 消息到其他核的缓存。sfence 是 SSE1 提供的指令。\n\nmfence\n\nmemory Fence: 即 read/write barrier。以 mfence 调用为界，定义了读和写操作的偏序集合。确保系统在执行到它的时候，把之前的所有 store 和 load 指令悉数完成，同事，在其之后的所有 store 和 load 指令必须在其之后完成，不能调度到它的签名。也就是说，它会清空 store buffer 和 invalidation queue。\n\nlock\n\nlock 前缀：它本身不是指令。但是我们用它来修饰一些 read-modify-write 指令，确保它们是原子的。带有 lock 前缀的指令的效果和 mfence 相同。另外，文档告诉我们，xchg 缺省带有 lock 属性，所以也可以作为 read/write barrier。所以在 内核里面有时会看到类似 lock; addl $0, 0(%%esp) 的代码，这里就是在加 memory barrier，同时检查 0(%%esp) 是否为零。\n\n\n\n\n其实 x86 还有一些指令也有 memory barrier 的作用，但是它们本身有很强的副作用，比如 IRET 会改变处理器的控制流，所以一般来说，要控制内存访问的顺序还是用专门的 memory barrier 和 lock 指令比较容易驾驭。\n\n\n所以有了 lfence 我们可以这么改\n\n\n\nvoid p2() {\n  int* local_p = p;\n  lfence();\n  int i = *local_p;\n}\n\n\n\n禁止处理器重排这两个 load 指令。\n\n\n\nC&amp;#43;&amp;#43; 的一致性模型\n\nC&amp;#43;&amp;#43; 程序员一般不会直接使用这些 memory barrier，它们太接近硬件，可移植性也很差。比如说 aarch64 上的 memory barrier 就叫别的 名字，功能也有些许的不同。所以 C&amp;#43;&amp;#43;11 以及之后的标准规定了几种内存一致性模型，用更抽象的工具来解决这些问题。\n\n\n在解释这些一致性模型之前，我们先回到刚才的面馆。假设老王顺利地下了单，老马也看到了老王的 placed_order ，开始做面条。但是问题来了，处理器不知道 beep 和 noodle 是有先后关系的，所以负责老马的那个核就自作主张，先刷新了 beep，而把 noodle 的写操作放在 store buffer 里面了。这是一种 store-store 重排，在 amd64 上不会发生，但是在其它架构是有可能的。\n\n\n\nbool placed_order = false;\nbool beep = false;\nuint64_t noodle;\n\nvoid wong() {\n  placed_order = true;\n  while (!beep);\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  noodle = cook();\n  beep = true;\n}\n\n\n\n这里有两种数据\n\n\n\n\n被保护的数据 noodle\n\n\n用来表示 noodle 状态的标志&amp;#8201;&amp;#8212;&amp;#8201;beep\n\n\n\n\n这有点像使用 mutex 的情况。mutex 一般用来保护共享的数据，它自己则是有明确的状态的，即 mutex 当前的所有者。在这里也是如此，\n\n\n\n老王\n\n通过读取 beep 的状态，获取锁，一旦 beep 告诉他，&amp;#8220;可以通过&amp;#8221;，那么他就可以放心访问被保护的 noodle。这个过程叫做 acquire。\n老马 开始的时候，老马其实已经是锁的所有者了。正是因为这样，他才得以放心地煮面，修改 noodle 。一旦完成了修改，他就可以通过修改 beep 的值来放弃锁。告诉别人，你们看到 beep 没有，它现在是响着的，可以来访问这个 noodle 了！这个过程叫做 release。\n\n\n\n\n所以，为了避免 store-store 重排，我们用 release-acquire 语义改进了实现：\n\n\n\nbool placed_order = false;\natomic&amp;lt;bool&amp;gt; beep = false;\nuint64_t noodle;\n\nvoid wong() {\n  placed_order = true;\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n这里除了避免 store-store 重排，其实还确保 load-load 的顺序：\n\n\n\n\nbeep.store(true, std::memory_order_release) 确保 noodle = cook() 产生的读写操作不会被放到 beep.store() 后面去。你想想，beep 一响，就像泼出去的水，如果这时候告诉顾客，我还在擀面，那不是很让人恼火？所以我们一定要保证 beep.store() 之前事情不会拖到后面去。\n\n\nbeep.load(std::memory_order_acquire) 确保 consume(noodle) 产生的读写操作不会放到 beep.load() 之前。否则就会出现老王在 beep 响之前，就直接去拿面的情况。让正在擀面的老马措手不及。\n\n\n\n\n我们再回到 load-store 的问题。这个问题其实很难用 release-acquire 的模型描述，因为 placed_order 不是用来保护一个共享的数据的，或者说它本身就是一个共享的标记。在老王下单之前，他没有加老马家拉面的微信号，也没有填写老马搞的调查问卷。不过这个问题可以这么思考，placed_order 应该在老马看到它之后重新设置成 false，这样老马再次看到它的时候就不会以为老王又要了一碗面了。老王这边其实也有类似的问题，和他一起去吃面的老李也会改 placed_order，要是两个人都把 placed_order 改成了 true，那么老马做的下一碗面到底归谁呢？所以程序应该这么改：\n\n\n\nbool placed_order = false;\natomic&amp;lt;bool&amp;gt; beep{false};\nuint64_t noodle;\n\nvoid wong() {\n  while (placed_order);\n  placed_order = true;\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  while (!placed_order);\n  placed_order = false;\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n这样还是有问题，因为老李搞不好会中途插一脚\n\n\n\n\n老王看到没人下单了，正准备把 placed_order 改成 true。他还没开始 placed_order = true 就开小差了，看着门外突如其来的暴雨，又陷入了沉思。\n\n\n老李也注意到了，他立即把 placed_order 改成了 true。开始看着 beep 焦急地等待自己的大碗牛肉面。\n\n\n老王回过神来，也把 placed_order 改成了 true。\n\n\n两个人一起焦急地等待那碗面。\n\n\n\n\n所以我们应该用原子操作来修改 placed_order，让 read-modify-write 一气呵成，用 compare-and-exchange 正合适：\n\n\n\natomic&amp;lt;bool&amp;gt; placed_order{false};\natomic&amp;lt;bool&amp;gt; beep{false};\nuint64_t noodle;\n\nvoid wong() {\n  bool expected = false;\n  while (!placed_order.compare_exchange_weak(expected, true,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n  while (!beep.load(std::memory_order_acquire));\n  consume(noodle);\n}\n\nvoid mars() {\n  bool expected = true;\n  while (!placed_order.compare_exchange_weak(expected, false,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n  noodle = cook();\n  beep.store(true, std::memory_order_release);\n}\n\n\n\n在 amd64 上\n\n\n\n  bool expected = false;\n  while (!placed_order.compare_exchange_weak(expected, true,\n                                             std::memory_order_relaxed,\n                                             std::memory_order_relaxed));\n\n\n\n会被 GCC 翻译成\n\n\n\n  movb   $0x0, -0x1(%rsp) ; expected = false\n  mov    $0x1, %edx       ; desired = true\nretry:\n  movzbl -0x1(%rsp), %eax ; expected =&amp;gt; %al\n  lock   cmpxchg %dl, 0x2ee2(%rip)\n  je     while_beep_load\n  mov    %al, -0x1(%rsp)  ; %al =&amp;gt; expected\n  jmp    retry\n\n\n\n因为我们只需要原子操作， 所以这里只用了 std::memory_order_relaxed，它对内存的访问顺序没有限制。但是前面提到，xchg 缺省带有 lock 属性，而 lock 前缀的效果和 mfence 相同。所以用不着专门加入 mfence，我们也能要求处理器顺序访问内存了。否则的话，我们需要这么写\n\n\n\n  placed_order = true;\n  std::atomic_thread_fence(std::memory_order_seq_cst);\n  while (!beep);\n\n\n\n这样 GCC 会产生\n\n\n\n  movb   $0x1,0x2ef2(%rip)\n  lock   orq $0x0,(%rsp)\n  nopl   0x0(%rax)\nretry:\n  movzbl 0x2ed9(%rip),%eax\n  test   %al,%al\n  je     retry\n\n\n\nclang 则会用 mfence 代替 lock orq 指令。效果是一样的。根据查到的文献，两者的性能不分伯仲。\n\n\nload 和 store 一般成对使用：\n\n\n\n\n\n\n\n\nload\nstore\n\n\n\n\nmemory_order_seq_cst\nmemory_order_seq_cst\n\n\nmemory_order_acquire\nmemory_order_release\n\n\nmemory_order_consume\nmemory_order_release\n\n\nmemory_order_relaxed\nmemory_order_relaxed\n\n\n\n\n在 x86 下：\n\n\n\n\n\n\n\n\nC&amp;#43;&amp;#43;\n汇编\n\n\n\n\nload(relaxed)\nmov (mem), reg\n\n\nload(consume)\nmov (mem), reg\n\n\nload(acquire)\nmov (mem), reg\n\n\nload(seq_cst)\nmov (mem), reg\n\n\nstore(relaxed)\nmov reg, (mem)\n\n\nstore(release)\nmov reg, (mem)\n\n\nstore(seq_cst)\nxchg reg, (mem)\n\n\nstore(relaxed)\nmov reg (mem)\n\n\nstore(relaxed)\nmov reg (mem)\n\n\n\n\n其中，load(seq_cst) 和 store(seq_cst) 也可以这么实现\n\n\n\n\n\n\n\n\nC&amp;#43;&amp;#43;\n汇编\n\n\n\n\nload(seq_cst)\nxchg (mem), reg\n\n\nstore(seq_cst)\nmov reg, (mem)\n\n\n\n\n刚才我们用了\n\n\n\n\nmemory_order_relaxed / memory_order_relaxed\n\n\nmemory_order_acquire / memory_order_release\n\n\n\n\n帮老王和老马摆脱了困境。如果要解决 Alpha 处理器的问题的话，可以\n\n\n\nint x = 1;\nint y = 0;\natomic&amp;lt;int*&amp;gt; p = &amp;amp;x;\n\nvoid p1() {\n  y = 1;\n  p.store(&amp;amp;y, memory_order_release);\n}\n\nvoid p2() {\n  int i = *p.load(std::memory_order_consume);\n}\n\n\n\n好在只有 Alpha 处理器这么粗犷，敢于无视数据依赖，用刚从内存里面读出来的数据作为地址，来索引缓存里面的老数据，得到指针指向的数值。其他体系架构都会重视数据依赖问题，在这个数据依赖链条上顺序执行。因为 amd64 不会重新排列 load-load，所以它天生对这个问题免疫。另外，C&amp;#43;&amp;#43;17 说\n\n\n\n\nmemory_order_consume: a load operation performs a consume operation on the affected memory location. [ Note: Prefer memory_order_acquire, which provides stronger guarantees than memory order_consume. Implementations have found it infeasible to provide performance better than that of memory_order_acquire. Specification revisions are under consideration.\n\n\n\n\n看起来要实现依赖链条的分析很麻烦，几家 C&amp;#43;&amp;#43; 编译器都懒得矫情，干脆杀鸡用牛刀，所以性能没提高。标准也向现实低头，那么我们如果一定要把依赖关系写得明明白白，做好跨平台的工作，还是用 memory_order_acquire 吧。图个省事儿，图个放心。等到真的有要求再手写汇编。这背离了设计 memory_order_consume 的 初衷，但是也是现阶段比较实际的办法。\n\n\n前面为了让程序更好懂，这些 memory barrier 都和一个 atomic&amp;lt;&amp;gt; 变量放在了一起了。毕竟活生生的变量对于描述程序的逻辑才是有意义的。memory barrier 只是用来保证执行的顺序而已。它就像 xchg 的前缀一样。但是我们也可以直接加入 memory barrier：atomic_thread_fence。前面如果不能用 CAS 的话，我们可能就只能用这一招了。\n\n\n\nstd::atomic_thread_fence(order)\n\n\n\n这样直接的 memory barrier，能和 atomic&amp;lt;&amp;gt; ，也能和其他 atomic_thread_fence&amp;lt;&amp;gt; 一起使用。效果是相同的。\n\n\n\n\n\n后记\n\n\nCeph 是个分布式的存储系统。它里面客户端访问的数据被叫做 object。我们用 PG 来对 object 分组，把属于同一个 PG 的 object 安排在集群里的一组磁盘上。每个磁盘上都有一个服务，叫 OSD，来管理这个磁盘，同时与集群还有客户端联系。所以客户端在访问自己读写的数据时，就会直接和负责存储的服务用 TCP 进行通讯。\n\n\n一个客户端对 OSD 的读写是原子的，这个是底线。那么我们是不是也可以乱序执行客户端发过来的读写指令呢？如果客户端发过来三个消息\n\n\n\n\nwrite(obj1, data), write_xattr(obj1, xattr), read(obj1), read(obj1)\n\n\nread(obj2)\n\n\nwrite(obj3), write_omap(obj3, omap)\n\n\n\n\n这里还有一些背景，客户端和 OSD 之间是通过 Ceph 自定义的 RADOS 协议联系的。RADOS 协议中一来一回的叫做 message，而用来访问 object 的 message 叫做 MOSDOp。它可以包含一系列的读写访问，但是同一个 MOSDOp 中操作的对象只能是同一个。这很大程度上限制了一个 message 里请求的可能性。因为在执行绝大多数访问 object 的操作之前，OSD 都需要读取这个 object 的 OI，即 object info，获取它的一些元数据，比如这个 object 的大小，版本， 快照信息。有的时候因为操作的 offset 越界，这类操作就被作为无效请求，给客户端返回个错误，或者干脆忽略这个无效的请求。但是不管如何，写请求一般来说仍然是比读请求慢的，对于多副本的数据池，我们要求这些副本是一致的。这里的一致性问题和内存一致性类似，其实也可以展开说我们留着以后聊。对于 erasure coded 的数据池，我们也要求 k+m 都落地了才能返回，这些都很花时间。\n\n\n之前在 crimson 例会上，曾经和同事讨论过 Ceph 是不是能乱序访问，Sam 说 RBD 的访问模式基本不可能有这种情况。因为 librbd 客户端的一个 message 里不会同时出现对同一个 object 的读操作和写操作。是的。块设备的访问模式和内存是完全不一样的。就算使用 RBD 的操作系统或者应用程序把一块内存 mmap 到这个设备，也会把读写尽量 cache 在缓存或者内存里面，除非不得已，比如说上层一定要 fsync。但是即使这样，fsync 所对应的 MOSDOp 也不会包含对所涉及的 object 的读操作。\n\n\n那么我们换个问题，有没有可能，或者说应不应该把对 obj1、obj2 和 obj3 的访问乱序执行呢？先假设这几个 object 都同属于一个 PG，毕竟两个连续的操作的 object 同属于一个 PG 的可能性很小。就像是学校里面上公共选修课，随机点名的时候，你和室友都被抽中一样。\n\n\n我们从有没有可能开始吧。librados 提供了两种操作，一种是同步的，另一种是异步的。同步的操作执行完毕才能返回，也就是说如果这个函数返回了，那么就可以认为集群已经把请求里面的写操作作为一个事务写到磁盘上了。异步的函数调用直接返回，不等待执行的操作落地。另外调用方需要给异步调用一个回调函数，这样 librados 就知道这个操作完成的时候该怎么处理了。前者相当于天然的 sequential consistency。后者就给 OSD 以可乘之机，在保证操作原子性的前提下，有一定的自由度可以调度队列里面的操作。\n\n\n在这个框架下，还有个很强的限制。Ceph 有个测试叫做 ceph_test_rados，它根据配置向集群发送一系列异步的读写操作，每组操作都有个单调递增的编号。异步操作完成的时候，根据这个操作的编号我们能知道它的返回是不是顺序的。如果不是顺序的，这个测试会失败。换句话说，这个测试要求 sequential consistency。如果这个测试是合理的，或者即使不合理也是无法变动的，比如说更高层的客户端，比如说 qemu 的 RBD 插件把这个作为一个协定，并且依赖这个行为，那么我们就必须比 amd64 更自律才行。当然，我们也可以异步返回，然后再在 librados 这一层再让新的调用阻塞在老的调用上，使得它们的返回看上去像是顺序的。\n\n\n为了满足 sequential consistency 的要求，我们有两个选择。\n\n\n\n\n执行上的 sequential consistency。这很明显是最稳妥的办法。把罪恶扼杀在摇篮之中，通讯层甚至可以等 MOSDOp 完成之后再读取下一个 message，但是这和咸鱼完全同步有什么区别呢？客户端之所以选择异步操作就是希望更高的并发啊。当然，即便如此，异步的操作仍然是有意义的。异步往 OSD 发射指令，而在 OSD 上顺序执行可以避免网络上的延迟。\n\n\n表现上的 sequential consistency。这个说法并不严谨，其实表现上的顺序一致也需要执行层面的支持。我们权且把第一个的选择作为最直接了当的、完全阻塞的实现吧。这个选择的执行需要更小心一些。因为执行层面上可能的乱序，我们可能需要考虑下面几种读写序列的乱序\n\n\n\nstore-store\n\n\n\n写同一 object。倘若 object 没有支持快照，只要最终的结果和顺序执行的结果一样，即可以认为这两次操作是顺序的。况且要是能在 object store 层面上实现 write collapsing 或者 write combining，岂不是一桩美事？\n\n\n写不同 object。回到老问题，这两个写操作是不是带有 acquire-release 语义？会不会有老马和老王的问题？\n\n\n\n\n\nstore-load\n\n\n\n读写同一 object。和前文中讨论的 CPU 的读写指令不同，RADOS 里面 store 和 load 指令的操作数都是立即数。所以不存在读写数据本身的数据依赖的问题。但是如果读的 extent 和之前写的 extent 有重叠，那么我们就必须小心了，至少需要先把写指令下发到 object store，然后由 object store 把 cache 修改了，并标记成 dirty 才能算是这个操作提交完成。这样等到执行 load 指令的时候才能读到最新的数据。\n\n\n\n\n\nload-store\n\n\nload-load\n\n\n\n\n\n\n\n十年前，用 mutex 和 condition_variable 就能解决很多多线程的问题。在今天，这些同步原语仍然很重要。但是如果我们对高并发有更高的追求，就需要更深入了解多核系统中的无锁编程，在体系结构上多理解一些 CPU 和内存的交互，这样对工作会更有帮助。\n\n\n"
} ,
  
  {
    "title"    : "longjmp 和 setcontext",
    "category" : "",
    "tags"     : " arch, x86",
    "url"      : "/2020/08/09/setjmp-setcontext.html",
    "date"     : "August 9, 2020",
    "excerpt"  : "\n\n\nlongjmp() 和 setcontext() 的性能孰优孰劣？\n\n\n这篇文章起源于 seastar-devel 上的一个 讨论。在开始之前，我们先说一下协程的背景。因为讨论涉及特定的操作系统、处理器系统架构以及调用约定，如果没有特殊说明的话，下面都以 sysv, amd64 和现代的 Linux 为例。\n\n\n\n\n协程的由来\n\n\ncoroutine 或者 cooperative threads，中文常常叫协程。在 Linux 里面，常规的调度单位是 LWP (light weigh...",
  "content"  : "\n\n\nlongjmp() 和 setcontext() 的性能孰优孰劣？\n\n\n这篇文章起源于 seastar-devel 上的一个 讨论。在开始之前，我们先说一下协程的背景。因为讨论涉及特定的操作系统、处理器系统架构以及调用约定，如果没有特殊说明的话，下面都以 sysv, amd64 和现代的 Linux 为例。\n\n\n\n\n协程的由来\n\n\ncoroutine 或者 cooperative threads，中文常常叫协程。在 Linux 里面，常规的调度单位是 LWP (light weight process)。 NPTL 实现下，LWP 和用户线程在数量上是一对一的对应关系。所以，以 Linux 为例，有这么几个问题:\n\n\n\n\n缺省 8MB 的栈空间。虽然 8M 只是虚拟地址的空间，但是内核里面在分配栈空间的时候必须立即分配对应的页表，这个开销是无法避免的。\n\n\n线程调度的时候必须借助内核。换言之，上下文切换也会引起一些开销。\n\n\n因为内核调度线程的不可预期性，比如一个线程把自己的时间片用完了。内核可能会把它调度出去，把另一个就绪的任务换进来。为了保证数据和逻辑的一致性，在一些可能产生 racing 的地方，必须加锁。而锁的引入进一步影响了性能和并发的粒度。\n\n\n\n\n所以为了避免这些问题，我们引入了协程的概念，在用户态实现 m:n 的映射。让线程自己调度自己。正是因为这种用户态线程是互相协作的，只有当一个线程主动把 CPU 让出来，另一个已经就绪的线程才能继续运行。这也是为什么协程叫做&quot;`协程`&quot;的原因。\n\n\n\n\n协程的基本要素\n\n\n协程要能自己调度自己，需要满足下面几个要求\n\n\n\n\n协程在让出 CPU 的时候，需要保存现场。这样当它以后继续执行的时候，能记得起来之前在做什么，然后继续当时未完成的任务。\n\n\n协程在让出 CPU 的时候，能找到另外一个就绪的协程，恢复它当初保存的现场。帮助它回忆起来之前的事情。\n\n\n\n\n这有点像晚上睡前看完书的时候，大家会在书里面夹一个书签，记住看到哪一页了。下次再翻开书的时候，找到书签的位置就能从上次停下来的地方继续看。只不过一个系统里面可能会有成百上千个线程，每个线程都有自己的&quot;`书签`&quot;。一般来说，协程库提供两个基本的操作：\n\n\n\n\nyield / swap out: 把控制权让出来，保存自己的状态。也就是插书签。\n\n\nresume / swap in: 获取控制权，恢复自己的状态。也就是根据书签的位置，继续读书。\n\n\n\n\n\n\n协程的实现\n\n\n书签和上下文\n\n书签保存的信息只有一个页码。但是对于一个线程来说，它在 CPU 上执行的状态对应着更多的信息。我们先看一个特例&amp;#8212;&amp;#8203;子函数的调用。假设我们在 main() 里面调用之前定义的函数 func()。\n\n\n\nint main()\n{\n  func();\n}\n\n\n\n为了让 func() 返回时，main() 能继续它当时未尽的事业，很明显，它需要\n\n\n\n\n在跳转到 func() 的起始地址之前，保存当下的 %ip。\n\n\n再把 %ip 改成 func() 的地址。\n\n\nfunc() 在返回的时候，需要把 %ip 恢复成之前保存的 版本。\n\n\n\n\nx86 很贴心的提供了 CALL 和 RET 两个指令。前者把 %ip 压栈，再根据 CALL 的参数更新 %ip。要是大家还能回忆相对寻址、绝对寻址的话，CALL 是支持这些寻址方式的。要是目标地址不在一个 %cs 段，它还能把当前 %cs 也一并保存了。RET 执行的是相反的功能。它把栈上的地址恢复回 %cs 和 %ip，如果 RET 还有参数的话，还顺带着把栈上的垃圾清理一下，也就是退栈。通常来说，调用方会把一些参数放到栈上，而参数的个数一般是确定的。所以被调用方在返回的时候，把那些参数从栈上清除也是理所当然的事情。\n\n\n可以说 CALL 和 RET 给了线程订了一张往返票，让它从一个地方走到另外一个地方出个差，然后再回来。 除了 %ip，根据 amd64 或者 x86-64 的 ABI 调用规范，在函数调用的时候，下面的寄存器是调用方负责的:\n\n\n\n\n​%rax\n\n\n​%rcx​\n\n\n​%rdx\n\n\n%rdi\n\n\n%rsi\n\n\n%r8 到 %r11\n\n\n\n\n换句话说，如果调用方觉得它无所谓函数返回之后这些寄存器的状态是否改变了，那么它完全可以选择不保存它们。其中，函数调用的前六个参数保存在 %rdi, %rsi，%rdx，%rcx, %r8d, %r9d。\n\n\n而被调用方则有义务保存：\n\n\n\n\n%rbx\n\n\n%rbp\n\n\n%rsp\n\n\n%r12 到 %r15\n\n\n\n\n也就是说，在函数返回之后，这些寄存器的值应该保持不变。这些要求定义了一个函数调用的行为规范，确保编译器能编译出有效率的代码，而不用花时间分析被调用的函数到底修改了哪些寄存器。所以一般来说，我们的 yield 实现也应该遵守这些基本的规范，保证调用方行为不受到干扰。\n\n\n那么从一个线程到另外一个线程呢？除了函数调用规范要求的那些寄存器，还有哪些状态需要保存呢？\n\n\n\n\npthread(7) 总结了一下。它说，POSIX.1 要求一个进程里面的线程有共同的一系列属性，比如说 process ID、uid、文件描述符以及 signal handler。它们也有自己的独立的属性，比如 errno、signalprocmask 还有 sigaltstack。这些属性有着各自不同的实现方式。\n\n\n\nerrno 它是 libc 实现的接口，让 libc 的函数能告诉调用方具体的错误号。 libc 一般把它保存在 %fs 段里面。但是如果我们不需要:\n\n\n int ret  = fstat(...);\n yield_to(another_thread);\n if (ret != 0) {\n   perror(&quot;fstat failed&quot;);\n }\n\n\n\n那么就没有必要保存和恢复 errno 了。\n\n\n\nsigprocmask 如果调度的线程 sigmask 不一样，那么我们的确需要保存恢复它们各自的 sigprocmask。但是如果它们的 sigmask 都一样的话，就可以不用管这个属性了。sigaltstack 也是类似的。\n\n\n\n\n\n函数调用使用栈来保存返回地址，传递一些参数。而每个线程都有自己的栈。在切换线程的时候，%rsp 和 %rbp 也需要指向新的线程自己的栈。\n\n\n浮点处理器的运行环境。这包括一系列寄存器。可以参考 FSTENV 和 FLDENV 这两个指令。\n\n\n\n\n\nlibc 的书签\n\n我们管这些林林总总的状态叫做&quot;`上下文`&quot;。 为了保存和恢复上下文，libc 提供了\n\n\n\n\nsetjmp() 保存当前的 %rbx, %rbp, %r12, %r13, %r14, %r15, %rsp, %rip 到指定的 jmp_buf 中。\n\n\nlongjmp() 从指定的 jmp_buf 恢复 %rbx, %rbp, %r12, %r13, %r14, %r15, %rsp 中。\n\n\n\n\n可以参考 musl-libc 的实现。可以说 setjmp() 和 longjmp() 是相当简练的。只提供了两个功能，一个是记录当前的位置，另一个是跳转到指定的位置。\n\n\n但是 glibc 的 longjmp 还更啰嗦一些，它在调用平台相关的__longjmp()之前，还调用了\n\n\n\n\n_longjmp_unwind()\n\n\n__sigprocmask()\n\n\n\n\n\nlibc 的 context\n\n虽然 setjmp() 和 longjmp() 很简练。但是它们只能允许我们回到一个已知的地方。这和之前书签的例子很像，如果之前没有用 setjmp() 得到 jmp_buf，那么是无法跳转到 jmp_buf 指示的地方的。如果我们希望实现协程的话。假设我们一开始启动了一个 POSIX 线程，当这个线程执行的函数希望 yield，把执行权交给另一个任务，而这个任务还从没执行过。那么 不手动修改jmp_buf 是无法实现这个功能的。读者可能会说，如果开始这个新任务的函数之前执行过，那么是不是在函数开始的时候用 setjmp()加个书签就可以了呢？这样会导致两个协程互相重用一个栈，导致原来的线程在返回时可能会读到错误的数据，也可能干脆跑飞掉。\n\n\n所以 glibc 干脆提供了下面这几个函数:\n\n\n\nint getcontext(ucontext_t *ucp);\nint setcontext(const ucontext_t *ucp);\nvoid makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);\nint swapcontext(ucontext_t *oucp, const ucontext_t *ucp);\n\n\n\n提供了比 setjmp() 和 longjmp() 更强大的功能。\n\n\ngetcontext() 记录当前的上下文。这个上下文可以作为一个模板，如果我们需要让它使用另一个栈，没问题！如果我们想让调度它的时候，运行 serve_request()，好的！对了，这个函数还应该有几个参数，嗯，我想在这里设置这些参数&amp;#8230;&amp;#8203;&amp;#8230;&amp;#8203;当然可以！这些函数满足了用户对协程的所有要求。但是它们也带来了一些问题\n\n\n\n\n过于完整的线程支持。setcontext() 和 swapcontext() 除了做了 longjmp() 的工作，还：\n\n\n\n用系统调用设置 sigprocmask\n\n\n设置 %fs，这是段寄存器。TLS 的变量都保存在这里面。\n\n\n\n\n\n不跨平台。 POSIX.1 已经把这几个函数去掉了。musl-libc 干脆[12][不实现他们]。\n\n\n把 context 串起来。调用当初设置的函数，要是执行完了，看看 uc_link，要是还有下一个 context。有的话，再调用 setcontext()，开始执行它。\n\n\n\n\n\nSeastar 的 thread\n\nSeastar 为了避免使用重量级的 swapcontext() 进行上下文切换，只是在开始的时候用 getcontext() 和 makecontext() 来初始化 context，而在平时调度的时候继续用 setjmp() 和 longjmp() 的组合。\n\n\n首先，每个用户态线程都有自己的 context，这个 context 包含\n\n\n\n\n一个 128KB 的栈\n\n\n一个 jmp_buf\n\n\n指向原来的 context 的指针\n\n\n\n\n在这里，ucontext 就像是一个通向 jmp_buf 的跳板。\n\n\n\n\n在初始化用户态线程的时候，Seastar 新建一个 ucontext，让它使用自己的栈，并把它指向一个静态函数 s_main()，这个函数的参数其实就是 thread_context 的地址，所以它得以调用 this-&amp;gt;main()。后者才会调用真正的任务函数。\n\n\n每个线程都用 TLS 保存着自己的 thread_context ，在工作线程调度到新的任务的时候，新的任务对应着新的 thread_context 实例。在这个新的 thread_context 开始运行之前，我们把当前的 context 作为成员变量保存在新的 thread_context 里面。然后用 setjmp() 把当前上下文保存在原来的 context 中。这时保存了原来 context 的上下文。\n\n\n不过我们并不保存这个新建的 ucontext，我们的目标是调度到 this-&amp;gt;main()。接下来用 setcontext() 跳转到这个 ucontext 完成调度。\n\n\n下一次要 yield 就简单很多，只需要 setjmp(this-&amp;gt;jmpbuf)，然后 longjmp(link-&amp;gt;jmpbuf) 就行了。\n\n\n类似的，如果是 resume，则是相反的操作。\n\n\n如果希望销毁这个用户态线程，则直接 longjmp(link-&amp;gt;jmpbuf) 。跳过保留上下文的步骤。\n\n\n\n\n\nBoost::context\n\nBoost::context 用汇编实现了平台相关的 fcontext_t ，它的性能据说比 ucontext_ 高一到两个数量级 。fcontext_ 保存的上下文 有\n\n\n\n\nMXCSR 中的控制字。x86 上 SSE/SSE2 用于保存浮点控制和状态的寄存器\n\n\nFPCR 即 X87 FPU control word。\n\n\n\n\n这两个寄存器状态和 Intel TSX 机制有关系。TSX (Intel Transactional Synchronisation Extensions) 是 Intel 实现的硬件内存事务机制，可以粗略地理解，它使用 L1 cache 跟踪读集合和写集合，如果出现冲突的话，就放弃当前核上的修改，不把它刷到内存里面去，导致不一致。我们可以在另外一篇文章里面继续讨论内存一致性、可见性和多核系统里面乱序执行的问题。不过这里保存它们的原因是因为，如果浮点 TSX 的事务中发现浮点状态字有变化，那么这个事务肯定会 终止。所以为了支持 TSX，Boost 也保存这些浮点寄存器。顺便说一下，内核里面是不能用浮点操作的。所以那边我们不需要关心这种问题。\n\n\n基于这套实现，Boost 实现了自己的协程库。\n\n\nseastar-devel 上的 讨论也是围绕着这一点。 Christian 觉得手工实现 longjmp() 会比较高效。Avi 提到当初他也考虑过 Boost::context。因为它比较简单明了，同时没有 glibc 中 _longjmp_unwind() 和 __sigprocmask() 的开销，所以对于广大的 glibc 用户来说，使用 Boost::context 性能会更好一些。 不过 Boost::context 在 1.55/1.56 中的实现还不成熟。为了精炼版的 longjmp()，只能有两条路，\n\n\n\n\n要求用户使用新版的 Boost\n\n\n把 fcontext_t 使用的汇编代码移植到 Seastar 里面去。\n\n\n\n\n不过 Avi 提到，glibc 中的 longjmp() 在上下文切换操作中占用的时间其实并不算多。所以就没有必要手撸汇编了。\n\n\n\n"
} ,
  
  {
    "title"    : "开场白",
    "category" : "misc",
    "tags"     : " ",
    "url"      : "/misc/2020/08/08/hello-world.html",
    "date"     : "August 8, 2020",
    "excerpt"  : "\n为了记录，也为了提高自己，写个 blog。\n",
  "content"  : "\n为了记录，也为了提高自己，写个 blog。\n"
} 
  
  
  
]
